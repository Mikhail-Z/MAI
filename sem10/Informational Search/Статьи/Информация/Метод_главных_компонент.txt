Метод главных компонент (англ. principal component analysis, PCA) — один из основных способов уменьшить размерность данных, потеряв наименьшее количество информации. Изобретён Карлом Пирсоном в 1901 году. Применяется во многих областях, в том числе, в эконометрике, биоинформатике, обработке изображений, для сжатия данных, в общественных науках.
Вычисление главных компонент может быть сведено к вычислению сингулярного разложения матрицы данных или к вычислению собственных векторов и собственных значений ковариационной матрицы исходных данных. Иногда метод главных компонент называют преобразованием Кархунена — Лоэва или преобразованием Хотеллинга (англ. Hotelling transform).

Формальная постановка задачи
Задача анализа главных компонент имеет, как минимум, четыре базовых версии:

аппроксимировать данные линейными многообразиями меньшей размерности;
найти подпространства меньшей размерности, в ортогональной проекции на которые разброс данных (то есть среднеквадратичное отклонение от среднего значения) максимален;
найти подпространства меньшей размерности, в ортогональной проекции на которые среднеквадратичное расстояние между точками максимально;
для данной многомерной случайной величины построить такое ортогональное преобразование координат, в результате которого корреляции между отдельными координатами обратятся в нуль.Первые три версии оперируют конечными множествами данных. Они эквивалентны и не используют никакой гипотезы о статистическом порождении данных. Четвёртая версия оперирует случайными величинами. Конечные множества появляются здесь как выборки из данного распределения, а решение трёх первых задач — как приближение к разложению по теореме Кархунена — Лоэва («истинному преобразованию Кархунена — Лоэва»). При этом возникает дополнительный и не вполне тривиальный вопрос о точности этого приближения.

Аппроксимация данных линейными многообразиями
Метод главных компонент начинался с задачи наилучшей аппроксимации конечного множества точек прямыми и плоскостями (Пирсон, 1901). Дано конечное множество векторов 
  
    
      
        
          x
          
            1
          
        
        ,
        
          x
          
            2
          
        
        ,
        …
        ,
        
          x
          
            m
          
        
        ∈
        
          
            R
          
          
            n
          
        
      
    
    {\displaystyle x_{1},x_{2},\dots ,x_{m}\in \mathbb {R} ^{n}}
  , для каждого 
  
    
      
        k
        =
        0
        ,
        1
        ,
        …
        ,
        n
        −
        1
      
    
    {\displaystyle k=0,1,\dots ,n-1}
   среди всех 
  
    
      
        k
      
    
    {\displaystyle k}
  -мерных линейных многообразий в 
  
    
      
        
          
            R
          
          
            n
          
        
      
    
    {\displaystyle \mathbb {R} ^{n}}
   найти такое 
  
    
      
        
          L
          
            k
          
        
        ⊂
        
          
            R
          
          
            n
          
        
      
    
    {\displaystyle L_{k}\subset \mathbb {R} ^{n}}
  , что сумма квадратов уклонений 
  
    
      
        
          x
          
            i
          
        
      
    
    {\displaystyle x_{i}}
   от 
  
    
      
        
          L
          
            k
          
        
      
    
    {\displaystyle L_{k}}
   минимальна:

  
    
      
        
          ∑
          
            i
            =
            1
          
          
            m
          
        
        
          dist
          
            2
          
        
        ⁡
        (
        
          x
          
            i
          
        
        ,
        
          L
          
            k
          
        
        )
        →
        min
      
    
    {\displaystyle \sum _{i=1}^{m}\operatorname {dist} ^{2}(x_{i},L_{k})\to \min }
  ,где 
  
    
      
        dist
        ⁡
        (
        
          x
          
            i
          
        
        ,
        
          L
          
            k
          
        
        )
      
    
    {\displaystyle \operatorname {dist} (x_{i},L_{k})}
   — евклидово расстояние от точки до линейного многообразия. Всякое 
  
    
      
        k
      
    
    {\displaystyle k}
  -мерное линейное многообразие в 
  
    
      
        
          
            R
          
          
            n
          
        
      
    
    {\displaystyle \mathbb {R} ^{n}}
   может быть задано как множество линейных комбинаций 
  
    
      
        
          L
          
            k
          
        
        =
        {
        
          a
          
            0
          
        
        +
        
          β
          
            1
          
        
        
          a
          
            1
          
        
        +
        ⋯
        +
        
          β
          
            k
          
        
        
          a
          
            k
          
        
        
          |
        
        
          β
          
            i
          
        
        ∈
        
          R
        
        }
      
    
    {\displaystyle L_{k}=\{a_{0}+\beta _{1}a_{1}+\dots +\beta _{k}a_{k}|\beta _{i}\in \mathbb {R} \}}
  , где параметры 
  
    
      
        
          β
          
            i
          
        
      
    
    {\displaystyle \beta _{i}}
   пробегают вещественную прямую 
  
    
      
        
          R
        
      
    
    {\displaystyle \mathbb {R} }
  , 
  
    
      
        
          a
          
            0
          
        
        ∈
        
          
            R
          
          
            n
          
        
      
    
    {\displaystyle a_{0}\in \mathbb {R} ^{n}}
   а 
  
    
      
        
          {
          
            
              a
              
                1
              
            
            ,
            …
            ,
            
              a
              
                k
              
            
          
          }
        
        ⊂
        
          
            R
          
          
            n
          
        
      
    
    {\displaystyle \left\{a_{1},\dots ,a_{k}\right\}\subset \mathbb {R} ^{n}}
   — ортонормированный набор векторов

  
    
      
        
          dist
          
            2
          
        
        ⁡
        (
        
          x
          
            i
          
        
        ,
        
          L
          
            k
          
        
        )
        =
        ‖
        
          x
          
            i
          
        
        −
        
          a
          
            0
          
        
        −
        
          ∑
          
            j
            =
            1
          
          
            k
          
        
        
          a
          
            j
          
        
        (
        
          a
          
            j
          
        
        ,
        
          x
          
            i
          
        
        −
        
          a
          
            0
          
        
        )
        
          ‖
          
            2
          
        
      
    
    {\displaystyle \operatorname {dist} ^{2}(x_{i},L_{k})=\Vert x_{i}-a_{0}-\sum _{j=1}^{k}a_{j}(a_{j},x_{i}-a_{0})\Vert ^{2}}
  ,где 
  
    
      
        ‖
        ⋅
        ‖
      
    
    {\displaystyle \Vert \cdot \Vert }
   евклидова норма, 
  
    
      
        
          (
          
            
              a
              
                j
              
            
            ,
            
              x
              
                i
              
            
          
          )
        
      
    
    {\displaystyle \left(a_{j},x_{i}\right)}
   — евклидово скалярное произведение, или в координатной форме:

  
    
      
        
          dist
          
            2
          
        
        ⁡
        (
        
          x
          
            i
          
        
        ,
        
          L
          
            k
          
        
        )
        =
        
          ∑
          
            l
            =
            1
          
          
            n
          
        
        
          
            (
            
              
                x
                
                  i
                  l
                
              
              −
              
                a
                
                  0
                  l
                
              
              −
              
                ∑
                
                  j
                  =
                  1
                
                
                  k
                
              
              
                a
                
                  j
                  l
                
              
              
                ∑
                
                  q
                  =
                  1
                
                
                  n
                
              
              
                a
                
                  j
                  q
                
              
              (
              
                x
                
                  i
                  q
                
              
              −
              
                a
                
                  0
                  q
                
              
              )
            
            )
          
          
            2
          
        
      
    
    {\displaystyle \operatorname {dist} ^{2}(x_{i},L_{k})=\sum _{l=1}^{n}\left(x_{il}-a_{0l}-\sum _{j=1}^{k}a_{jl}\sum _{q=1}^{n}a_{jq}(x_{iq}-a_{0q})\right)^{2}}
  .Решение задачи аппроксимации для 
  
    
      
        k
        =
        0
        ,
        1
        ,
        …
        ,
        n
        −
        1
      
    
    {\displaystyle k=0,1,\dots ,n-1}
   даётся набором вложенных линейных многообразий 
  
    
      
        
          L
          
            0
          
        
        ⊂
        
          L
          
            1
          
        
        ⊂
        …
        
          L
          
            n
            −
            1
          
        
      
    
    {\displaystyle L_{0}\subset L_{1}\subset \dots L_{n-1}}
  , 
  
    
      
        
          L
          
            k
          
        
        =
        {
        
          a
          
            0
          
        
        +
        
          β
          
            1
          
        
        
          a
          
            1
          
        
        +
        …
        +
        
          β
          
            k
          
        
        
          a
          
            k
          
        
        
          |
        
        
          β
          
            i
          
        
        ∈
        
          R
        
        }
      
    
    {\displaystyle L_{k}=\{a_{0}+\beta _{1}a_{1}+\ldots +\beta _{k}a_{k}|\beta _{i}\in \mathbb {R} \}}
  . Эти линейные многообразия определяются ортонормированным набором векторов 
  
    
      
        
          {
          
            
              a
              
                1
              
            
            ,
            .
            .
            .
            ,
            
              a
              
                n
                −
                1
              
            
          
          }
        
      
    
    {\displaystyle \left\{a_{1},...,a_{n-1}\right\}}
   (векторами главных компонент) и вектором 
  
    
      
        
          a
          
            0
          
        
      
    
    {\displaystyle a_{0}}
  .
Вектор 
  
    
      
        
          a
          
            0
          
        
      
    
    {\displaystyle a_{0}}
   ищется как решение задачи минимизации для 
  
    
      
        
          L
          
            0
          
        
      
    
    {\displaystyle L_{0}}
  :

  
    
      
        
          a
          
            0
          
        
        =
        
          
            argmin
            
              
                a
                
                  0
                
              
              ∈
              
                
                  R
                
                
                  n
                
              
            
          
        
        
          (
          
            
              ∑
              
                i
                =
                1
              
              
                m
              
            
            
              dist
              
                2
              
            
            ⁡
            (
            
              x
              
                i
              
            
            ,
            
              L
              
                0
              
            
            )
          
          )
        
      
    
    {\displaystyle a_{0}={\underset {a_{0}\in \mathbb {R} ^{n}}{\operatorname {argmin} }}\left(\sum _{i=1}^{m}\operatorname {dist} ^{2}(x_{i},L_{0})\right)}
  ,то есть

  
    
      
        
          a
          
            0
          
        
        =
        
          
            argmin
            
              
                a
                
                  0
                
              
              ∈
              
                
                  R
                
                
                  n
                
              
            
          
        
        
          (
          
            
              ∑
              
                i
                =
                1
              
              
                m
              
            
            ‖
            
              x
              
                i
              
            
            −
            
              a
              
                0
              
            
            
              ‖
              
                2
              
            
          
          )
        
      
    
    {\displaystyle a_{0}={\underset {a_{0}\in \mathbb {R} ^{n}}{\operatorname {argmin} }}\left(\sum _{i=1}^{m}\Vert x_{i}-a_{0}\Vert ^{2}\right)}
  .Это — выборочное среднее: 
  
    
      
        
          a
          
            0
          
        
        =
        
          
            1
            m
          
        
        
          ∑
          
            i
            =
            1
          
          
            m
          
        
        
          x
          
            i
          
        
        =
        
          
            X
            ¯
          
        
      
    
    {\displaystyle a_{0}={\frac {1}{m}}\sum _{i=1}^{m}x_{i}={\overline {X}}}
  .
Фреше в 1948 году обратил внимание, что вариационное определение среднего (как точки, минимизирующей сумму квадратов расстояний до точек данных) очень удобно для построения статистики в произвольном метрическом пространстве, и построил обобщение классической статистики для общих пространств (обобщённый метод наименьших квадратов).
Векторы главных компонент могут быть найдены как решения однотипных задач оптимизации:

Централизуются данные (вычитанием среднего): 
  
    
      
        
          x
          
            i
          
        
        :=
        
          x
          
            i
          
        
        −
        
          
            X
            ¯
          
        
      
    
    {\displaystyle x_{i}:=x_{i}-{\overline {X}}}
  . Теперь 
  
    
      
        
          ∑
          
            i
            =
            1
          
          
            m
          
        
        
          x
          
            i
          
        
        =
        0
      
    
    {\displaystyle \sum _{i=1}^{m}x_{i}=0}
  ;
Отыскивается первая главная компонента как решение задачи:

  
    
      
        
          a
          
            1
          
        
        =
        
          
            argmin
            
              ‖
              
                a
                
                  1
                
              
              ‖
              =
              1
            
          
        
        
          (
          
            
              ∑
              
                i
                =
                1
              
              
                m
              
            
            ‖
            
              x
              
                i
              
            
            −
            
              a
              
                1
              
            
            (
            
              a
              
                1
              
            
            ,
            
              x
              
                i
              
            
            )
            
              ‖
              
                2
              
            
          
          )
        
      
    
    {\displaystyle a_{1}={\underset {\Vert a_{1}\Vert =1}{\operatorname {argmin} }}\left(\sum _{i=1}^{m}\Vert x_{i}-a_{1}(a_{1},x_{i})\Vert ^{2}\right)}
  .
если решение не единственно, то осуществляется выбор одного из них.
Из данных вычитается проекция на первую главную компоненту:

  
    
      
        
          x
          
            i
          
        
        :=
        
          x
          
            i
          
        
        −
        
          a
          
            1
          
        
        
          (
          
            
              a
              
                1
              
            
            ,
            
              x
              
                i
              
            
          
          )
        
      
    
    {\displaystyle x_{i}:=x_{i}-a_{1}\left(a_{1},x_{i}\right)}
  ;
Отыскивается вторая главная компонента как решение задачи:

  
    
      
        
          a
          
            2
          
        
        =
        
          
            argmin
            
              ‖
              
                a
                
                  2
                
              
              ‖
              =
              1
            
          
        
        
          (
          
            
              ∑
              
                i
                =
                1
              
              
                m
              
            
            ‖
            
              x
              
                i
              
            
            −
            
              a
              
                2
              
            
            (
            
              a
              
                2
              
            
            ,
            
              x
              
                i
              
            
            )
            
              ‖
              
                2
              
            
          
          )
        
      
    
    {\displaystyle a_{2}={\underset {\Vert a_{2}\Vert =1}{\operatorname {argmin} }}\left(\sum _{i=1}^{m}\Vert x_{i}-a_{2}(a_{2},x_{i})\Vert ^{2}\right)}
  .
Если решение не единственно, то выбирается одно из них.Далее процесс продолжается, то есть на шаге 
  
    
      
        2
        k
        −
        1
      
    
    {\displaystyle 2k-1}
   вычитается проекция на 
  
    
      
        (
        k
        −
        1
        )
      
    
    {\displaystyle (k-1)}
  -ю главную компоненту (к этому моменту проекции на предшествующие 
  
    
      
        (
        k
        −
        2
        )
      
    
    {\displaystyle (k-2)}
   главные компоненты уже вычтены):

  
    
      
        
          x
          
            i
          
        
        :=
        
          x
          
            i
          
        
        −
        
          a
          
            k
            −
            1
          
        
        
          (
          
            
              a
              
                k
                −
                1
              
            
            ,
            
              x
              
                i
              
            
          
          )
        
      
    
    {\displaystyle x_{i}:=x_{i}-a_{k-1}\left(a_{k-1},x_{i}\right)}
  ;и на шаге 
  
    
      
        2
        k
      
    
    {\displaystyle 2k}
   определяется 
  
    
      
        k
      
    
    {\displaystyle k}
  -я главная компонента как решение задачи:

  
    
      
        
          a
          
            k
          
        
        =
        
          
            argmin
            
              ‖
              
                a
                
                  k
                
              
              ‖
              =
              1
            
          
        
        
          (
          
            
              ∑
              
                i
                =
                1
              
              
                m
              
            
            ‖
            
              x
              
                i
              
            
            −
            
              a
              
                k
              
            
            (
            
              a
              
                k
              
            
            ,
            
              x
              
                i
              
            
            )
            
              ‖
              
                2
              
            
          
          )
        
      
    
    {\displaystyle a_{k}={\underset {\Vert a_{k}\Vert =1}{\operatorname {argmin} }}\left(\sum _{i=1}^{m}\Vert x_{i}-a_{k}(a_{k},x_{i})\Vert ^{2}\right)}
   (если решение не единственно, то выбирается одно из них).На каждом подготовительном шаге 
  
    
      
        (
        2
        k
        −
        1
        )
      
    
    {\displaystyle (2k-1)}
   вычитается проекция на предшествующую главную компоненту. Найденные векторы 
  
    
      
        
          {
          
            
              a
              
                1
              
            
            ,
            .
            .
            .
            ,
            
              a
              
                n
                −
                1
              
            
          
          }
        
      
    
    {\displaystyle \left\{a_{1},...,a_{n-1}\right\}}
   ортонормированы просто в результате решения описанной задачи оптимизации, однако чтобы не дать ошибкам вычисления нарушить взаимную ортогональность векторов главных компонент, можно включать 
  
    
      
        
          a
          
            k
          
        
        ⊥
        {
        
          a
          
            1
          
        
        ,
        .
        .
        .
        ,
        
          a
          
            k
            −
            1
          
        
        }
      
    
    {\displaystyle a_{k}\bot \{a_{1},...,a_{k-1}\}}
   в условия задачи оптимизации.
Неединственность в определении 
  
    
      
        
          a
          
            k
          
        
      
    
    {\displaystyle a_{k}}
   помимо тривиального произвола в выборе знака (
  
    
      
        
          a
          
            k
          
        
      
    
    {\displaystyle a_{k}}
   и 
  
    
      
        −
        
          a
          
            k
          
        
      
    
    {\displaystyle -a_{k}}
   решают ту же задачу) может быть более существенной и происходить, например, из условий симметрии данных. Последняя главная компонента 
  
    
      
        
          a
          
            n
          
        
      
    
    {\displaystyle a_{n}}
   — единичный вектор, ортогональный всем предыдущим 
  
    
      
        
          a
          
            k
          
        
      
    
    {\displaystyle a_{k}}
  .

Поиск ортогональных проекций с наибольшим рассеянием
Пусть нам дан центрированный набор векторов данных 
  
    
      
        
          x
          
            i
          
        
        ∈
        
          
            R
          
          
            n
          
        
        
        (
        i
        =
        1
        ,
        .
        .
        .
        ,
        m
        )
      
    
    {\displaystyle x_{i}\in \mathbb {R} ^{n}\;(i=1,...,m)}
   (среднее арифметическое значение 
  
    
      
        
          x
          
            i
          
        
      
    
    {\displaystyle x_{i}}
   равно нулю). Задача — найти такое ортогональное преобразование в новую систему координат, для которого были бы верны следующие условия:

Выборочная дисперсия данных вдоль первой координаты максимальна (эту координату называют первой главной компонентой);
Выборочная дисперсия данных вдоль второй координаты максимальна при условии ортогональности первой координате (вторая главная компонента);
…
Выборочная дисперсия данных вдоль значений 
  
    
      
        k
      
    
    {\displaystyle k}
  -ой координаты максимальна при условии ортогональности первым 
  
    
      
        k
        −
        1
      
    
    {\displaystyle k-1}
   координатам;
…Выборочная дисперсия данных вдоль направления, заданного нормированным вектором 
  
    
      
        
          a
          
            k
          
        
      
    
    {\displaystyle a_{k}}
  , это

  
    
      
        
          S
          
            m
          
          
            2
          
        
        
          [
          
            (
            X
            ,
            
              a
              
                k
              
            
            )
          
          ]
        
        =
        
          
            1
            m
          
        
        
          ∑
          
            i
            =
            1
          
          
            m
          
        
        (
        
          a
          
            k
          
        
        ,
        
          x
          
            i
          
        
        
          )
          
            2
          
        
        =
        
          
            1
            m
          
        
        
          ∑
          
            i
            =
            1
          
          
            m
          
        
        
          
            (
            
              
                ∑
                
                  j
                  =
                  1
                
                
                  n
                
              
              
                x
                
                  i
                  j
                
              
              
                a
                
                  k
                  j
                
              
            
            )
          
          
            2
          
        
      
    
    {\displaystyle S_{m}^{2}\left[(X,a_{k})\right]={\frac {1}{m}}\sum \limits _{i=1}^{m}(a_{k},x_{i})^{2}={\frac {1}{m}}\sum \limits _{i=1}^{m}\left(\sum \limits _{j=1}^{n}x_{ij}a_{kj}\right)^{2}}
  (поскольку данные центрированы, выборочная дисперсия здесь совпадает со средним квадратом уклонения от нуля).
Решение задачи о наилучшей аппроксимации даёт то же множество главных компонент 
  
    
      
        
          {
          
            a
            
              i
            
          
          }
        
      
    
    {\displaystyle \left\{a_{i}\right\}}
  , что и поиск ортогональных проекций с наибольшим рассеянием, по очень простой причине: 
  
    
      
        ‖
        
          x
          
            i
          
        
        −
        
          a
          
            k
          
        
        (
        
          a
          
            k
          
        
        ,
        
          x
          
            i
          
        
        )
        
          ‖
          
            2
          
        
        =
        ‖
        
          x
          
            i
          
        
        
          ‖
          
            2
          
        
        −
        (
        
          a
          
            k
          
        
        ,
        
          x
          
            i
          
        
        
          )
          
            2
          
        
        ,
      
    
    {\displaystyle \Vert x_{i}-a_{k}(a_{k},x_{i})\Vert ^{2}=\Vert x_{i}\Vert ^{2}-(a_{k},x_{i})^{2},}
   и первое слагаемое не зависит от 
  
    
      
        
          a
          
            k
          
        
      
    
    {\displaystyle a_{k}}
  .

Поиск ортогональных проекций с наибольшим среднеквадратичным расстоянием между точками
Ещё одна эквивалентная формулировка следует из очевидного тождества, верного для любых 
  
    
      
        m
      
    
    {\displaystyle m}
   векторов 
  
    
      
        
          x
          
            i
          
        
      
    
    {\displaystyle x_{i}}
  :

  
    
      
        
          
            1
            
              m
              (
              m
              −
              1
              )
            
          
        
        
          ∑
          
            i
            ,
            j
            =
            1
          
          
            m
          
        
        (
        
          x
          
            i
          
        
        −
        
          x
          
            j
          
        
        
          )
          
            2
          
        
        =
        
          
            
              2
              
                m
                
                  2
                
              
            
            
              m
              (
              m
              −
              1
              )
            
          
        
        
          [
          
            
              
                1
                m
              
            
            
              ∑
              
                i
                =
                1
              
              
                m
              
            
            
              x
              
                i
              
              
                2
              
            
            −
            
              
                (
                
                  
                    
                      1
                      m
                    
                  
                  
                    ∑
                    
                      i
                    
                    
                      m
                    
                  
                  
                    x
                    
                      i
                    
                  
                
                )
              
              
                2
              
            
          
          ]
        
        .
      
    
    {\displaystyle {\frac {1}{m(m-1)}}\sum _{i,j=1}^{m}(x_{i}-x_{j})^{2}={\frac {2m^{2}}{m(m-1)}}\left[{\frac {1}{m}}\sum _{i=1}^{m}x_{i}^{2}-\left({\frac {1}{m}}\sum _{i}^{m}x_{i}\right)^{2}\right].}
  В левой части этого тождества стоит среднеквадратичное расстояние между точками, а в квадратных скобках справа — выборочная дисперсия. Таким образом, в методе главных компонент ищутся подпространства, в проекции на которые среднеквадратичное расстояние между точками максимально (или, что то же самое, его искажение в результате проекции минимально). Такая переформулировка позволяет строить обобщения с взвешиванием различных парных расстояний (а не только точек).

Аннулирование корреляций между координатами
Для заданной 
  
    
      
        n
      
    
    {\displaystyle n}
  -мерной случайной величины 
  
    
      
        X
      
    
    {\displaystyle X}
   найти такой ортонормированный базис, 
  
    
      
        
          {
          
            
              a
              
                1
              
            
            ,
            .
            .
            .
            ,
            
              a
              
                n
              
            
          
          }
        
      
    
    {\displaystyle \left\{a_{1},...,a_{n}\right\}}
  , в котором коэффициент ковариации между различными координатами равен нулю. После преобразования к этому базису

  
    
      
        cov
        ⁡
        (
        
          X
          
            i
          
        
        ,
        
          X
          
            j
          
        
        )
        =
        0
      
    
    {\displaystyle \operatorname {cov} (X_{i},X_{j})=0}
   для 
  
    
      
        i
        ≠
        j
      
    
    {\displaystyle i\neq j}
  .Здесь 
  
    
      
        cov
        ⁡
        (
        
          X
          
            i
          
        
        ,
        
          X
          
            j
          
        
        )
        =
        E
        ⁡
        [
        (
        
          X
          
            i
          
        
        −
        E
        ⁡
        [
        
          X
          
            i
          
        
        ]
        )
        (
        
          X
          
            j
          
        
        −
        E
        ⁡
        [
        
          X
          
            j
          
        
        ]
        )
        ]
      
    
    {\displaystyle \operatorname {cov} (X_{i},X_{j})=\operatorname {E} [(X_{i}-\operatorname {E} [X_{i}])(X_{j}-\operatorname {E} [X_{j}])]}
   — коэффициент ковариации, где 
  
    
      
        E
      
    
    {\displaystyle \operatorname {E} }
   — математическое ожидание.

Диагонализация ковариационной матрицы
Все задачи о главных компонентах приводят к задаче диагонализации ковариационной матрицы или выборочной ковариационной матрицы. Эмпирическая или выборочная ковариационная матрица, это

  
    
      
        C
        =
        [
        
          c
          
            i
            j
          
        
        ]
        ,
         
        
          c
          
            i
            j
          
        
        =
        
          
            1
            
              m
              −
              1
            
          
        
        
          ∑
          
            l
            =
            1
          
          
            m
          
        
        (
        
          x
          
            l
            i
          
        
        −
        
          
            
              X
              
                i
              
            
            ¯
          
        
        )
        (
        
          x
          
            l
            j
          
        
        −
        
          
            
              X
              
                j
              
            
            ¯
          
        
        )
        .
      
    
    {\displaystyle C=[c_{ij}],\ c_{ij}={\frac {1}{m-1}}\sum _{l=1}^{m}(x_{li}-{\overline {X_{i}}})(x_{lj}-{\overline {X_{j}}}).}
  Ковариационная матрица многомерной случайной величины 
  
    
      
        X
      
    
    {\displaystyle X}
  , это

  
    
      
        Σ
        =
        [
        
          σ
          
            i
            j
          
        
        ]
        ,
         
        
          σ
          
            i
            j
          
        
        =
        cov
        ⁡
        (
        
          X
          
            i
          
        
        ,
        
          X
          
            j
          
        
        )
        =
        E
        ⁡
        [
        (
        
          X
          
            i
          
        
        −
        E
        ⁡
        [
        
          X
          
            i
          
        
        ]
        )
        (
        
          X
          
            j
          
        
        −
        E
        ⁡
        [
        
          X
          
            j
          
        
        ]
        )
        ]
        .
      
    
    {\displaystyle \Sigma =[\sigma _{ij}],\ \sigma _{ij}=\operatorname {cov} (X_{i},X_{j})=\operatorname {E} [(X_{i}-\operatorname {E} [X_{i}])(X_{j}-\operatorname {E} [X_{j}])].}
  Векторы главных компонент для задач о наилучшей аппроксимации и о поиске ортогональных проекций с наибольшим рассеянием — это ортонормированный набор 
  
    
      
        
          {
          
            
              a
              
                1
              
            
            ,
            .
            .
            .
            ,
            
              a
              
                n
              
            
          
          }
        
      
    
    {\displaystyle \left\{a_{1},...,a_{n}\right\}}
   собственных векторов эмпирической ковариационной матрицы 
  
    
      
        C
      
    
    {\displaystyle C}
  , расположенных в порядке убывания собственных значений 
  
    
      
        λ
        :
        
          λ
          
            1
          
        
        ≥
        
          λ
          
            2
          
        
        ≥
        …
        ≥
        
          λ
          
            n
          
        
        ≥
        0.
      
    
    {\displaystyle \lambda :\lambda _{1}\geq \lambda _{2}\geq \ldots \geq \lambda _{n}\geq 0.}
   Эти векторы служат оценкой для собственных векторов ковариационной матрицы 
  
    
      
        cov
        ⁡
        (
        
          X
          
            i
          
        
        ,
        
          X
          
            j
          
        
        )
      
    
    {\displaystyle \operatorname {cov} (X_{i},X_{j})}
  . В базисе из собственных векторов ковариационной матрицы она, естественно, диагональна, и в этом базисе коэффициент ковариации между различными координатами равен нулю.
Если спектр ковариационной матрицы вырожден, то выбирают произвольный ортонормированный базис собственных векторов. Он существует всегда, а собственные числа ковариационной матрицы всегда вещественны и неотрицательны.

Сингулярное разложение матрицы данных
Идея сингулярного разложения
Математическое содержание метода главных компонент — это спектральное разложение ковариационной матрицы 
  
    
      
        C
      
    
    {\displaystyle C}
  , то есть представление пространства данных в виде суммы взаимно ортогональных собственных подпространств 
  
    
      
        C
      
    
    {\displaystyle C}
  , а самой матрицы 
  
    
      
        C
      
    
    {\displaystyle C}
   — в виде линейной комбинации ортогональных проекторов на эти подпространства с коэффициентами 
  
    
      
        
          λ
          
            i
          
        
      
    
    {\displaystyle \lambda _{i}}
  . Если 
  
    
      
        X
        =
        
          
            {
            
              
                x
                
                  1
                
              
              ,
              .
              .
              .
              ,
              
                x
                
                  m
                
              
            
            }
          
          
            T
          
        
      
    
    {\displaystyle \operatorname {X} =\left\{x_{1},...,x_{m}\right\}^{T}}
   — матрица, составленная из векторов-строк (размерности 
  
    
      
        n
      
    
    {\displaystyle n}
  ) центрированных данных, то 
  
    
      
        C
        =
        
          
            1
            
              m
              −
              1
            
          
        
        
          X
          
            T
          
        
        ⁡
        X
      
    
    {\displaystyle C={\frac {1}{m-1}}\operatorname {X} ^{T}\operatorname {X} }
   и задача о спектральном разложении ковариационной матрицы 
  
    
      
        C
      
    
    {\displaystyle C}
   превращается в задачу о сингулярном разложении матрицы данных 
  
    
      
        X
      
    
    {\displaystyle \operatorname {X} }
  .
Число 
  
    
      
        σ
        ≥
        0
      
    
    {\displaystyle \sigma \geq 0}
   называется сингулярным числом матрицы 
  
    
      
        X
      
    
    {\displaystyle \operatorname {X} }
   тогда и только тогда, когда существуют правый и левый сингулярные векторы: такие 
  
    
      
        m
      
    
    {\displaystyle m}
  -мерный вектор-строка 
  
    
      
        
          b
          
            σ
          
        
      
    
    {\displaystyle b_{\sigma }}
   и 
  
    
      
        n
      
    
    {\displaystyle n}
  -мерный вектор-столбец 
  
    
      
        
          a
          
            σ
          
        
      
    
    {\displaystyle a_{\sigma }}
   (оба единичной длины), что выполнено два равенства:

  
    
      
        X
        ⁡
        
          a
          
            σ
          
        
        =
        σ
        
          b
          
            σ
          
          
            T
          
        
        ;
        
        
        
          b
          
            σ
          
        
        X
        =
        σ
        
          a
          
            σ
          
          
            T
          
        
        .
      
    
    {\displaystyle \operatorname {X} a_{\sigma }=\sigma b_{\sigma }^{T};\,\,b_{\sigma }\operatorname {X} =\sigma a_{\sigma }^{T}.}
  Пусть 
  
    
      
        p
        =
        rang
        ⁡
        X
        ≤
        min
        {
        n
        ,
        m
        }
      
    
    {\displaystyle p=\operatorname {rang} \operatorname {X} \leq \min\{n,m\}}
   — ранг матрицы данных. Сингулярное разложение матрицы данных 
  
    
      
        X
      
    
    {\displaystyle \operatorname {X} }
   — это её представление в виде

  
    
      
        X
        =
        
          ∑
          
            l
            =
            1
          
          
            p
          
        
        
          σ
          
            l
          
        
        
          b
          
            l
          
          
            T
          
        
        
          a
          
            l
          
          
            T
          
        
        ;
        
        
          X
          
            T
          
        
        =
        
          ∑
          
            l
            =
            1
          
          
            p
          
        
        
          σ
          
            l
          
        
        
          a
          
            l
          
        
        
          b
          
            l
          
        
        
        
          (
          
            
              x
              
                i
                j
              
            
            =
            
              ∑
              
                l
                =
                1
              
              
                p
              
            
            
              σ
              
                l
              
            
            
              b
              
                l
                i
              
            
            
              a
              
                l
                j
              
            
          
          )
        
        ,
      
    
    {\displaystyle \operatorname {X} =\sum _{l=1}^{p}\sigma _{l}b_{l}^{T}a_{l}^{T};\;\operatorname {X} ^{T}=\sum _{l=1}^{p}\sigma _{l}a_{l}b_{l}\;\left(x_{ij}=\sum _{l=1}^{p}\sigma _{l}b_{li}a_{lj}\right),}
  где 
  
    
      
        
          σ
          
            l
          
        
        >
        0
      
    
    {\displaystyle \sigma _{l}>0}
   — сингулярное число, 
  
    
      
        
          a
          
            l
          
        
        =
        (
        
          a
          
            l
            j
          
        
        )
        ,
        
        j
        =
        1
        ,
        .
        .
        .
        n
      
    
    {\displaystyle a_{l}=(a_{lj}),\,j=1,...n}
   — соответствующий правый сингулярный вектор-столбец, а 
  
    
      
        
          b
          
            l
          
        
        =
        (
        
          b
          
            l
            i
          
        
        )
        ,
        
        i
        =
        1
        ,
        .
        .
        .
        m
      
    
    {\displaystyle b_{l}=(b_{li}),\,i=1,...m}
   — соответствующий левый сингулярный вектор-строка (
  
    
      
        l
        =
        1
        ,
        .
        .
        .
        p
      
    
    {\displaystyle l=1,...p}
  ). Правые сингулярные векторы-столбцы 
  
    
      
        
          a
          
            l
          
        
      
    
    {\displaystyle a_{l}}
  , участвующие в этом разложении, являются векторами главных компонент и собственными векторами эмпирической ковариационной матрицы

  
    
      
        C
        =
        
          
            1
            
              m
              −
              1
            
          
        
        
          X
          
            T
          
        
        ⁡
        X
      
    
    {\displaystyle C={\frac {1}{m-1}}\operatorname {X} ^{T}\operatorname {X} }
  , отвечающими положительным собственным числам 
  
    
      
        
          λ
          
            l
          
        
        =
        
          
            1
            
              m
              −
              1
            
          
        
        
          σ
          
            l
          
          
            2
          
        
        >
        0
      
    
    {\displaystyle \lambda _{l}={\frac {1}{m-1}}\sigma _{l}^{2}>0}
  .
Хотя формально задачи сингулярного разложения матрицы данных и спектрального разложения ковариационной матрицы совпадают, алгоритмы вычисления сингулярного разложения напрямую, без вычисления ковариационной матрицы и её спектра, более эффективны и устойчивы.
Теория сингулярного разложения была создана Джеймсом Джозефом Сильвестром в 1889 году и изложена во всех подробных руководствах по теории матриц.

Простой итерационный алгоритм сингулярного разложения
Основная процедура — поиск наилучшего приближения произвольной 
  
    
      
        m
        ×
        n
      
    
    {\displaystyle m\times n}
   матрицы 
  
    
      
        X
        =
        (
        
          x
          
            i
            j
          
        
        )
      
    
    {\displaystyle X=(x_{ij})}
   матрицей вида 
  
    
      
        b
        ⊗
        a
        =
        (
        
          b
          
            i
          
        
        
          a
          
            j
          
        
        )
      
    
    {\displaystyle b\otimes a=(b_{i}a_{j})}
   (где 
  
    
      
        b
      
    
    {\displaystyle b}
   — 
  
    
      
        m
      
    
    {\displaystyle m}
  -мерный вектор, а 
  
    
      
        a
      
    
    {\displaystyle a}
   — 
  
    
      
        n
      
    
    {\displaystyle n}
  -мерный вектор) методом наименьших квадратов:

  
    
      
        F
        (
        b
        ,
        a
        )
        =
        
          
            1
            2
          
        
        
          ∑
          
            i
            =
            1
          
          
            m
          
        
        
          ∑
          
            j
            =
            1
          
          
            n
          
        
        (
        
          x
          
            i
            j
          
        
        −
        
          b
          
            i
          
        
        
          a
          
            j
          
        
        
          )
          
            2
          
        
        →
        min
      
    
    {\displaystyle F(b,a)={\frac {1}{2}}\sum _{i=1}^{m}\sum _{j=1}^{n}(x_{ij}-b_{i}a_{j})^{2}\to \min }
  Решение этой задачи дается последовательными итерациями по явным формулам. При фиксированном векторе 
  
    
      
        a
        =
        (
        
          a
          
            j
          
        
        )
      
    
    {\displaystyle a=(a_{j})}
   значения 
  
    
      
        b
        =
        (
        
          b
          
            i
          
        
        )
      
    
    {\displaystyle b=(b_{i})}
  , доставляющие минимум форме 
  
    
      
        F
        (
        b
        ,
        a
        )
      
    
    {\displaystyle F(b,a)}
  , однозначно и явно определяются из равенств 
  
    
      
        ∂
        F
        
          /
        
        ∂
        
          b
          
            i
          
        
        =
        0
      
    
    {\displaystyle \partial F/\partial b_{i}=0}
   :

  
    
      
        
          
            
              ∂
              F
            
            
              ∂
              
                b
                
                  i
                
              
            
          
        
        =
        −
        
          ∑
          
            j
            =
            1
          
          
            n
          
        
        (
        
          x
          
            i
            j
          
        
        −
        
          b
          
            i
          
        
        
          a
          
            j
          
        
        )
        
          a
          
            j
          
        
        =
        0
        ;
        
        
        
          b
          
            i
          
        
        =
        
          
            
              
                ∑
                
                  j
                  =
                  1
                
                
                  n
                
              
              
                x
                
                  i
                  j
                
              
              
                a
                
                  j
                
              
            
            
              
                ∑
                
                  j
                  =
                  1
                
                
                  n
                
              
              
                a
                
                  j
                
                
                  2
                
              
            
          
        
        
        .
      
    
    {\displaystyle {\frac {\partial F}{\partial b_{i}}}=-\sum _{j=1}^{n}(x_{ij}-b_{i}a_{j})a_{j}=0;\;\;b_{i}={\frac {\sum _{j=1}^{n}x_{ij}a_{j}}{\sum _{j=1}^{n}a_{j}^{2}}}\,.}
  Аналогично, при фиксированном векторе 
  
    
      
        b
        =
        (
        
          b
          
            i
          
        
        )
      
    
    {\displaystyle b=(b_{i})}
   определяются значения 
  
    
      
        a
        =
        (
        
          a
          
            j
          
        
        )
      
    
    {\displaystyle a=(a_{j})}
  :

  
    
      
        
          a
          
            j
          
        
        =
        
          
            
              
                ∑
                
                  i
                  =
                  1
                
                
                  m
                
              
              
                b
                
                  i
                
              
              
                x
                
                  i
                  j
                
              
            
            
              
                ∑
                
                  i
                  =
                  1
                
                
                  m
                
              
              
                b
                
                  i
                
                
                  2
                
              
            
          
        
        
        .
      
    
    {\displaystyle a_{j}={\frac {\sum _{i=1}^{m}b_{i}x_{ij}}{\sum _{i=1}^{m}b_{i}^{2}}}\,.}
  B качестве начального приближения вектора 
  
    
      
        a
      
    
    {\displaystyle a}
   берётся случайный вектор единичной длины, вычисляем вектор 
  
    
      
        b
      
    
    {\displaystyle b}
  , далее для этого вектора 
  
    
      
        b
      
    
    {\displaystyle b}
   вычисляем вектор 
  
    
      
        a
      
    
    {\displaystyle a}
   и т. д. Каждый шаг уменьшает значение 
  
    
      
        F
        (
        b
        ,
        a
        )
      
    
    {\displaystyle F(b,a)}
  . В качестве критерия остановки используется малость относительного уменьшения значения минимизируемого функционала 
  
    
      
        F
        (
        b
        ,
        a
        )
      
    
    {\displaystyle F(b,a)}
   за шаг итерации (
  
    
      
        Δ
        F
        
          /
        
        F
      
    
    {\displaystyle \Delta F/F}
  ) или малость самого значения 
  
    
      
        F
      
    
    {\displaystyle F}
  .
В результате для матрицы 
  
    
      
        X
        =
        (
        
          x
          
            i
            j
          
        
        )
      
    
    {\displaystyle X=(x_{ij})}
   получается наилучшее приближение матрицей 
  
    
      
        
          P
          
            1
          
        
      
    
    {\displaystyle P_{1}}
   вида 
  
    
      
        
          b
          
            1
          
        
        ⊗
        
          a
          
            1
          
        
        =
        (
        
          b
          
            i
          
          
            1
          
        
        
          a
          
            j
          
          
            1
          
        
        )
      
    
    {\displaystyle b^{1}\otimes a^{1}=(b_{i}^{1}a_{j}^{1})}
   (здесь верхним индексом обозначен номер приближения). Далее, из матрицы 
  
    
      
        X
      
    
    {\displaystyle X}
   вычитается полученная матрицу 
  
    
      
        
          P
          
            1
          
        
      
    
    {\displaystyle P_{1}}
  , и для полученной матрицы уклонений 
  
    
      
        
          X
          
            1
          
        
        =
        X
        −
        
          P
          
            1
          
        
      
    
    {\displaystyle X_{1}=X-P_{1}}
   вновь ищется наилучшее приближение 
  
    
      
        
          P
          
            2
          
        
      
    
    {\displaystyle P_{2}}
   этого же вида и т. д., пока, например, норма 
  
    
      
        
          X
          
            k
          
        
      
    
    {\displaystyle X_{k}}
   не станет достаточно малой. В результате получили итерационную процедуру разложения матрицы 
  
    
      
        X
      
    
    {\displaystyle X}
   в виде суммы матриц ранга 1, то есть 
  
    
      
        X
        =
        
          P
          
            1
          
        
        +
        
          P
          
            2
          
        
        +
        …
        +
        
          P
          
            q
          
        
        
        (
        
          P
          
            l
          
        
        =
        
          b
          
            l
          
        
        ⊗
        
          a
          
            l
          
        
        )
      
    
    {\displaystyle X=P_{1}+P_{2}+\ldots +P_{q}\;(P_{l}=b^{l}\otimes a^{l})}
   . Полагаем 
  
    
      
        
          σ
          
            l
          
        
        =
        ‖
        
          a
          
            l
          
        
        ‖
        ‖
        
          b
          
            l
          
        
        ‖
      
    
    {\displaystyle \sigma _{l}=\|a^{l}\|\|b^{l}\|}
   и нормируем векторы 
  
    
      
        
          a
          
            l
          
        
        
        ,
        
        
          b
          
            l
          
        
      
    
    {\displaystyle a^{l}\,,\,b^{l}}
  : 
  
    
      
        
          a
          
            l
          
        
        :=
        
          a
          
            l
          
        
        
          /
        
        ‖
        
          a
          
            l
          
        
        ‖
        ;
        
        
        
          b
          
            l
          
        
        :=
        
          b
          
            l
          
        
        
          /
        
        ‖
        
          b
          
            l
          
        
        ‖
        .
      
    
    {\displaystyle a^{l}:=a^{l}/\|a^{l}\|;\,\,b^{l}:=b^{l}/\|b^{l}\|.}
   В результате получена аппроксимация сингулярных чисел 
  
    
      
        
          σ
          
            l
          
        
      
    
    {\displaystyle \sigma _{l}}
   и сингулярных векторов (правых — 
  
    
      
        
          a
          
            l
          
        
      
    
    {\displaystyle a^{l}}
   и левых — 
  
    
      
        
          b
          
            l
          
        
      
    
    {\displaystyle b^{l}}
  ).
К достоинствам этого алгоритма относится его исключительная простота и возможность почти без изменений перенести его на данные с пробелами, а также взвешенные данные.
Существуют различные модификации базового алгоритма, улучшающие точность и устойчивость. Например, векторы главных компонент 
  
    
      
        
          a
          
            l
          
        
      
    
    {\displaystyle a^{l}}
   при разных 
  
    
      
        l
      
    
    {\displaystyle l}
   должны быть ортогональны «по построению», однако при большом числе итерации (большая размерность, много компонент) малые отклонения от ортогональности накапливаются и может потребоваться специальная коррекция 
  
    
      
        
          a
          
            l
          
        
      
    
    {\displaystyle a^{l}}
   на каждом шаге, обеспечивающая его ортогональность ранее найденным главным компонентам.
Для квадратных симметричных положительно определённых матриц описанный алгоритм превращается в метод прямых итераций для поиска собственных векторов (см. статью Собственные векторы, значения и пространства).

Сингулярное разложение тензоров и тензорный метод главных компонент
Часто вектор данных имеет дополнительную структуру прямоугольной таблицы (например, плоское изображение) или даже многомерной таблицы — то есть тензора: 
  
    
      
        
          x
          
            
              i
              
                1
              
            
            
              i
              
                2
              
            
            .
            .
            .
            
              i
              
                q
              
            
          
        
      
    
    {\displaystyle x_{i_{1}i_{2}...i_{q}}}
  , 
  
    
      
        1
        ≤
        
          i
          
            j
          
        
        ≤
        
          n
          
            j
          
        
      
    
    {\displaystyle 1\leq i_{j}\leq n_{j}}
  . В этом случае также эффективно применять сингулярное разложение. Определение, основные формулы и алгоритмы переносятся практически без изменений: вместо матрицы данных имеем 
  
    
      
        q
        +
        1
      
    
    {\displaystyle q+1}
  -индексную величину 
  
    
      
        X
        =
        (
        
          x
          
            
              i
              
                0
              
            
            
              i
              
                1
              
            
            
              i
              
                2
              
            
            .
            .
            .
            
              i
              
                q
              
            
          
        
        )
      
    
    {\displaystyle \operatorname {X} =(x_{i_{0}i_{1}i_{2}...i_{q}})}
  , где первый индекс 
  
    
      
        
          i
          
            0
          
        
      
    
    {\displaystyle i_{0}}
  -номер точки (тензора) данных.
Основная процедура — поиск наилучшего приближения тензора 
  
    
      
        
          x
          
            
              i
              
                0
              
            
            
              i
              
                1
              
            
            
              i
              
                2
              
            
            .
            .
            .
            
              i
              
                q
              
            
          
        
      
    
    {\displaystyle x_{i_{0}i_{1}i_{2}...i_{q}}}
   тензором вида 
  
    
      
        
          a
          
            
              i
              
                0
              
            
          
          
            0
          
        
        
          a
          
            
              i
              
                1
              
            
          
          
            1
          
        
        
          a
          
            
              i
              
                2
              
            
          
          
            2
          
        
        .
        .
        .
        
          a
          
            
              i
              
                q
              
            
          
          
            q
          
        
      
    
    {\displaystyle a_{i_{0}}^{0}a_{i_{1}}^{1}a_{i_{2}}^{2}...a_{i_{q}}^{q}}
   (где 
  
    
      
        
          a
          
            0
          
        
        =
        (
        
          a
          
            
              i
              
                0
              
            
          
          
            0
          
        
        )
      
    
    {\displaystyle a^{0}=(a_{i_{0}}^{0})}
   — 
  
    
      
        m
      
    
    {\displaystyle m}
  -мерный вектор (
  
    
      
        m
      
    
    {\displaystyle m}
   — число точек данных), 
  
    
      
        
          a
          
            l
          
        
        =
        (
        
          a
          
            
              i
              
                l
              
            
          
          
            l
          
        
        )
      
    
    {\displaystyle a^{l}=(a_{i_{l}}^{l})}
   — вектор размерности 
  
    
      
        
          n
          
            l
          
        
      
    
    {\displaystyle n_{l}}
   при 
  
    
      
        l
        >
        0
      
    
    {\displaystyle l>0}
  ) методом наименьших квадратов:

  
    
      
        F
        =
        
          
            1
            2
          
        
        
          ∑
          
            
              i
              
                0
              
            
            =
            1
          
          
            m
          
        
        
          ∑
          
            
              i
              
                1
              
            
            =
            1
          
          
            
              n
              
                1
              
            
          
        
        .
        .
        .
        
          ∑
          
            
              i
              
                q
              
            
            =
            1
          
          
            
              n
              
                q
              
            
          
        
        (
        
          x
          
            
              i
              
                0
              
            
            
              i
              
                1
              
            
            .
            .
            .
            
              i
              
                q
              
            
          
        
        −
        
          a
          
            
              i
              
                0
              
            
          
          
            0
          
        
        
          a
          
            
              i
              
                1
              
            
          
          
            1
          
        
        .
        .
        .
        
          a
          
            
              i
              
                q
              
            
          
          
            q
          
        
        
          )
          
            2
          
        
        →
        min
      
    
    {\displaystyle F={\frac {1}{2}}\sum _{i_{0}=1}^{m}\sum _{i_{1}=1}^{n_{1}}...\sum _{i_{q}=1}^{n_{q}}(x_{i_{0}i_{1}...i_{q}}-a_{i_{0}}^{0}a_{i_{1}}^{1}...a_{i_{q}}^{q})^{2}\to \min }
  Решение этой задачи дается последовательными итерациями по явным формулам. Если заданы все векторы-сомножители кроме одного 
  
    
      
        
          a
          
            
              i
              
                k
              
            
          
          
            k
          
        
      
    
    {\displaystyle a_{i_{k}}^{k}}
  , то этот оставшийся определяется явно из достаточных условий минимума.

  
    
      
        
          a
          
            
              i
              
                k
              
            
          
          
            k
          
        
        =
        
          
            
              
                ∑
                
                  
                    i
                    
                      0
                    
                  
                  =
                  1
                
                
                  m
                
              
              
                ∑
                
                  
                    i
                    
                      1
                    
                  
                  =
                  1
                
                
                  
                    n
                    
                      1
                    
                  
                
              
              .
              .
              .
              
                ∑
                
                  
                    i
                    
                      k
                      −
                      1
                    
                  
                  =
                  1
                
                
                  
                    n
                    
                      k
                      −
                      1
                    
                  
                
              
              
                ∑
                
                  
                    i
                    
                      k
                      +
                      1
                    
                  
                  =
                  1
                
                
                  
                    n
                    
                      k
                      +
                      1
                    
                  
                
              
              .
              .
              .
              
                ∑
                
                  
                    i
                    
                      q
                    
                  
                  =
                  1
                
                
                  
                    n
                    
                      q
                    
                  
                
              
              
                x
                
                  
                    i
                    
                      0
                    
                  
                  
                    i
                    
                      1
                    
                  
                  .
                  .
                  .
                  
                    i
                    
                      k
                      −
                      1
                    
                  
                  
                    i
                    
                      k
                    
                  
                  
                    i
                    
                      k
                      +
                      1
                    
                  
                  .
                  .
                  .
                  
                    i
                    
                      q
                    
                  
                
              
              
                a
                
                  
                    i
                    
                      0
                    
                  
                
                
                  0
                
              
              
                a
                
                  
                    i
                    
                      k
                      −
                      1
                    
                  
                
                
                  k
                  −
                  1
                
              
              
                a
                
                  
                    i
                    
                      k
                      +
                      1
                    
                  
                
                
                  k
                  +
                  1
                
              
              .
              .
              .
              
                a
                
                  
                    i
                    
                      q
                    
                  
                
                
                  q
                
              
            
            
              
                ∏
                
                  j
                  ≠
                  k
                
              
              ‖
              
                a
                
                  j
                
              
              
                ‖
                
                  2
                
              
            
          
        
        
        .
      
    
    {\displaystyle a_{i_{k}}^{k}={\frac {\sum _{i_{0}=1}^{m}\sum _{i_{1}=1}^{n_{1}}...\sum _{i_{k-1}=1}^{n_{k-1}}\sum _{i_{k+1}=1}^{n_{k+1}}...\sum _{i_{q}=1}^{n_{q}}x_{i_{0}i_{1}...i_{k-1}i_{k}i_{k+1}...i_{q}}a_{i_{0}}^{0}a_{i_{k-1}}^{k-1}a_{i_{k+1}}^{k+1}...a_{i_{q}}^{q}}{\prod _{j\neq k}\|a^{j}\|^{2}}}\,.}
  B качестве начального приближения векторов 
  
    
      
        
          a
          
            l
          
        
        =
        (
        
          a
          
            
              i
              
                l
              
            
          
          
            l
          
        
        )
      
    
    {\displaystyle a^{l}=(a_{i_{l}}^{l})}
   (
  
    
      
        l
        >
        0
      
    
    {\displaystyle l>0}
  ) берутся случайные векторы единичной длины, вычислим вектор 
  
    
      
        
          a
          
            0
          
        
      
    
    {\displaystyle a^{0}}
  , далее для этого вектора 
  
    
      
        
          a
          
            0
          
        
      
    
    {\displaystyle a^{0}}
   и данных векторов 
  
    
      
        
          a
          
            2
          
        
        ,
        
          a
          
            3
          
        
        ,
        .
        .
        .
      
    
    {\displaystyle a^{2},a^{3},...}
   вычисляется вектор 
  
    
      
        
          a
          
            1
          
        
      
    
    {\displaystyle a^{1}}
   и так далее (циклически перебирая индексы). Каждый шаг уменьшает значение 
  
    
      
        F
        (
        b
        ,
        a
        )
      
    
    {\displaystyle F(b,a)}
  . Алгоритм, очевидно, сходится. В качестве критерия остановки используется малость относительного уменьшения значения минимизируемого функционала 
  
    
      
        F
      
    
    {\displaystyle F}
   за цикл или малость самого значения 
  
    
      
        F
      
    
    {\displaystyle F}
  . Далее, из тензора 
  
    
      
        X
      
    
    {\displaystyle \operatorname {X} }
   вычитается полученное приближение 
  
    
      
        
          a
          
            
              i
              
                0
              
            
          
          
            0
          
        
        
          a
          
            
              i
              
                1
              
            
          
          
            1
          
        
        
          a
          
            
              i
              
                2
              
            
          
          
            2
          
        
        .
        .
        .
        
          a
          
            
              i
              
                q
              
            
          
          
            q
          
        
      
    
    {\displaystyle a_{i_{0}}^{0}a_{i_{1}}^{1}a_{i_{2}}^{2}...a_{i_{q}}^{q}}
   и для остатка вновь ищется наилучшее приближение этого же вида и т. д., пока, например, норма очередного остатка не станет достаточно малой.
Это многокомпонентное сингулярное разложение (тензорный метод главных компонент) успешно применяется при обработке изображений, видеосигналов, и, шире, любых данных, имеющих табличную или тензорную структуру.

Матрица преобразования к главным компонентам
Матрица 
  
    
      
        A
      
    
    {\displaystyle A}
   преобразования данных к главным компонентам состоит из векторов главных компонент, расположенных в порядке убывания собственных значений:

  
    
      
        A
        =
        
          
            {
            
              
                a
                
                  1
                
              
              ,
              .
              .
              .
              ,
              
                a
                
                  n
                
              
            
            }
          
          
            T
          
        
      
    
    {\displaystyle A=\left\{a_{1},...,a_{n}\right\}^{T}}
   (
  
    
      
        
          
            
          
          
            T
          
        
      
    
    {\displaystyle {\,}^{T}}
   означает транспонирование),причём

  
    
      
        A
        
          A
          
            T
          
        
        =
        1.
      
    
    {\displaystyle AA^{T}=1.}
  То есть, матрица 
  
    
      
        A
      
    
    {\displaystyle A}
   является ортогональной.
Большая часть вариации данных будет сосредоточена в первых координатах, что позволяет перейти к пространству меньшей размерности.

Остаточная дисперсия
Пусть данные центрированы, 
  
    
      
        
          
            X
            ¯
          
        
        =
        0
      
    
    {\displaystyle {\overline {X}}=0}
  . При замене векторов данных 
  
    
      
        
          x
          
            i
          
        
      
    
    {\displaystyle x_{i}}
   на их проекцию на первые 
  
    
      
        k
      
    
    {\displaystyle k}
   главных компонент 
  
    
      
        
          x
          
            i
          
        
        ↦
        
          ∑
          
            j
            =
            1
          
          
            k
          
        
        
          a
          
            j
          
        
        (
        
          a
          
            j
          
        
        ,
        
          x
          
            i
          
        
        )
      
    
    {\displaystyle x_{i}\mapsto \sum _{j=1}^{k}a_{j}(a_{j},x_{i})}
   вносится средний квадрат ошибки в расчете на один вектор данных:

  
    
      
        
          
            1
            m
          
        
        
          ∑
          
            i
            =
            1
          
          
            m
          
        
        
          
            ‖
            
              
                x
                
                  i
                
              
              −
              
                ∑
                
                  j
                  =
                  1
                
                
                  k
                
              
              
                a
                
                  j
                
              
              (
              
                a
                
                  j
                
              
              ,
              
                x
                
                  i
                
              
              )
            
            ‖
          
          
            2
          
        
        =
        
          ∑
          
            l
            =
            k
            +
            1
          
          
            n
          
        
        
          λ
          
            l
          
        
        ,
      
    
    {\displaystyle {\frac {1}{m}}\sum _{i=1}^{m}\left\Vert x_{i}-\sum _{j=1}^{k}a_{j}(a_{j},x_{i})\right\Vert ^{2}=\sum _{l=k+1}^{n}\lambda _{l},}
  где 
  
    
      
        
          λ
          
            1
          
        
        ≥
        
          λ
          
            2
          
        
        ≥
        …
        ≥
        
          λ
          
            n
          
        
        ≥
        0
      
    
    {\displaystyle \lambda _{1}\geq \lambda _{2}\geq \ldots \geq \lambda _{n}\geq 0}
   собственные значения эмпирической ковариационной матрицы 
  
    
      
        C
      
    
    {\displaystyle C}
  , расположенные в порядке убывания, с учётом кратности.
Эта величина называется остаточной дисперсией. Величина

  
    
      
        
          
            1
            m
          
        
        
          ∑
          
            i
            =
            1
          
          
            m
          
        
        
          
            ‖
            
              
                ∑
                
                  j
                  =
                  1
                
                
                  k
                
              
              
                a
                
                  j
                
              
              (
              
                a
                
                  j
                
              
              ,
              
                x
                
                  i
                
              
              )
            
            ‖
          
          
            2
          
        
        =
        
          
            1
            m
          
        
        
          ∑
          
            i
            =
            1
          
          
            m
          
        
        
          ∑
          
            j
            =
            1
          
          
            k
          
        
        (
        
          a
          
            j
          
        
        ,
        
          x
          
            i
          
        
        
          )
          
            2
          
        
        =
        
          ∑
          
            l
            =
            1
          
          
            k
          
        
        
          λ
          
            l
          
        
      
    
    {\displaystyle {\frac {1}{m}}\sum _{i=1}^{m}\left\Vert \sum _{j=1}^{k}a_{j}(a_{j},x_{i})\right\Vert ^{2}={\frac {1}{m}}\sum _{i=1}^{m}\sum _{j=1}^{k}(a_{j},x_{i})^{2}=\sum _{l=1}^{k}\lambda _{l}}
  называется объяснённой дисперсией. Их сумма равна выборочной дисперсии. Соответствующий квадрат относительной ошибки — это отношение остаточной дисперсии к выборочной дисперсии (то есть доля необъяснённой дисперсии):

  
    
      
        
          δ
          
            k
          
          
            2
          
        
        =
        
          
            
              
                λ
                
                  k
                  +
                  1
                
              
              +
              
                λ
                
                  k
                  +
                  2
                
              
              +
              …
              +
              
                λ
                
                  n
                
              
            
            
              
                λ
                
                  1
                
              
              +
              
                λ
                
                  2
                
              
              +
              …
              +
              
                λ
                
                  n
                
              
            
          
        
        .
      
    
    {\displaystyle \delta _{k}^{2}={\frac {\lambda _{k+1}+\lambda _{k+2}+\ldots +\lambda _{n}}{\lambda _{1}+\lambda _{2}+\ldots +\lambda _{n}}}.}
  По относительной ошибке 
  
    
      
        
          δ
          
            k
          
        
      
    
    {\displaystyle \delta _{k}}
   оценивается применимость метода главных компонент с проецированием на первые 
  
    
      
        k
      
    
    {\displaystyle k}
   компонент.
Замечание: в большинстве вычислительных алгоритмов собственные числа 
  
    
      
        
          λ
          
            i
          
        
      
    
    {\displaystyle \lambda _{i}}
   с соответствующими собственными векторами — главными компонентами 
  
    
      
        
          a
          
            i
          
        
      
    
    {\displaystyle a_{i}}
   вычисляются в порядке «от больших 
  
    
      
        
          λ
          
            i
          
        
      
    
    {\displaystyle \lambda _{i}}
   — к меньшим». Для вычисления 
  
    
      
        
          δ
          
            k
          
        
      
    
    {\displaystyle \delta _{k}}
   достаточно вычислить первые 
  
    
      
        k
      
    
    {\displaystyle k}
   собственных чисел и след эмпирической ковариационной матрицы 
  
    
      
        C
      
    
    {\displaystyle C}
  , 
  
    
      
        tr
        ⁡
        C
      
    
    {\displaystyle \operatorname {tr} C}
   (сумму диагональных элементов 
  
    
      
        C
      
    
    {\displaystyle C}
  , то есть дисперсий по осям). Тогда

  
    
      
        
          δ
          
            k
          
          
            2
          
        
        =
        
          
            1
            
              tr
              ⁡
              C
            
          
        
        
          (
          
            tr
            ⁡
            C
            −
            
              ∑
              
                i
                =
                1
              
              
                k
              
            
            
              λ
              
                i
              
            
          
          )
        
        .
      
    
    {\displaystyle \delta _{k}^{2}={\frac {1}{\operatorname {tr} C}}\left(\operatorname {tr} C-\sum _{i=1}^{k}\lambda _{i}\right).}

Отбор главных компонент по правилу Кайзера
Целевой подход к оценке числа главных компонент по необходимой доле объяснённой дисперсии формально применим всегда, однако неявно он предполагает, что нет разделения на «сигнал» и «шум», и любая заранее заданная точность имеет смысл. Поэтому часто более продуктивна иная эвристика, основывающаяся на гипотезе о наличии «сигнала» (сравнительно малая размерность, относительно большая амплитуда) и «шума» (большая размерность, относительно малая амплитуда). С этой точки зрения метод главных компонент работает как фильтр: сигнал содержится, в основном, в проекции на первые главные компоненты, а в остальных компонентах пропорция шума намного выше.
Вопрос: как оценить число необходимых главных компонент, если отношение «сигнал/шум» заранее неизвестно?
Простейший и старейший метод отбора главных компонент даёт правило Кайзера (англ. Kaiser's rule): значимы те главные компоненты, для которых

  
    
      
        
          λ
          
            i
          
        
        >
        
          
            1
            n
          
        
        tr
        ⁡
        C
        ,
      
    
    {\displaystyle \lambda _{i}>{\frac {1}{n}}\operatorname {tr} C,}
  то есть 
  
    
      
        
          λ
          
            i
          
        
      
    
    {\displaystyle \lambda _{i}}
   превосходит среднее значение 
  
    
      
        λ
      
    
    {\displaystyle \lambda }
   (среднюю выборочную дисперсию координат вектора данных). Правило Кайзера хорошо работает в простых случаях, когда есть несколько главных компонент с 
  
    
      
        
          λ
          
            i
          
        
      
    
    {\displaystyle \lambda _{i}}
  , намного превосходящими среднее значение, а остальные собственные числа меньше него. В более сложных случаях оно может давать слишком много значимых главных компонент. Если данные нормированы на единичную выборочную дисперсию по осям, то правило Кайзера приобретает особо простой вид: значимы только те главные компоненты, для которых 
  
    
      
        
          λ
          
            i
          
        
        >
        1.
      
    
    {\displaystyle \lambda _{i}>1.}

Оценка числа главных компонент по правилу сломанной трости
Одним из наиболее популярных эвристических подходов к оценке числа необходимых главных компонент является правило сломанной трости (англ. Broken stick model). Набор нормированных на единичную сумму собственных чисел (
  
    
      
        
          λ
          
            i
          
        
        
          /
        
        tr
        ⁡
        C
      
    
    {\displaystyle \lambda _{i}/\operatorname {tr} C}
  , 
  
    
      
        i
        =
        1
        ,
        .
        .
        .
        n
      
    
    {\displaystyle i=1,...n}
  ) сравнивается с распределением длин обломков трости единичной длины, сломанной в 
  
    
      
        n
        −
        1
      
    
    {\displaystyle n-1}
  -й случайно выбранной точке (точки разлома выбираются независимо и равнораспределены по длине трости). Пусть 
  
    
      
        
          L
          
            i
          
        
      
    
    {\displaystyle L_{i}}
   (
  
    
      
        i
        =
        1
        ,
        .
        .
        .
        n
      
    
    {\displaystyle i=1,...n}
  ) — длины полученных кусков трости, занумерованные в порядке убывания длины: 
  
    
      
        
          L
          
            1
          
        
        ≥
        
          L
          
            2
          
        
        ≥
        .
        .
        .
        
          L
          
            n
          
        
      
    
    {\displaystyle L_{1}\geq L_{2}\geq ...L_{n}}
  . Нетрудно найти математическое ожидание 
  
    
      
        
          L
          
            i
          
        
      
    
    {\displaystyle L_{i}}
  :

  
    
      
        
          l
          
            i
          
        
        =
        E
        ⁡
        (
        
          L
          
            i
          
        
        )
        =
        
          
            1
            n
          
        
        
          ∑
          
            j
            =
            i
          
          
            n
          
        
        
          
            1
            j
          
        
        .
      
    
    {\displaystyle l_{i}=\operatorname {E} (L_{i})={\frac {1}{n}}\sum _{j=i}^{n}{\frac {1}{j}}.}
  По правилу сломанной трости 
  
    
      
        k
      
    
    {\displaystyle k}
  -й собственный вектор (в порядке убывания собственных чисел 
  
    
      
        
          λ
          
            i
          
        
      
    
    {\displaystyle \lambda _{i}}
  ) сохраняется в списке главных компонент, если

  
    
      
        
          
            
              λ
              
                1
              
            
            
              tr
              ⁡
              C
            
          
        
        >
        
          l
          
            1
          
        
        
        a
        n
        d
        
        
          
            
              λ
              
                2
              
            
            
              tr
              ⁡
              C
            
          
        
        >
        
          l
          
            2
          
        
        
        a
        n
        d
        
        .
        .
        .
        
          
            
              λ
              
                k
              
            
            
              tr
              ⁡
              C
            
          
        
        >
        
          l
          
            k
          
        
        .
      
    
    {\displaystyle {\frac {\lambda _{1}}{\operatorname {tr} C}}>l_{1}\;and\;{\frac {\lambda _{2}}{\operatorname {tr} C}}>l_{2}\;and\;...{\frac {\lambda _{k}}{\operatorname {tr} C}}>l_{k}.}
  На Рис. приведён пример для 5-мерного случая:

  
    
      
        
          l
          
            1
          
        
      
    
    {\displaystyle l_{1}}
  =(1+1/2+1/3+1/4+1/5)/5; 
  
    
      
        
          l
          
            2
          
        
      
    
    {\displaystyle l_{2}}
  =(1/2+1/3+1/4+1/5)/5; 
  
    
      
        
          l
          
            3
          
        
      
    
    {\displaystyle l_{3}}
  =(1/3+1/4+1/5)/5; 
  
    
      
        
          l
          
            4
          
        
      
    
    {\displaystyle l_{4}}
  =(1/4+1/5)/5; 
  
    
      
        
          l
          
            5
          
        
      
    
    {\displaystyle l_{5}}
  =(1/5)/5.Для примера выбрано

  
    
      
        
          
            
              λ
              
                1
              
            
            
              tr
              ⁡
              C
            
          
        
      
    
    {\displaystyle {\frac {\lambda _{1}}{\operatorname {tr} C}}}
  =0.5; 
  
    
      
        
          
            
              λ
              
                2
              
            
            
              tr
              ⁡
              C
            
          
        
      
    
    {\displaystyle {\frac {\lambda _{2}}{\operatorname {tr} C}}}
  =0.3; 
  
    
      
        
          
            
              λ
              
                3
              
            
            
              tr
              ⁡
              C
            
          
        
      
    
    {\displaystyle {\frac {\lambda _{3}}{\operatorname {tr} C}}}
  =0.1; 
  
    
      
        
          
            
              λ
              
                4
              
            
            
              tr
              ⁡
              C
            
          
        
      
    
    {\displaystyle {\frac {\lambda _{4}}{\operatorname {tr} C}}}
  =0.06; 
  
    
      
        
          
            
              λ
              
                5
              
            
            
              tr
              ⁡
              C
            
          
        
      
    
    {\displaystyle {\frac {\lambda _{5}}{\operatorname {tr} C}}}
  =0.04.По правилу сломанной трости в этом примере следует оставлять 2 главных компоненты:

  
    
      
        
          
            
              λ
              
                1
              
            
            
              tr
              ⁡
              C
            
          
        
        >
        
          l
          
            1
          
        
        
        ;
        
        
          
            
              λ
              
                2
              
            
            
              tr
              ⁡
              C
            
          
        
        >
        
          l
          
            2
          
        
        
        ;
        
        
          
            
              λ
              
                3
              
            
            
              tr
              ⁡
              C
            
          
        
        <
        
          l
          
            3
          
        
        
        .
      
    
    {\displaystyle {\frac {\lambda _{1}}{\operatorname {tr} C}}>l_{1}\;;\;{\frac {\lambda _{2}}{\operatorname {tr} C}}>l_{2}\;;\;{\frac {\lambda _{3}}{\operatorname {tr} C}}<l_{3}\;.}
  По оценкам пользователей, правило сломанной трости имеет тенденцию занижать количество значимых главных компонент.

Нормировка
Нормировка после приведения к главным компонентам
После проецирования на первые 
  
    
      
        k
      
    
    {\displaystyle k}
   главных компонент с 
  
    
      
        
          λ
          
            1
          
        
        ≥
        
          λ
          
            2
          
        
        ≥
        …
        ≥
        
          λ
          
            k
          
        
        >
        0
      
    
    {\displaystyle \lambda _{1}\geq \lambda _{2}\geq \ldots \geq \lambda _{k}>0}
   удобно произвести нормировку на единичную (выборочную) дисперсию по осям. Дисперсия вдоль 
  
    
      
        i
      
    
    {\displaystyle i}
  й главной компоненты равна

  
    
      
        
          λ
          
            i
          
        
        >
        0
        
        (
        1
        ≤
        i
        ≤
        k
      
    
    {\displaystyle \lambda _{i}>0\;(1\leq i\leq k}
  ), поэтому для нормировки надо разделить соответствующую координату на 
  
    
      
        
          
            
              λ
              
                i
              
            
          
        
      
    
    {\displaystyle {\sqrt {\lambda _{i}}}}
  . Это преобразование не является ортогональным и не сохраняет скалярного произведения. Ковариационная матрица проекции данных после нормировки становится единичной, проекции на любые два ортогональных направления становятся независимыми величинами, а любой ортонормированный базис становится базисом главных компонент (напомним, что нормировка меняет отношение ортогональности векторов). Отображение из пространства исходных данных на первые 
  
    
      
        k
      
    
    {\displaystyle k}
   главных компонент вместе с нормировкой задается матрицей

  
    
      
        K
        =
        
          
            {
            
              
                
                  
                    a
                    
                      1
                    
                  
                  
                    
                      λ
                      
                        1
                      
                    
                  
                
              
              ,
              
                
                  
                    a
                    
                      2
                    
                  
                  
                    
                      λ
                      
                        2
                      
                    
                  
                
              
              ,
              .
              .
              .
              ,
              
                
                  
                    a
                    
                      k
                    
                  
                  
                    
                      λ
                      
                        k
                      
                    
                  
                
              
            
            }
          
          
            T
          
        
      
    
    {\displaystyle K=\left\{{\frac {a_{1}}{\sqrt {\lambda _{1}}}},{\frac {a_{2}}{\sqrt {\lambda _{2}}}},...,{\frac {a_{k}}{\sqrt {\lambda _{k}}}}\right\}^{T}}
  .Именно это преобразование чаще всего называется преобразованием Кархунена — Лоэва. Здесь 
  
    
      
        
          a
          
            i
          
        
      
    
    {\displaystyle a_{i}}
   — векторы-столбцы, а верхний индекс 
  
    
      
        T
      
    
    {\displaystyle T}
   означает транспонирование.

Нормировка до вычисления главных компонент
Предупреждение: не следует путать нормировку, проводимую после преобразования к главным компонентам, с нормировкой и «обезразмериванием» при предобработке данных, проводимой до вычисления главных компонент. Предварительная нормировка нужна для обоснованного выбора метрики, в которой будет вычисляться наилучшая аппроксимация данных, или будут искаться направления наибольшего разброса (что эквивалентно). Например, если данные представляют собой трёхмерные векторы из «метров, литров и килограммов», то при использовании стандартного евклидового расстояния разница в 1 метр по первой координате будет вносить тот же вклад, что разница в 1 литр по второй, или в 1 кг по третьей. Обычно системы единиц, в которых представлены исходные данные, недостаточно точно отображают наши представления о естественных масштабах по осям, и проводится «обезразмеривание»: каждая координата делится на некоторый масштаб, определяемый данными, целями их обработки и процессами измерения и сбора данных.
Есть три существенно различных стандартных подхода к такой нормировке: на единичную дисперсию по осям (масштабы по осям равны средним квадратичным уклонениям — после этого преобразования ковариационная матрица совпадает с матрицей коэффициентов корреляции), на равную точность измерения (масштаб по оси пропорционален точности измерения данной величины) и на равные требования в задаче (масштаб по оси определяется требуемой точностью прогноза данной величины или допустимым её искажением — уровнем толерантности). На выбор предобработки влияют содержательная постановка задачи, а также условия сбора данных (например, если коллекция данных принципиально не завершена и данные будут ещё поступать, то нерационально выбирать нормировку строго на единичную дисперсию, даже если это соответствует смыслу задачи, поскольку это предполагает перенормировку всех данных после получения новой порции; разумнее выбрать некоторый масштаб, грубо оценивающий стандартное отклонение, и далее его не менять).
Предварительная нормировка на единичную дисперсию по осям разрушается поворотом системы координат, если оси не являются главными компонентами, и нормировка при предобработке данных не заменяет нормировку после приведения к главным компонентам.

Механическая аналогия и метод главных компонент для взвешенных данных
Если сопоставить каждому вектору данных единичную массу, то эмпирическая ковариационная матрица

  
    
      
        C
      
    
    {\displaystyle C}
   совпадёт с тензором инерции этой системы точечных масс (делённым на полную массу 
  
    
      
        m
      
    
    {\displaystyle m}
  ), а задача о главных компонентах — с задачей приведения тензора инерции к главным осям. Можно использовать дополнительную свободу в выборе значений масс для учёта важности точек данных или надежности их значений (важным данным или данным из более надежных источников приписываются бо́льшие массы). Если вектору данных 
  
    
      
        
          x
          
            l
          
        
      
    
    {\displaystyle x_{l}}
   придаётся масса 
  
    
      
        
          w
          
            l
          
        
      
    
    {\displaystyle w_{l}}
  , то вместо эмпирической ковариационной матрицы 
  
    
      
        C
      
    
    {\displaystyle C}
   получим

  
    
      
        
          C
          
            w
          
        
        =
        [
        
          c
          
            i
            j
          
          
            w
          
        
        ]
        ,
         
        
          c
          
            i
            j
          
          
            w
          
        
        =
        
          
            1
            
              
                ∑
                
                  l
                
              
              
                w
                
                  l
                
              
            
          
        
        
          ∑
          
            l
            =
            1
          
          
            m
          
        
        
          w
          
            l
          
        
        (
        
          x
          
            l
            i
          
        
        −
        
          
            
              X
              
                i
              
            
            ¯
          
        
        )
        (
        
          x
          
            l
            j
          
        
        −
        
          
            
              X
              
                j
              
            
            ¯
          
        
        )
        .
      
    
    {\displaystyle C^{w}=[c_{ij}^{w}],\ c_{ij}^{w}={\frac {1}{\sum _{l}w_{l}}}\sum _{l=1}^{m}w_{l}(x_{li}-{\overline {X_{i}}})(x_{lj}-{\overline {X_{j}}}).}
  Все дальнейшие операции по приведению к главным компонентам производятся так же, как и в основной версии метода: ищется ортонормированный собственный базис 
  
    
      
        
          C
          
            w
          
        
      
    
    {\displaystyle C^{w}}
  , упорядочивается по убыванию собственных значений, оценивается средневзвешенная ошибка аппроксимации данных первыми 
  
    
      
        k
      
    
    {\displaystyle k}
   компонентами (по суммам собственных чисел 
  
    
      
        
          C
          
            w
          
        
      
    
    {\displaystyle C^{w}}
  ), проводится нормировка и так далее.
Более общий способ взвешивания даёт максимизация взвешенной суммы попарных расстояний между проекциями. Для каждых двух точек данных, 
  
    
      
        
          x
          
            l
          
        
        ,
         
        
          x
          
            q
          
        
      
    
    {\displaystyle x_{l},\ x_{q}}
   вводится вес 
  
    
      
        
          d
          
            l
            q
          
        
      
    
    {\displaystyle d_{lq}}
  ; 
  
    
      
        
          d
          
            l
            q
          
        
        =
        
          d
          
            q
            l
          
        
      
    
    {\displaystyle d_{lq}=d_{ql}}
   и 
  
    
      
        
          d
          
            l
          
        
        =
        
          ∑
          
            q
            =
            1
          
          
            m
          
        
        
          d
          
            l
            q
          
        
      
    
    {\displaystyle d_{l}=\sum _{q=1}^{m}d_{lq}}
  . Вместо эмпирической ковариационной матрицы 
  
    
      
        C
      
    
    {\displaystyle C}
   используется

  
    
      
        
          C
          
            d
          
        
        =
        [
        
          c
          
            i
            j
          
          
            d
          
        
        ]
        ,
         
        
          c
          
            i
            j
          
          
            d
          
        
        =
        
          ∑
          
            l
            =
            1
          
          
            m
          
        
        
          d
          
            l
          
        
        (
        
          x
          
            l
            i
          
        
        −
        
          
            
              X
              
                i
              
            
            ¯
          
        
        )
        (
        
          x
          
            l
            j
          
        
        −
        
          
            
              X
              
                j
              
            
            ¯
          
        
        )
        −
        
          ∑
          
            l
            ≠
            q
            ,
             
            l
            ,
            q
            =
            1
          
          
            m
          
        
        
          d
          
            l
            q
          
        
        (
        
          x
          
            l
            i
          
        
        −
        
          
            
              X
              
                i
              
            
            ¯
          
        
        )
        (
        
          x
          
            q
            j
          
        
        −
        
          
            
              X
              
                j
              
            
            ¯
          
        
        )
        .
      
    
    {\displaystyle C^{d}=[c_{ij}^{d}],\ c_{ij}^{d}=\sum _{l=1}^{m}d_{l}(x_{li}-{\overline {X_{i}}})(x_{lj}-{\overline {X_{j}}})-\sum _{l\neq q,\ l,q=1}^{m}d_{lq}(x_{li}-{\overline {X_{i}}})(x_{qj}-{\overline {X_{j}}}).}
  При 
  
    
      
        
          d
          
            l
            q
          
        
        >
        0
      
    
    {\displaystyle d_{lq}>0}
   симметричная матрица 
  
    
      
        
          C
          
            d
          
        
      
    
    {\displaystyle C^{d}}
   положительно определена, поскольку положительна квадратичная форма:

  
    
      
        
          ∑
          
            i
            j
          
        
        
          c
          
            i
            j
          
          
            d
          
        
        
          a
          
            i
          
        
        
          a
          
            j
          
        
        =
        
          
            1
            2
          
        
        
          ∑
          
            l
            q
          
        
        
          d
          
            l
            q
          
        
        
          
            (
            
              
                ∑
                
                  i
                
              
              
                a
                
                  i
                
              
              (
              
                x
                
                  l
                  i
                
              
              −
              
                x
                
                  q
                  i
                
              
              )
            
            )
          
          
            2
          
        
        .
      
    
    {\displaystyle \sum _{ij}c_{ij}^{d}a_{i}a_{j}={\frac {1}{2}}\sum _{lq}d_{lq}\left(\sum _{i}a_{i}(x_{li}-x_{qi})\right)^{2}.}
  Далее ищем ортонормированный собственный базис 
  
    
      
        
          C
          
            d
          
        
      
    
    {\displaystyle C^{d}}
  , упорядочиваем его по убыванию собственных значений, оцениваем средневзвешенную ошибку аппроксимации данных первыми 
  
    
      
        k
      
    
    {\displaystyle k}
   компонентами и т. д. — в точности так же, как и в основном алгоритме.
Этот способ применяется при наличии классов: для 
  
    
      
        
          x
          
            l
          
        
        ,
         
        
          x
          
            q
          
        
      
    
    {\displaystyle x_{l},\ x_{q}}
   из разных классов вес 
  
    
      
        
          d
          
            l
            q
          
        
      
    
    {\displaystyle d_{lq}}
   вес выбирается бо́льшим, чем для точек одного класса. В результате, в проекции на взвешенные главные компоненты различные классы «раздвигаются» на большее расстояние.
Другое применение — снижение влияния больших уклонений, так называемых аутлайеров (en.:outlier), которые могут искажать картину из-за использования среднеквадратичного расстояния: если выбрать 
  
    
      
        
          d
          
            l
            q
          
        
        =
        1
        
          /
        
        ‖
        
          x
          
            l
          
        
        −
        
          x
          
            q
          
        
        ‖
      
    
    {\displaystyle d_{lq}=1/\|x_{l}-x_{q}\|}
  , то влияние больших уклонений будет уменьшено. Таким образом, описанная модификация метода главных компонент является более робастной, чем классическая.

Специальная терминология
В статистике при использовании метода главных компонент используют несколько специальных терминов.

Матрица данных — 
  
    
      
        
          X
        
        =
        {
        
          x
          
            1
          
        
        ,
        .
        .
        .
        
          x
          
            m
          
        
        
          }
          
            T
          
        
      
    
    {\displaystyle \mathbf {X} =\{x_{1},...x_{m}\}^{T}}
  ; каждая строка — вектор предобработанных данных (центрированных и правильно нормированных), число строк — 
  
    
      
        m
      
    
    {\displaystyle m}
   (количество векторов данных), число столбцов — 
  
    
      
        n
      
    
    {\displaystyle n}
   (размерность пространства данных);
Матрица нагрузок (англ. loadings) — 
  
    
      
        
          P
        
        =
        {
        
          a
          
            1
          
        
        ,
        .
        .
        .
        
          a
          
            k
          
        
        }
      
    
    {\displaystyle \mathbf {P} =\{a_{1},...a_{k}\}}
  ; каждый столбец — вектор главных компонент, число строк — 
  
    
      
        n
      
    
    {\displaystyle n}
   (размерность пространства данных), число столбцов — 
  
    
      
        k
      
    
    {\displaystyle k}
   (количество векторов главных компонент, выбранных для проецирования);
Матрица счетов (англ. scores) — 
  
    
      
        
          T
        
        =
        [
        
          t
          
            i
            j
          
        
        ]
        ;
        
        
          t
          
            i
            j
          
        
        =
        (
        
          x
          
            i
          
        
        ,
        
          a
          
            j
          
        
        )
      
    
    {\displaystyle \mathbf {T} =[t_{ij}];\;t_{ij}=(x_{i},a_{j})}
  ; каждая строка — проекция вектора данных на 
  
    
      
        k
      
    
    {\displaystyle k}
   главных компонент; число строк — 
  
    
      
        m
      
    
    {\displaystyle m}
   (количество векторов данных), число столбцов — 
  
    
      
        k
      
    
    {\displaystyle k}
   (количество векторов главных компонент, выбранных для проецирования);
Матрица 
  
    
      
        Z
      
    
    {\displaystyle Z}
  -счетов (англ. 
  
    
      
        Z
      
    
    {\displaystyle Z}
  -scores) — 
  
    
      
        
          Z
        
        =
        [
        
          z
          
            i
            j
          
        
        ]
        ;
        
        
          z
          
            i
            j
          
        
        =
        
          
            
              (
              
                x
                
                  i
                
              
              ,
              
                a
                
                  j
                
              
              )
            
            
              
                λ
                
                  j
                
              
            
          
        
      
    
    {\displaystyle \mathbf {Z} =[z_{ij}];\;z_{ij}={\frac {(x_{i},a_{j})}{\sqrt {\lambda _{j}}}}}
  ; каждая строка — проекция вектора данных на 
  
    
      
        k
      
    
    {\displaystyle k}
   главных компонент, нормированная на единичную выборочную дисперсию; число строк — 
  
    
      
        m
      
    
    {\displaystyle m}
   (количество векторов данных), число столбцов — 
  
    
      
        k
      
    
    {\displaystyle k}
   (количество векторов главных компонент, выбранных для проецирования);
Матрица ошибок (или остатков) (англ. errors или residuals) - 
  
    
      
        
          E
        
        =
        
          X
        
        −
        
          T
        
        
          
            P
          
          
            T
          
        
      
    
    {\displaystyle \mathbf {E} =\mathbf {X} -\mathbf {T} \mathbf {P} ^{T}}
  .
Основная формула: 
  
    
      
        
          X
        
        =
        
          T
        
        
          
            P
          
          
            T
          
        
        +
        
          E
        
      
    
    {\displaystyle \mathbf {X} =\mathbf {T} \mathbf {P} ^{T}+\mathbf {E} }
  .

Границы применимости и ограничения эффективности метода
Метод главных компонент применим всегда. Распространённое утверждение о том, что он применим только к нормально распределённым данным (или для распределений, близких к нормальным) неверно: в исходной формулировке Пирсона ставится задача об аппроксимации конечного множества данных и отсутствует даже гипотеза о их статистическом порождении, не говоря уж о распределении.
Однако метод не всегда эффективно снижает размерность при заданных ограничениях на точность 
  
    
      
        
          δ
          
            k
          
        
      
    
    {\displaystyle \delta _{k}}
  . Прямые и плоскости не всегда обеспечивают хорошую аппроксимацию. Например, данные могут с хорошей точностью следовать какой-нибудь кривой, а эта кривая может быть сложно расположена в пространстве данных. В этом случае метод главных компонент для приемлемой точности потребует нескольких компонент (вместо одной), или вообще не даст снижения размерности при приемлемой точности. Для работы с такими «кривыми» главными компонентами изобретен метод главных многообразий и различные версии нелинейного метода главных компонент. Больше неприятностей могут доставить данные сложной топологии. Для их аппроксимации также изобретены различные методы, например самоорганизующиеся карты Кохонена, нейронный газ или топологические грамматики. Если данные статистически порождены с распределением, сильно отличающимся от нормального, то для аппроксимации распределения полезно перейти от главных компонент к независимым компонентам, которые уже не ортогональны в исходном скалярном произведении. Наконец, для изотропного распределения (даже нормального) вместо эллипсоида рассеяния получаем шар, и уменьшить размерность методами аппроксимации невозможно.

Примеры использования
Визуализация данных
Визуализация данных — представление в наглядной форме данных эксперимента или результатов теоретического исследования.
Первым выбором в визуализации множества данных является ортогональное проецирование на плоскость первых двух главных компонент (или 3-мерное пространство первых трёх главных компонент). Плоскость проецирования является, по сути плоским двумерным «экраном», расположенным таким образом, чтобы обеспечить «картинку» данных с наименьшими искажениями. Такая проекция будет оптимальна (среди всех ортогональных проекций на разные двумерные экраны) в трех отношениях:

Минимальна сумма квадратов расстояний от точек данных до проекций на плоскость первых главных компонент, то есть экран расположен максимально близко по отношению к облаку точек.
Минимальна сумма искажений квадратов расстояний между всеми парами точек из облака данных после проецирования точек на плоскость.
Минимальна сумма искажений квадратов расстояний между всеми точками данных и их «центром тяжести».Визуализация данных является одним из наиболее широко используемых приложений метода главных компонент и его нелинейных обобщений.

Компрессия изображений и видео
Для уменьшения пространственной избыточности пикселей при кодировании изображений и видео используется линейное преобразование блоков пикселей. Последующие квантования полученных коэффициентов и кодирование без потерь позволяют получить значительные коэффициенты сжатия. Использование преобразования PCA в качестве линейного преобразования является для некоторых типов данных оптимальным с точки зрения размера полученных данных при одинаковом искажении. На данный момент этот метод активно не используется, в основном из-за большой вычислительной сложности. Также сжатия данных можно достичь отбрасывая последние коэффициенты преобразования.

Подавление шума на изображениях
Основная суть метода — при удалении шума из блока пикселей представить окрестность этого блока в виде набора точек в многомерном пространстве, применить к нему PCA и оставить только первые компоненты преобразования. При этом предполагается, что в первых компонентах содержится основная полезная информация, оставшиеся же компоненты содержат ненужный шум. Применив обратное преобразование после редукции базиса главных компонент, мы получим изображение без шума.

Индексация видео
Основная идея — представить при помощи PCA каждый кадр видео несколькими значениями, которые в дальнейшем будут использоваться при построении базы данных и запросам к ней. Столь существенная редукция данных позволяет значительно увеличить скорость работы и устойчивость к ряду искажений в видео.

Биоинформатика
Метод главных компонент интенсивно используется в биоинформатике для сокращения размерности описания, выделения значимой информации, визуализации данных и др. Один из распространённых вариантов использования — анализ соответствий. На иллюстрациях (Рис. А, Б) генетический текст представлен как множество точек в 64-мерном пространстве частот триплетов. Каждая точка соответствует фрагменту ДНК в скользящем окне длиной 300 нуклеотидов (ДНК-блуждание). Этот фрагмент разбивается на неперекрывающиеся триплеты, начиная с первой позиции. Относительные частоты этих триплетов во фрагменте и составляют 64-мерный вектор. На Рис. А представлена проекция на первые 2 главные компоненты для генома бактерии Streptomyces coelicolor. На Рис. Б представлена проекция на первые 3 главные компоненты. Оттенками красного и коричневого выделены фрагменты кодирующих последовательностей в прямой цепи ДНК, а оттенками зелёного выделены фрагменты кодирующих последовательностей в обратной цепи ДНК. Чёрным помечены фрагменты, принадлежащие некодирующей части. Анализ методом главных компонент большинства известных бактериальных геномов представлен на специализированном сайте.

Хемометрика
Метод главных компонент — один из основных методов в хемометрике. Позволяет разделить матрицу исходных данных X на две части: «содержательную» и «шум».

Психодиагностика
Психодиагностика является одной из наиболее разработанных областей приложения метода главных компонент. Стратегия использования основывается на гипотезе об автоинформативности экспериментальных данных, которая подразумевает, что диагностическую модель можно создать путём аппроксимации геометрической структуры множества объектов в пространстве исходных признаков. Хорошую линейную диагностическую модель удается построить, когда значительная часть исходных признаков внутренне согласованна. Если эта внутренняя согласованность отражает искомый психологический конструкт, то параметры линейной диагностической модели (веса признаков) дает метод главных компонент.

Эконометрика
Метод главных компонент — один из ключевых инструментов эконометрики, он применяется для наглядного представления данных, обеспечения лаконизма моделей, упрощения счёта и интерпретации, сжатия объёмов хранимой информации. Метод обеспечивает максимальную информативность и минимальное искажение геометрической структуры исходных данных.

Социология
В социологии метод необходим для решения первых двух основных задач:

анализ данных (описание результатов опросов или других исследований, представленных в виде массивов числовых данных);
описание социальных явлений (построение моделей явлений, в том числе и математических моделей).

Политология
В политологии метод главных компонент был основным инструментом проекта «Политический атлас современности» для линейного и нелинейного анализа рейтингов 192 стран мира по пяти специально разработанным интегральным индексам (уровня жизни, международного влияния, угроз, государственности и демократии). Для картографии результатов этого анализа разработана специальная геоинформационная система, объединяющая географическое пространство с пространством признаков. Также созданы карты данных политического атласа, использующие в качестве подложки двумерные главные многообразия в пятимерном пространстве стран. Отличие карты данных от географической карты заключается в том, что на географической карте рядом оказываются объекты, которые имеют сходные географические координаты, в то время как на карте данных рядом оказываются объекты (страны) с похожими признаками (индексами).

Сокращение размерности динамических моделей
Проклятие размерности затрудняет моделирование сложных систем. Сокращение размерности модели — необходимое условие успеха моделирования. Для достижения этой цели создана разветвленная математическая технология. Метод главных компонент также используется в этих задачах (часто под названием истинное или собственное ортогональное разложение — англ. proper orthogonal decomposition (POD)). Например, при описании динамики турбулентности динамические переменные — поле скоростей — принадлежат бесконечномерному пространству (или, если представлять поле его значениями на достаточно мелкой сетке, — конечномерному пространству большой размерности). Можно набрать большую коллекцию мгновенных значений полей и применить к этому множеству многомерных «векторов данных» метод главных компонент. Эти главные компоненты называются также эмпирические собственные векторы. В некоторых случаях (структурная турбулентность) метод дает впечатляющее сокращение размерности Другие области применения этой техники сокращения динамических моделей чрезвычайно разнообразны — от теоретических основ химической технологии до океанологии и климатологии.

Сенсорная оценка пищевых продуктов
Свое применение метод главных компонент получил при проведении сенсорной (органолептической) оценки свойств пищевых продуктов. Метод главных компонент позволяет проводить классификации пищевых продуктов в тех случаях, когда для характеристики их свойств используется одновременно большое число дескрипторов, например при оценке свойств вина, мармелада, экструдированных пищевых продуктов, сыра, и других.

Альтернативы и обобщения
Метод главных компонент — наиболее распространённый подход к снижению размерности, однако существуют и другие способы, в частности, метод независимых компонент, многомерное шкалирование, а также многочисленные нелинейные обобщения: метод главных кривых и многообразий, метод упругих карт, поиск наилучшей проекции (англ. Projection Pursuit), нейросетевые методы «узкого горла», самоорганизующиеся карты Кохонена.

См. также
SSA (метод)

Примечания
Литература
Классические работыPearson K., On lines and planes of closest fit to systems of points in space, Philosophical Magazine, (1901) 2, 559—572; а также на сайте PCA.
Sylvester J.J., On the reduction of a bilinear quantic of the nth order to the form of a sum of n products by a double orthogonal substitution, Messenger of Mathematics, 19 (1889), 42—46; а также на сайте PCA.
Frećhet M.  Les élements aléatoires de nature quelconque dans un espace distancié. Ann. Inst. H. Poincaré, 10 (1948), 215—310.Основные руководстваАйвазян С. А., Бухштабер В. М., Енюков И. С., Мешалкин Л. Д. Прикладная статистика. Классификация и снижение размерности.— М.: Финансы и статистика, 1989.— 607 с.
Jolliffe I.T. Principal Component Analysis, Series: Springer Series in Statistics, 2nd ed., Springer, NY, 2002, XXIX, 487 p. 28 illus. ISBN 978-0-387-95442-4Современные обзорыGorban A. N., Kegl B., Wunsch D., Zinovyev A. Y. (Eds.), Principal Manifolds for Data Visualisation and Dimension Reduction, Series: Lecture Notes in Computational Science and Engineering 58, Springer, Berlin — Heidelberg — New York, 2007, XXIV, 340 p. 82 illus. ISBN 978-3-540-73749-0 (а также онлайн).Учебное программное обеспечениеJava-апплет «Метод главных компонент и самоорганизующиеся карты» (E.M. Mirkes, Principal Component Analysis and Self-Organizing Maps: applet. University of Leicester, 2011). Свободно распространяемая программа с моделями метода главных компонент, самоорганизуюшихся карт (SOM) и растущих самоорганизующихся карт (Growing Self-Organized Maps, GSOM). Дано описание алгоритмов (англ.), приведены руководства и некоторые публикации. Используется для выполнения небольших студенческих исследовательских работ по сравнению различных алгоритмов аппроксимации данных.

Ссылки
Курс «Анализ лингвистических данных: квантитативные методы и визуализация»
A tutorial on Principal Components Analysis, Jonathon Shlens, 22, 2009; Version 3.01.
Нелинейный метод главных компонент (сайт-библиотека)
Онлайн руководство «Метод Главных Компонент (PCA)» c примерами, выполненными в рабочей книге Excel, Алексей Померанцев.