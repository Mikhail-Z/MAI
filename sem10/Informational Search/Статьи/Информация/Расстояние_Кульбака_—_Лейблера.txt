Расстояние (расхождение, дивергенция) Ку́льбака — Ле́йблера (англ. Kullback–Leibler divergence), РКЛ, информационное расхождение, различающая информация, информационный выигрыш, относительная энтропия (англ. relative entropy) — неотрицательнозначный функционал, являющийся несимметричной мерой удалённости друг от друга двух вероятностных распределений, определённых на общем пространстве элементарных событий. Часто применяется в теории информации и математической статистике.
Расхождение Кульбака — Лейблера распределения 
  
    
      
        Q
      
    
    {\displaystyle Q}
   относительно 
  
    
      
        P
      
    
    {\displaystyle P}
   (или, условно говоря, «расстояние от 
  
    
      
        P
      
    
    {\displaystyle P}
   до 
  
    
      
        Q
      
    
    {\displaystyle Q}
  ») обозначается 
  
    
      
        
          D
          
            
              K
              L
            
          
        
        (
        P
        ∥
        Q
        )
      
    
    {\displaystyle D_{\mathrm {KL} }(P\parallel Q)}
  . Первый аргумент функционала (распределение 
  
    
      
        P
      
    
    {\displaystyle P}
  ) обычно интерпретируется как истинное или постулируемое априори распределение, второй (распределение 
  
    
      
        Q
      
    
    {\displaystyle Q}
  ) — как предполагаемое (проверяемое). Распределение 
  
    
      
        Q
      
    
    {\displaystyle Q}
   часто служит приближением распределения 
  
    
      
        P
      
    
    {\displaystyle P}
  . Значение функционала можно понимать как количество неучтённой информации распределения 
  
    
      
        P
      
    
    {\displaystyle P}
  , если 
  
    
      
        Q
      
    
    {\displaystyle Q}
   было использовано для приближения 
  
    
      
        P
      
    
    {\displaystyle P}
  . Данная мера расстояния в теории информации также интерпретируется как величина потерь информации при замене истинного распределения 
  
    
      
        P
      
    
    {\displaystyle P}
   на распределение 
  
    
      
        Q
      
    
    {\displaystyle Q}
  .
В общем случае, если 
  
    
      
        μ
      
    
    {\displaystyle \mu }
   — любая мера на 
  
    
      
        X
      
    
    {\displaystyle X}
  , для которой существуют абсолютно непрерывные относительно 
  
    
      
        μ
      
    
    {\displaystyle \mu }
   функции 
  
    
      
        p
        =
        
          
            
              
                
                  d
                
              
              P
            
            
              
                
                  d
                
              
              μ
            
          
        
      
    
    {\displaystyle p={\frac {{\rm {d}}P}{{\rm {d}}\mu }}}
   и 
  
    
      
        q
        =
        
          
            
              
                
                  d
                
              
              Q
            
            
              
                
                  d
                
              
              μ
            
          
        
      
    
    {\displaystyle q={\frac {{\rm {d}}Q}{{\rm {d}}\mu }}}
  , тогда расхождение Кульбака — Лейблера распределения 
  
    
      
        Q
      
    
    {\displaystyle Q}
   относительно 
  
    
      
        P
      
    
    {\displaystyle P}
   определяется как

  
    
      
        
          D
          
            
              K
              L
            
          
        
        (
        P
        ∥
        Q
        )
        =
        
          ∫
          
            X
          
        
        p
        
        log
        ⁡
        
          
            p
            q
          
        
        
        
          
            d
          
        
        μ
      
    
    {\displaystyle D_{\mathrm {KL} }(P\parallel Q)=\int _{X}p\,\log {\frac {p}{q}}\,{\rm {d}}\mu }
  .Основание логарифма в этой формуле существенной роли не играет. Его выбор позволяет зафиксировать конкретный вид функционала из семейства эквивалентных функционалов и равносилен выбору единицы измерения расхождения Кульбака — Лейблера (подобно ситуации с вычислением энтропии), поэтому возможно применение логарифма с любым основанием, большим единицы. Другими словами, функционал определён с точностью до положительного постоянного сомножителя. Наиболее употребительными являются натуральный логарифм (по соображениям удобства), а также двоичный логарифм — для измерения расхождения в битах (обычно используется в теории информации). Расхождение Кульбака — Лейблера является безразмерной величиной независимо от размерности исходных случайных величин.
Хотя расстояние Кульбака — Лейблера (РКЛ) часто рассматривается как способ измерения расстояния между вероятностными распределениями, данный функционал не является метрикой в пространстве распределений, поскольку не удовлетворяет неравенству треугольника и не удовлетворяет аксиоме симметричности: 
  
    
      
        
          D
          
            
              K
              L
            
          
        
        (
        P
        ∥
        Q
        )
        ≠
        
          D
          
            
              K
              L
            
          
        
        (
        Q
        ∥
        P
        )
      
    
    {\displaystyle D_{\mathrm {KL} }(P\parallel Q)\neq D_{\mathrm {KL} }(Q\parallel P)}
  . Тем не менее, его инфинитезимальная форма, особенно его Гессиан, дает метрический тензор, который известен как информационная метрика Фишера.
Расстояние Кульбака — Лейблера — это частный случай более общего класса расхождений, которые называются f-расхождения, а также частный случай класса расхождений Брегмана. РКЛ — это единственное расхождение вероятностей, которое принадлежит и тому, и другому классу.
РКЛ изначально было представлено Соломоном Кульбаком и Ричардом Лейблером в 1951 как направленное расхождение между двумя распределениями. Это обсуждается в тексте Кульбака «Информационная теория и статистика».Расстояние Кульбака — Лейблера 
  
    
      
        
          D
          
            
              K
              L
            
          
        
        (
        P
        ∥
        Q
        )
      
    
    {\displaystyle D_{\mathrm {KL} }(P\parallel Q)}
   иногда также интерпретируют как информационный выигрыш, достигнутый, если 
  
    
      
        P
      
    
    {\displaystyle P}
   использовано вместо 
  
    
      
        Q
      
    
    {\displaystyle Q}
  . Иногда для РКЛ используют вносящие путаницу названия относительная энтропия 
  
    
      
        P
      
    
    {\displaystyle P}
   относительно 
  
    
      
        Q
      
    
    {\displaystyle Q}
   (обозначается 
  
    
      
        H
        (
        P
        ∣
        Q
        )
      
    
    {\displaystyle H(P\mid Q)}
  ) или перекрёстная энтропия.
Существуют различные соглашения относительно того, как читать обозначение
  
    
      
        
          D
          
            
              K
              L
            
          
        
        (
        P
        ∥
        Q
        )
      
    
    {\displaystyle D_{\mathrm {KL} }(P\parallel Q)}
  . Часто его называют просто расхождением или расстоянием между 
  
    
      
        P
      
    
    {\displaystyle P}
   и 
  
    
      
        Q
      
    
    {\displaystyle Q}
  , однако это не позволяет передать фундаментальную асимметрию в соотношении. Иногда говорят «расхождение 
  
    
      
        P
      
    
    {\displaystyle P}
   из (относительно) 
  
    
      
        Q
      
    
    {\displaystyle Q}
  » или, условно говоря, «расстояние из 
  
    
      
        Q
      
    
    {\displaystyle Q}
   в 
  
    
      
        P
      
    
    {\displaystyle P}
  » (обычно в контексте относительной энтропии или информационного выигрыша). При этом распределение 
  
    
      
        Q
      
    
    {\displaystyle Q}
   интерпретируется, как истинное.

Частные определения и определения через производную Радона—Никодима
Для дискретных вероятностных распределений 
  
    
      
        P
      
    
    {\displaystyle P}
   и 
  
    
      
        Q
      
    
    {\displaystyle Q}
   с числом элементарных событий 
  
    
      
        n
      
    
    {\displaystyle n}
   расхождение Кульбака — Лейблера распределения 
  
    
      
        Q
      
    
    {\displaystyle Q}
   относительно распределения 
  
    
      
        P
      
    
    {\displaystyle P}
   (или «расстояние от 
  
    
      
        P
      
    
    {\displaystyle P}
   до 
  
    
      
        Q
      
    
    {\displaystyle Q}
  ») определяется как:

  
    
      
        
          D
          
            K
            L
          
        
        (
        P
        ∥
        Q
        )
        =
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          p
          
            i
          
        
        log
        ⁡
        
          
            
              p
              
                i
              
            
            
              q
              
                i
              
            
          
        
      
    
    {\displaystyle D_{KL}(P\parallel Q)=\sum \limits _{i=1}^{n}p_{i}\log {\frac {p_{i}}{q_{i}}}}
  .Другими словами, это математическое ожидание логарифмической разности между вероятностями 
  
    
      
        p
      
    
    {\displaystyle p}
   и 
  
    
      
        q
      
    
    {\displaystyle q}
  , где математическое ожидание берётся по распределению 
  
    
      
        P
      
    
    {\displaystyle P}
  . РКЛ определено, только если 
  
    
      
        
          q
          
            i
          
        
        =
        0
        ⇒
        
          p
          
            i
          
        
        =
        0
      
    
    {\displaystyle q_{i}=0\Rightarrow p_{i}=0}
  , для всех 
  
    
      
        i
        =
        1
        ,
        .
        .
        .
        ,
        n
      
    
    {\displaystyle i=1,...,n}
   (абсолютная непрерывность). Всякий раз, когда 
  
    
      
        
          p
          
            i
          
        
        =
        0
      
    
    {\displaystyle p_{i}=0}
  , вклад 
  
    
      
        i
      
    
    {\displaystyle i}
  -го члена интерпретируется как ноль, потому что 
  
    
      
        
          lim
          
            x
            →
            0
          
        
        x
        log
        ⁡
        (
        x
        )
        =
        0
      
    
    {\displaystyle \lim _{x\to 0}x\log(x)=0}
  .
Для 
  
    
      
        k
      
    
    {\displaystyle k}
  -мерных абсолютно непрерывных распределений 
  
    
      
        P
      
    
    {\displaystyle P}
   и 
  
    
      
        Q
      
    
    {\displaystyle Q}
   расстояние Кульбака — Лейблера задаётся выражением

  
    
      
        
          D
          
            
              K
              L
            
          
        
        (
        P
        ∥
        Q
        )
        =
        
          ∫
          
            X
          
        
        
        p
        (
        x
        )
        log
        ⁡
        
          
            
              p
              (
              x
              )
            
            
              q
              (
              x
              )
            
          
        
        
        
          
            d
          
        
        x
      
    
    {\displaystyle D_{\mathrm {KL} }(P\parallel Q)=\int _{X}\,p(x)\log {\frac {p(x)}{q(x)}}\,{\rm {d}}x}
  ,где 
  
    
      
        p
        (
        x
        )
      
    
    {\displaystyle p(x)}
   и 
  
    
      
        q
        (
        x
        )
      
    
    {\displaystyle q(x)}
   — функции плотности распределений 
  
    
      
        P
      
    
    {\displaystyle P}
   и 
  
    
      
        Q
      
    
    {\displaystyle Q}
   соответственно, определённые на интервале 
  
    
      
        X
        ⊆
        
          R
          
            k
          
        
      
    
    {\displaystyle X\subseteq R^{k}}
  .
В более общем смысле, если 
  
    
      
        P
      
    
    {\displaystyle P}
   и 
  
    
      
        Q
      
    
    {\displaystyle Q}
   — вероятностные меры на множестве 
  
    
      
        X
      
    
    {\displaystyle X}
  , и 
  
    
      
        P
      
    
    {\displaystyle P}
   абсолютно непрерывна относительно 
  
    
      
        Q
      
    
    {\displaystyle Q}
  , тогда РКЛ от 
  
    
      
        P
      
    
    {\displaystyle P}
   до 
  
    
      
        Q
      
    
    {\displaystyle Q}
   определено как:

  
    
      
        
          D
          
            
              K
              L
            
          
        
        (
        P
        ∥
        Q
        )
        =
        
          ∫
          
            X
          
        
        log
        ⁡
        
          
            
              
                
                  d
                
              
              P
            
            
              
                
                  d
                
              
              Q
            
          
        
        
        
          
            d
          
        
        P
      
    
    {\displaystyle D_{\mathrm {KL} }(P\parallel Q)=\int _{X}\log {\frac {{\rm {d}}P}{{\rm {d}}Q}}\,{\rm {d}}P}
  ,где 
  
    
      
        
          
            
              
                
                  d
                
              
              P
            
            
              
                
                  d
                
              
              Q
            
          
        
      
    
    {\displaystyle {\frac {{\rm {d}}P}{{\rm {d}}Q}}}
   — это производная Радона—Никодима 
  
    
      
        P
      
    
    {\displaystyle P}
   относительно 
  
    
      
        Q
      
    
    {\displaystyle Q}
  , и при условии, что выражение справа существует. Эквивалентно это может быть записано как

  
    
      
        
          D
          
            
              K
              L
            
          
        
        (
        P
        ∥
        Q
        )
        =
        
          ∫
          
            X
          
        
        log
        
        
          (
          
            
              
                
                  
                    d
                  
                
                P
              
              
                
                  
                    d
                  
                
                Q
              
            
          
          )
        
        
          
            
              
                
                  d
                
              
              P
            
            
              
                
                  d
                
              
              Q
            
          
        
        
        
          
            d
          
        
        Q
      
    
    {\displaystyle D_{\mathrm {KL} }(P\parallel Q)=\int _{X}\log \!\left({\frac {{\rm {d}}P}{{\rm {d}}Q}}\right){\frac {{\rm {d}}P}{{\rm {d}}Q}}\,{\rm {d}}Q}
  .Следует заметить, что использование производной Радона — Никодима служит формальным средством записи данных выражений, однако не раскрывает их содержательный смысл.
Функционал дивергенции Кульбака — Лейблера является безразмерным, однако его значения могут иметь различные единицы измерения. Так, если логарифмы в этих формулах берутся по основанию 2, то дивергенция (она же — информация, с точки зрения теории информации) измеряется в битах; если по основанию e (с натуральным основанием), то дивергенция (информация) измеряется в натах. Большинство формул, содержащих РКЛ, сохраняют смысл независимо от основания логарифма.

Характеризация
Артур Хобсон доказал, что расстояние Кульбака — Лейблера — это единственная мера разницы между вероятностными распределениями, которая удовлетворяют некоторым желательным свойствам, являющимся каноническими расширениями для появляющихся в часто используемых характеризациях энтропии. Следовательно, взаимная информация — это единственная мера взаимной зависимости, которая подчиняется некоторым связанным условиям, так как она может быть определена в терминах РКЛ.
Существует также Байесовская характеризация расстояния Кульбака — Лейблера.

Мотивировка
В теории информации теорема Крафта — Макмиллана устанавливает, что любую непосредственно декодируемую схему кодирования для кодировки сообщения для идентификации одного значения 
  
    
      
        
          x
          
            i
          
        
        ⊂
        X
      
    
    {\displaystyle x_{i}\subset X}
  , можно рассматривать как представление неявного распределения вероятностей 
  
    
      
        q
        (
        
          x
          
            i
          
        
        )
        =
        
          2
          
            −
            
              I
              
                i
              
            
          
        
      
    
    {\displaystyle q(x_{i})=2^{-I_{i}}}
   над 
  
    
      
        X
      
    
    {\displaystyle X}
  , где 
  
    
      
        
          I
          
            i
          
        
      
    
    {\displaystyle I_{i}}
   — длина кода для 
  
    
      
        
          x
          
            i
          
        
      
    
    {\displaystyle x_{i}}
   в битах. Поэтому, РКЛ может быть интерпретировано, как ожидаемая дополнительная длина сообщения с нулевой отметки, которая должна быть передана, если код, который является оптимальным для данного (неправильного) распределения Q, используется, по сравнению с использованием кода на основе истинного распределения P.

  
    
      
        
          
            
              
                
                  D
                  
                    
                      K
                      L
                    
                  
                
                (
                P
                ∥
                Q
                )
                =
                −
                
                  ∑
                  
                    x
                  
                
                p
                (
                x
                )
                log
                ⁡
                q
                (
                x
                )
                +
                
                  ∑
                  
                    x
                  
                
                p
                (
                x
                )
                log
                ⁡
                p
                (
                x
                )
                =
                H
                (
                P
                ,
                Q
                )
                −
                H
                (
                P
                )
                
                
              
            
          
        
      
    
    {\textstyle {\begin{matrix}D_{\mathrm {KL} }(P\parallel Q)=-\sum _{x}p(x)\log q(x)+\sum _{x}p(x)\log p(x)=H(P,Q)-H(P)\,\!\end{matrix}}}
  , где 
  
    
      
        H
        (
        P
        ,
        Q
        )
      
    
    {\displaystyle H(P,Q)}
   — перекрестная энтропия P и Q, 
  
    
      
        H
        (
        P
        )
      
    
    {\displaystyle H(P)}
   — энтропия P.
Отметим также, что существует связь между РКЛ и «функцией скорости» в теории больших отклонений.

Свойства
Расстояние Кульбака — Лейблера всегда неотрицательно, 
  
    
      
        
          D
          
            
              K
              L
            
          
        
        (
        P
        ∥
        Q
        )
        ≥
        0
        ,
      
    
    {\displaystyle D_{\mathrm {KL} }(P\parallel Q)\geq 0,}
  это результат, который известен как неравенство Гиббса, 
  
    
      
        
          D
          
            K
            L
          
        
        (
        P
        ∥
        Q
        )
        =
        0
        
        ⟺
        
        P
        =
        Q
      
    
    {\displaystyle D_{KL}(P\parallel Q)=0\iff P=Q}
   почти всюду. Энтропия H(P), таким образом, задаёт минимальное значение перекрестной энтропии H(P,Q), ожидаемое число дополнительных битов, требуемых когда используется код, основанный на Q, а не на P. Поэтому РКЛ представляет собой ожидаемое число дополнительных битов, которые должны быть переданы, чтобы определить значение 
  
    
      
        x
        ⊂
        X
      
    
    {\displaystyle x\subset X}
  , если используется код, соответствующий распределению вероятностей Q, а не «истинному» распределения P.
Расстояние Кульбака — Лейблера не симметрично: 
  
    
      
        
          D
          
            
              K
              L
            
          
        
        (
        P
        ∥
        Q
        )
        ≠
        
          D
          
            
              K
              L
            
          
        
        (
        Q
        ∥
        P
        )
      
    
    {\displaystyle D_{\mathrm {KL} }(P\parallel Q)\neq D_{\mathrm {KL} }(Q\parallel P)}
  .Расстояние Кульбака — Лейблера остается строго определенным для непрерывных распределений, и кроме того инвариантно относительно замены переменных. Например, если сделана замена переменной x на переменную y(x), тогда, так как 
  
    
      
        P
        (
        x
        )
        d
        x
        =
        P
        (
        y
        )
        d
        y
      
    
    {\displaystyle P(x)dx=P(y)dy}
   и 
  
    
      
        Q
        (
        x
        )
        d
        x
        =
        Q
        (
        y
        )
      
    
    {\displaystyle Q(x)dx=Q(y)}
  , РКЛ может переписано:
  
    
      
        
          D
          
            
              K
              L
            
          
        
        (
        P
        ∥
        Q
        )
        =
        
          ∫
          
            
              x
              
                a
              
            
          
          
            
              x
              
                b
              
            
          
        
        P
        (
        x
        )
        log
        ⁡
        
          (
          
            
              
                P
                (
                x
                )
              
              
                Q
                (
                x
                )
              
            
          
          )
        
        
        d
        x
        =
        
          ∫
          
            
              y
              
                a
              
            
          
          
            
              y
              
                b
              
            
          
        
        P
        (
        y
        )
        log
        ⁡
        
          (
          
            
              
                P
                (
                y
                )
                d
                y
                
                  /
                
                d
                x
              
              
                Q
                (
                y
                )
                d
                y
                
                  /
                
                d
                x
              
            
          
          )
        
        
        d
        y
        =
        
          ∫
          
            
              y
              
                a
              
            
          
          
            
              y
              
                b
              
            
          
        
        P
        (
        y
        )
        log
        ⁡
        
          (
          
            
              
                P
                (
                y
                )
              
              
                Q
                (
                y
                )
              
            
          
          )
        
        
        d
        y
      
    
    {\textstyle D_{\mathrm {KL} }(P\parallel Q)=\int _{x_{a}}^{x_{b}}P(x)\log \left({\frac {P(x)}{Q(x)}}\right)\,dx=\int _{y_{a}}^{y_{b}}P(y)\log \left({\frac {P(y)dy/dx}{Q(y)dy/dx}}\right)\,dy=\int _{y_{a}}^{y_{b}}P(y)\log \left({\frac {P(y)}{Q(y)}}\right)\,dy}
  ,
где 
  
    
      
        
          y
          
            a
          
        
        =
        y
        (
        
          x
          
            a
          
        
        )
      
    
    {\displaystyle y_{a}=y(x_{a})}
   и 
  
    
      
        
          y
          
            b
          
        
        =
        y
        (
        
          x
          
            b
          
        
        )
      
    
    {\displaystyle y_{b}=y(x_{b})}
  . Несмотря на предположение, что преобразование было непрерывным, это не необходимо в данном случае. Это также показывает, что РКЛ задаёт величину согласованную с размерностью, так как если x — размерная переменная, то P(x) и Q(x) также имеют размерность, так как 
  
    
      
        P
        (
        x
        )
        d
        x
      
    
    {\displaystyle P(x)dx}
   является безрамерной величиной. Тем не менее, выражение под логарифмом остаётся безразмерным, как и должно. Поэтому расстояние Кульбака — Лейблера можно рассматривать, в некотором смысле, как более фундаментальную величину, чем некоторые другие свойства в теории информации (такие как собственная информация или энтропия Шеннона), которые могут стать неопределёнными или отрицательными для недискретных вероятностей.

РКЛ аддитивна для независимых распределений во многом таким же образом, как энтропия Шеннона. Если 
  
    
      
        
          P
          
            1
          
        
        ,
        
          P
          
            2
          
        
      
    
    {\displaystyle P_{1},P_{2}}
   являются независимыми распределениями с совместным распределением 
  
    
      
        P
        (
        x
        ,
        y
        )
        =
        
          P
          
            1
          
        
        (
        x
        )
        
          P
          
            2
          
        
        (
        y
        )
      
    
    {\displaystyle P(x,y)=P_{1}(x)P_{2}(y)}
   и, аналогично, 
  
    
      
        Q
        (
        x
        ,
        y
        )
        =
        
          Q
          
            1
          
        
        (
        x
        )
        
          Q
          
            2
          
        
        (
        y
        )
      
    
    {\displaystyle Q(x,y)=Q_{1}(x)Q_{2}(y)}
  , то 
  
    
      
        
          D
          
            
              K
              L
            
          
        
        (
        P
        ∥
        Q
        )
        =
        
          D
          
            
              K
              L
            
          
        
        (
        
          P
          
            1
          
        
        ∥
        
          Q
          
            1
          
        
        )
        +
        
          D
          
            
              K
              L
            
          
        
        (
        
          P
          
            2
          
        
        ∥
        
          Q
          
            2
          
        
        )
        .
      
    
    {\displaystyle D_{\mathrm {KL} }(P\parallel Q)=D_{\mathrm {KL} }(P_{1}\parallel Q_{1})+D_{\mathrm {KL} }(P_{2}\parallel Q_{2}).}

Расстояние Кульбака — Лейблера для многомерного нормального распределения
Допустим, что мы имеем два многомерных нормальных распределения, со средними 
  
    
      
        
          μ
          
            0
          
        
        ,
        
          μ
          
            1
          
        
      
    
    {\displaystyle \mu _{0},\mu _{1}}
   и с (обратимыми) матрицами ковариаций 
  
    
      
        
          Σ
          
            0
          
        
        ,
        
          Σ
          
            1
          
        
      
    
    {\displaystyle \Sigma _{0},\Sigma _{1}}
  . Если два распределения имеют одинаковую размерность k, то РКЛ между распределениями следующее:

  
    
      
        
          D
          
            KL
          
        
        (
        
          
            
              N
            
          
          
            0
          
        
        ∥
        
          
            
              N
            
          
          
            1
          
        
        )
        =
        
          
            1
            2
          
        
        
          (
          
            
              t
              r
            
            
              (
              
                
                  Σ
                  
                    1
                  
                  
                    −
                    1
                  
                
                
                  Σ
                  
                    0
                  
                
              
              )
            
            +
            
              
                (
                
                  
                    μ
                    
                      1
                    
                  
                  −
                  
                    μ
                    
                      0
                    
                  
                
                )
              
              
                ⊤
              
            
            
              Σ
              
                1
              
              
                −
                1
              
            
            (
            
              μ
              
                1
              
            
            −
            
              μ
              
                0
              
            
            )
            −
            k
            +
            ln
            ⁡
            
              (
              
                
                  
                    det
                    
                      Σ
                      
                        1
                      
                    
                  
                  
                    det
                    
                      Σ
                      
                        0
                      
                    
                  
                
              
              )
            
          
          )
        
        .
      
    
    {\displaystyle D_{\text{KL}}({\mathcal {N}}_{0}\parallel {\mathcal {N}}_{1})={1 \over 2}\left(\mathrm {tr} \left(\Sigma _{1}^{-1}\Sigma _{0}\right)+\left(\mu _{1}-\mu _{0}\right)^{\top }\Sigma _{1}^{-1}(\mu _{1}-\mu _{0})-k+\ln \left({\det \Sigma _{1} \over \det \Sigma _{0}}\right)\right).}
  
Логарифм в последнем члене должен быть взят по основанию e, так как все члены, кроме последнего, являются натуральными логарифмами выражений, которые являются либо любыми множителями функции плотности, либо, в противном случае, возникают естественным образом. Поэтому уравнение дает результат, измеряемый в натах. Целиком разделив это выражение на loge2, получим распределение в битах.

Отношение к метрикам
Можно было бы назвать РКЛ «метрикой» в пространстве вероятностных распределений, но это было бы некорректно, так как оно не симметрично
  
    
      
        
          D
          
            
              K
              L
            
          
        
        (
        P
        ∥
        Q
        )
        ≠
        
          D
          
            
              K
              L
            
          
        
        (
        Q
        ∥
        P
        )
      
    
    {\displaystyle D_{\mathrm {KL} }(P\parallel Q)\neq D_{\mathrm {KL} }(Q\parallel P)}
  , и не удовлетворяет неравенству треугольника. Все-таки, будучи предварительной метрикой, она порождает топологию в пространстве вероятностных распределений. Более конкретно, если 
  
    
      
        {
        
          P
          
            1
          
        
        ,
        
          P
          
            2
          
        
        ,
        ⋯
        }
      
    
    {\displaystyle \{P_{1},P_{2},\cdots \}}
  - это последовательность распределений такая, что 
  
    
      
        
          lim
          
            n
            →
            ∞
          
        
        
          D
          
            
              K
              L
            
          
        
        (
        
          P
          
            n
          
        
        ∥
        Q
        )
        =
        0
      
    
    {\displaystyle \lim _{n\rightarrow \infty }D_{\mathrm {KL} }(P_{n}\parallel Q)=0}
  , тогда говорят, что 
  
    
      
        
          P
          
            n
          
        
        
          
            →
            
              D
            
          
        
        Q
      
    
    {\displaystyle P_{n}{\xrightarrow {D}}Q}
  . Из неравенства Пинскера следует, что — 
  
    
      
        
          P
          
            n
          
        
        
          
            →
            
              
                D
              
            
          
        
        P
        ⇒
        
          P
          
            n
          
        
        
          
            →
            
              
                T
                V
              
            
          
        
        P
      
    
    {\displaystyle P_{n}{\xrightarrow {\mathrm {D} }}P\Rightarrow P_{n}{\xrightarrow {\mathrm {TV} }}P}
  , где последнее нужно для для сходимости по вариации.
Согласно Альфреду Реньи (1970, 1961).

Информационная метрика Фишера
Однако, расстояние Кульбака — Лейблера и напрямую связано с метрикой, а именно с информационной метрикой Фишера. Предположим, что у нас имеются вероятностные распределения P и Q, они оба параметризованы одинаковым (возможно многомерным) параметром 
  
    
      
        θ
      
    
    {\displaystyle \theta }
  . Рассмотрим теперь два близких значения 
  
    
      
        P
        =
        P
        (
        θ
        )
      
    
    {\displaystyle P=P(\theta )}
   и 
  
    
      
        Q
        =
        P
        (
        
          θ
          
            0
          
        
        )
      
    
    {\displaystyle Q=P(\theta _{0})}
  , таких что параметр 
  
    
      
        θ
      
    
    {\displaystyle \theta }
   отличается только на небольшое число от параметра 
  
    
      
        
          θ
          
            0
          
        
      
    
    {\displaystyle \theta _{0}}
  . А именно, разлагая в ряд Тейлора вплоть до первого порядка, имеем (используя соглашение Эйнштейна)

  
    
      
        P
        (
        θ
        )
        =
        P
        (
        
          θ
          
            0
          
        
        )
        +
        Δ
        
          θ
          
            j
          
        
        
          P
          
            j
          
        
        (
        
          θ
          
            0
          
        
        )
        +
        ⋯
      
    
    {\displaystyle P(\theta )=P(\theta _{0})+\Delta \theta ^{j}P_{j}(\theta _{0})+\cdots }
  ,
где 
  
    
      
        Δ
        
          θ
          
            j
          
        
        =
        (
        θ
        −
        
          θ
          
            0
          
        
        
          )
          
            j
          
        
      
    
    {\displaystyle \Delta \theta ^{j}=(\theta -\theta _{0})^{j}}
   — малое изменение 
  
    
      
        θ
      
    
    {\displaystyle \theta }
   в j-м направлении, а 
  
    
      
        
          P
          
            j
          
        
        (
        
          θ
          
            0
          
        
        )
        =
        
          
            
              ∂
              P
            
            
              ∂
              
                θ
                
                  j
                
              
            
          
        
        (
        
          θ
          
            0
          
        
        )
      
    
    {\displaystyle P_{j}(\theta _{0})={\frac {\partial P}{\partial \theta ^{j}}}(\theta _{0})}
   соответствующая скорость изменения распределения вероятностей. Так как РКЛ имеет абсолютный минимум, равный 0, при P=Q, то есть 
  
    
      
        θ
        =
        
          θ
          
            0
          
        
      
    
    {\displaystyle \theta =\theta _{0}}
   то РКЛ имеет второй порядок малости по параметрам 
  
    
      
        Δ
        
          θ
          
            j
          
        
      
    
    {\displaystyle \Delta \theta ^{j}}
  . Более формально, как и для любого минимума, первая производная расхождения обращается в ноль 
  
    
      
        
          
            
            
              
                ∂
                
                  ∂
                  
                    θ
                    
                      j
                    
                  
                
              
            
            |
          
          
            θ
            =
            
              θ
              
                0
              
            
          
        
        
          D
          
            K
            L
          
        
        (
        P
        (
        θ
        )
        ∥
        P
        (
        
          θ
          
            0
          
        
        )
        )
        =
        0
        ,
      
    
    {\displaystyle \left.{\frac {\partial }{\partial \theta ^{j}}}\right|_{\theta =\theta _{0}}D_{KL}(P(\theta )\parallel P(\theta _{0}))=0,}
  
и разложение Тейлора начинается со второго порядка малости

  
    
      
        
          D
          
            
              K
              L
            
          
        
        (
        P
        (
        θ
        )
        ∥
        P
        (
        
          θ
          
            0
          
        
        )
        )
        =
        
          
            1
            2
          
        
        Δ
        
          θ
          
            j
          
        
        Δ
        
          θ
          
            k
          
        
        
          g
          
            j
            k
          
        
        (
        
          θ
          
            0
          
        
        )
        +
        ⋯
      
    
    {\displaystyle D_{\mathrm {KL} }(P(\theta )\parallel P(\theta _{0}))={\frac {1}{2}}\Delta \theta ^{j}\Delta \theta ^{k}g_{jk}(\theta _{0})+\cdots }
  ,
где Гессиан 
  
    
      
        
          g
          
            j
            k
          
        
        (
        θ
        )
      
    
    {\displaystyle g_{jk}(\theta )}
   должен быть неотрицательным. Если позволить 
  
    
      
        
          θ
          
            0
          
        
      
    
    {\displaystyle \theta _{0}}
   изменяться (и опуская подиндекс 0), то Гессиан 
  
    
      
        
          g
          
            j
            k
          
        
        (
        θ
        )
      
    
    {\displaystyle g_{jk}(\theta )}
   определяет (возможно, вырожденную) метрику Римана в пространстве параметра 
  
    
      
        θ
      
    
    {\displaystyle \theta }
  , называемую информационной метрикой Фишера.

Отношение к другим величинам информационной теории
Многие другие величины информационной теории могут быть интерпретированы как применение расстояния Кульбака — Лейблера к частным случаям.
Собственная информация 
  
    
      
        
          D
          
            
              K
              L
            
          
        
        (
        
          δ
          
            i
            m
          
        
        ∥
        {
        
          p
          
            i
          
        
        }
        )
      
    
    {\displaystyle D_{\mathrm {KL} }(\delta _{im}\parallel \{p_{i}\})}
   является РКЛ вероятностного распределения 
  
    
      
        P
        (
        i
        )
      
    
    {\displaystyle P(i)}
   из символа Кронекера, представляющего определённость в том, что 
  
    
      
        i
        =
        m
      
    
    {\displaystyle i=m}
   — то есть число дополнительных бит, которые должны быть переданы для определения 
  
    
      
        i
      
    
    {\displaystyle i}
  , если только вероятностное распределение 
  
    
      
        P
        (
        i
        )
      
    
    {\displaystyle P(i)}
   доступно для получателя, не факт, что 
  
    
      
        i
        =
        m
      
    
    {\displaystyle i=m}
  .
Взаимная информация -

  
    
      
        
          
            
              
                I
                (
                X
                ;
                Y
                )
              
              
                
                =
                
                  D
                  
                    
                      K
                      L
                    
                  
                
                (
                P
                (
                X
                ,
                Y
                )
                ∥
                P
                (
                X
                )
                P
                (
                Y
                )
                )
              
            
            
              
              
                
                =
                
                  E
                  
                    X
                  
                
                ⁡
                {
                
                  D
                  
                    
                      K
                      L
                    
                  
                
                (
                P
                (
                Y
                ∣
                X
                )
                ∥
                P
                (
                Y
                )
                )
                }
              
            
            
              
              
                
                =
                
                  E
                  
                    Y
                  
                
                ⁡
                {
                
                  D
                  
                    
                      K
                      L
                    
                  
                
                (
                P
                (
                X
                ∣
                Y
                )
                ∥
                P
                (
                X
                )
                )
                }
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}I(X;Y)&=D_{\mathrm {KL} }(P(X,Y)\parallel P(X)P(Y))\\&=\operatorname {E} _{X}\{D_{\mathrm {KL} }(P(Y\mid X)\parallel P(Y))\}\\&=\operatorname {E} _{Y}\{D_{\mathrm {KL} }(P(X\mid Y)\parallel P(X))\}\end{aligned}}}
  
является РКЛ произведения 
  
    
      
        P
        (
        X
        )
        P
        (
        Y
        )
      
    
    {\displaystyle P(X)P(Y)}
   двух маргинальных вероятностных распределений из совместного вероятностного распределения 
  
    
      
        P
        (
        X
        ,
        Y
        )
      
    
    {\displaystyle P(X,Y)}
   — то есть ожидаемое число дополнительных битов, которые должны быть посланы, чтобы определить 
  
    
      
        X
      
    
    {\displaystyle X}
   и 
  
    
      
        Y
      
    
    {\displaystyle Y}
  , если они закодированы, используя только их маргинальное распределение вместо совместного распределения. Эквивалентно, если совместная вероятность 
  
    
      
        P
        (
        X
        ,
        Y
        )
      
    
    {\displaystyle P(X,Y)}
   известна, это ожидаемое число дополнительных битов, которые должны быть в среднем отправлены для определения 
  
    
      
        Y
      
    
    {\displaystyle Y}
  , если значение 
  
    
      
        X
      
    
    {\displaystyle X}
   уже не известны получателю.
Энтропия Шеннона -

  
    
      
        
          
            
              
                
                  H
                
                (
                X
                )
              
              
                
                =
                E
                ⁡
                [
                
                  I
                  
                    X
                  
                
                ⁡
                (
                x
                )
                ]
              
            
            
              
              
                
                =
                log
                ⁡
                (
                N
                )
                −
                
                  D
                  
                    KL
                  
                
                (
                P
                (
                X
                )
                ∥
                
                  P
                  
                    U
                  
                
                (
                X
                )
                )
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}\mathrm {H} (X)&=\operatorname {E} [\operatorname {I} _{X}(x)]\\&=\log(N)-D_{\text{KL}}(P(X)\parallel P_{U}(X))\end{aligned}}}
  
это число битов, которые должны быть переданы для идентификации 
  
    
      
        X
      
    
    {\displaystyle X}
   из 
  
    
      
        N
      
    
    {\displaystyle N}
   одинаково вероятных исходов, это меньше, чем РКЛ равномерного распределения 
  
    
      
        
          P
          
            U
          
        
        (
        X
        )
      
    
    {\displaystyle P_{U}(X)}
   из истинного распределения 
  
    
      
        P
        (
        X
        )
      
    
    {\displaystyle P(X)}
   — то есть меньше ожидаемого числа сохраненных битов, которые должны быть отправлены, если значение 
  
    
      
        X
      
    
    {\displaystyle X}
   закодировано согласно с равномерным распределением 
  
    
      
        
          P
          
            U
          
        
        (
        X
        )
      
    
    {\displaystyle P_{U}(X)}
  , а не истинным распределение 
  
    
      
        P
        (
        X
        )
      
    
    {\displaystyle P(X)}
  .
Условная энтропия -

  
    
      
        
          
            
              
                
                  H
                
                (
                X
                ∣
                Y
                )
              
              
                
                =
                log
                ⁡
                (
                N
                )
                −
                
                  D
                  
                    KL
                  
                
                (
                P
                (
                X
                ,
                Y
                )
                ∥
                
                  P
                  
                    U
                  
                
                (
                X
                )
                P
                (
                Y
                )
                )
              
            
            
              
              
                
                =
                log
                ⁡
                (
                N
                )
                −
                
                  D
                  
                    KL
                  
                
                (
                P
                (
                X
                ,
                Y
                )
                ∥
                P
                (
                X
                )
                P
                (
                Y
                )
                )
                −
                
                  D
                  
                    KL
                  
                
                (
                P
                (
                X
                )
                ∥
                
                  P
                  
                    U
                  
                
                (
                X
                )
                )
              
            
            
              
              
                
                =
                
                  H
                
                (
                X
                )
                −
                I
                ⁡
                (
                X
                ;
                Y
                )
              
            
            
              
              
                
                =
                log
                ⁡
                (
                N
                )
                −
                
                  E
                  
                    Y
                  
                
                ⁡
                
                  
                    [
                  
                
                
                  D
                  
                    KL
                  
                
                (
                P
                (
                X
                ∣
                Y
                )
                ∥
                
                  P
                  
                    U
                  
                
                (
                X
                )
                )
                
                  
                    ]
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}\mathrm {H} (X\mid Y)&=\log(N)-D_{\text{KL}}(P(X,Y)\parallel P_{U}(X)P(Y))\\&=\log(N)-D_{\text{KL}}(P(X,Y)\parallel P(X)P(Y))-D_{\text{KL}}(P(X)\parallel P_{U}(X))\\&=\mathrm {H} (X)-\operatorname {I} (X;Y)\\&=\log(N)-\operatorname {E} _{Y}{\bigl [}D_{\text{KL}}(P(X\mid Y)\parallel P_{U}(X)){\bigr ]}\end{aligned}}}
  
это число битов, которые должны быть переданы для идентификации 
  
    
      
        X
      
    
    {\displaystyle X}
   из 
  
    
      
        N
      
    
    {\displaystyle N}
   одинаково вероятных исходов, это меньше, чем РКЛ произведения распределений 
  
    
      
        
          P
          
            U
          
        
        (
        X
        )
      
    
    {\displaystyle P_{U}(X)}
   из истинного совместного распределения 
  
    
      
        P
        (
        X
        ,
        Y
        )
      
    
    {\displaystyle P(X,Y)}
   — то есть меньше ожидаемого числа сохраненных битов, которые должны быть отправлены, если значение 
  
    
      
        X
      
    
    {\displaystyle X}
   закодировано согласно с равномерным распределением 
  
    
      
        
          P
          
            U
          
        
        (
        X
        )
      
    
    {\displaystyle P_{U}(X)}
  , а не с условным распределением 
  
    
      
        P
        (
        X
        ∣
        Y
        )
      
    
    {\displaystyle P(X\mid Y)}
   данных 
  
    
      
        X
      
    
    {\displaystyle X}
   и 
  
    
      
        Y
      
    
    {\displaystyle Y}
  .
Перекрестная энтропия между двумя вероятностными распределениями измеряет среднее число битов, необходимых для определения события из множества возможных, если использована схема кодирования, основанная на данном распределении вероятности 
  
    
      
        Q
      
    
    {\displaystyle Q}
  , а не «истинного» распределения 
  
    
      
        P
      
    
    {\displaystyle P}
  . Перекрестная энтропия для двух распределений 
  
    
      
        P
      
    
    {\displaystyle P}
   и 
  
    
      
        Q
      
    
    {\displaystyle Q}
   над тем же вероятностным пространством определяется так: 
  
    
      
        H
        (
        p
        ,
        q
        )
        =
        
          E
          
            p
          
        
        ⁡
        [
        −
        log
        ⁡
        q
        ]
        =
        H
        (
        p
        )
        +
        
          D
          
            
              K
              L
            
          
        
        (
        p
        ∥
        q
        )
        .
      
    
    {\displaystyle H(p,q)=\operatorname {E} _{p}[-\log q]=H(p)+D_{\mathrm {KL} }(p\parallel q).}

Расстояние Кульбака — Лейблера и Байесовская модификация
В Байесовской статистике Расстояние Кульбака — Лейблера может быть использовано как мера информационного выигрыша при переходе от априорного к апостериорному вероятностному распределению. Если обнаружен некоторый новый факт 
  
    
      
        Y
        =
        y
      
    
    {\displaystyle Y=y}
  , оно может быть использовано для модификации (априорного) распределения вероятностей 
  
    
      
        p
        (
        x
        ∣
        I
        )
      
    
    {\displaystyle p(x\mid I)}
   для 
  
    
      
        X
      
    
    {\displaystyle X}
   в новое (апостериорное) распределение вероятностей 
  
    
      
        p
        (
        x
        ∣
        y
        ,
        I
        )
      
    
    {\displaystyle p(x\mid y,I)}
   используя Теорему Байеса:

  
    
      
        p
        (
        x
        ∣
        y
        ,
        I
        )
        =
        
          
            
              p
              (
              y
              ∣
              x
              ,
              I
              )
              p
              (
              x
              ∣
              I
              )
            
            
              p
              (
              y
              ∣
              I
              )
            
          
        
        .
      
    
    {\displaystyle p(x\mid y,I)={\frac {p(y\mid x,I)p(x\mid I)}{p(y\mid I)}}.}
  Это распределение имеет новую энтропию

  
    
      
        H
        
          
            (
          
        
        p
        (
        ⋅
        ∣
        y
        ,
        I
        )
        
          
            )
          
        
        =
        −
        
          ∑
          
            x
          
        
        p
        (
        x
        ∣
        y
        ,
        I
        )
        log
        ⁡
        p
        (
        x
        ∣
        y
        ,
        I
        )
        ,
      
    
    {\displaystyle H{\big (}p(\cdot \mid y,I){\big )}=-\sum _{x}p(x\mid y,I)\log p(x\mid y,I),}
  которая может быть меньше или больше, чем изначальная энтропия 
  
    
      
        H
        
          
            (
          
        
        p
        (
        ⋅
        ∣
        I
        )
        
          
            )
          
        
      
    
    {\displaystyle H{\big (}p(\cdot \mid I){\big )}}
  . Однако, с точки зрения нового распределения вероятностей можно оценить, что использование исходного кода, основанного на 
  
    
      
        p
        (
        x
        ∣
        I
        )
      
    
    {\displaystyle p(x\mid I)}
   вместо нового кода, основанного на 
  
    
      
        p
        (
        x
        ∣
        y
        ,
        I
        )
      
    
    {\displaystyle p(x\mid y,I)}
  , добавило бы ожидаемое число битов — 
  
    
      
        
          D
          
            
              K
              L
            
          
        
        
          
            (
          
        
        p
        (
        ⋅
        ∣
        y
        ,
        I
        )
        ∣
        p
        (
        ⋅
        ∣
        I
        )
        
          
            )
          
        
        =
        
          ∑
          
            x
          
        
        p
        (
        x
        ∣
        y
        ,
        I
        )
        log
        ⁡
        
          
            
              p
              (
              x
              ∣
              y
              ,
              I
              )
            
            
              p
              (
              x
              ∣
              I
              )
            
          
        
      
    
    {\displaystyle D_{\mathrm {KL} }{\big (}p(\cdot \mid y,I)\mid p(\cdot \mid I){\big )}=\sum _{x}p(x\mid y,I)\log {\frac {p(x\mid y,I)}{p(x\mid I)}}}
   к длине сообщения. Это, таким образом, представляет собой количество полезной информации, или информационного выигрыша, касательно 
  
    
      
        X
      
    
    {\displaystyle X}
  , которое было получено при обнаружении, что 
  
    
      
        Y
        =
        y
      
    
    {\displaystyle Y=y}
  .
Если впоследствии приходит еще один фрагмент данных, 
  
    
      
        
          Y
          
            2
          
        
        =
        
          y
          
            2
          
        
      
    
    {\displaystyle Y_{2}=y_{2}}
  , то вероятностное распределение для x может быть обновлено далее, чтобы дать новое лучшее предположение 
  
    
      
        p
        (
        x
        ∣
        
          y
          
            1
          
        
        ,
        
          y
          
            2
          
        
        ,
        I
        )
      
    
    {\displaystyle p(x\mid y_{1},y_{2},I)}
  . Если исследовать заново информационный выигрыш для использования 
  
    
      
        p
        (
        x
        ∣
        
          y
          
            1
          
        
        ,
        I
        )
      
    
    {\displaystyle p(x\mid y_{1},I)}
  , а не 
  
    
      
        p
        (
        x
        ∣
        I
        )
      
    
    {\displaystyle p(x\mid I)}
  , оказывается, что это может быть больше или меньше, чем предполагалось ранее: 
  
    
      
        
          ∑
          
            x
          
        
        p
        (
        x
        ∣
        
          y
          
            1
          
        
        ,
        
          y
          
            2
          
        
        ,
        I
        )
        log
        ⁡
        
          
            
              p
              (
              x
              ∣
              
                y
                
                  1
                
              
              ,
              
                y
                
                  2
                
              
              ,
              I
              )
            
            
              p
              (
              x
              ∣
              I
              )
            
          
        
      
    
    {\displaystyle \sum _{x}p(x\mid y_{1},y_{2},I)\log {\frac {p(x\mid y_{1},y_{2},I)}{p(x\mid I)}}}
  , может быть 
  
    
      
        ≤
      
    
    {\displaystyle \leq }
   или 
  
    
      
        >
      
    
    {\displaystyle >}
  , чем 
  
    
      
        
          
            ∑
            
              x
            
          
          p
          (
          x
          ∣
          
            y
            
              1
            
          
          ,
          I
          )
          log
          ⁡
          
            
              
                p
                (
                x
                ∣
                
                  y
                  
                    1
                  
                
                ,
                I
                )
              
              
                p
                (
                x
                ∣
                I
                )
              
            
          
        
      
    
    {\displaystyle \displaystyle \sum _{x}p(x\mid y_{1},I)\log {\frac {p(x\mid y_{1},I)}{p(x\mid I)}}}
  , и поэтому общий информационный выигрыш не выполняет неравенство треугольника:

  
    
      
        
          D
          
            
              K
              L
            
          
        
        
          
            (
          
        
        p
        (
        ⋅
        ∣
        
          y
          
            1
          
        
        ,
        
          y
          
            2
          
        
        ,
        I
        )
        ∥
        p
        (
        ⋅
        ∣
        I
        )
        
          
            )
          
        
      
    
    {\displaystyle D_{\mathrm {KL} }{\big (}p(\cdot \mid y_{1},y_{2},I)\parallel p(\cdot \mid I){\big )}}
  , может быть больше, меньше или равно 
  
    
      
        
          D
          
            
              K
              L
            
          
        
        
          
            (
          
        
        p
        (
        ⋅
        ∣
        
          y
          
            1
          
        
        ,
        
          y
          
            2
          
        
        ,
        I
        )
        ∥
        p
        (
        ⋅
        ∣
        
          y
          
            1
          
        
        ,
        I
        )
        
          
            )
          
        
        +
        
          D
          
            
              K
              L
            
          
        
        
          
            (
          
        
        p
        (
        ⋅
        ∣
        
          y
          
            1
          
        
        ,
        I
        )
        ∥
        p
        (
        x
        ∣
        I
        )
        
          
            )
          
        
        .
      
    
    {\displaystyle D_{\mathrm {KL} }{\big (}p(\cdot \mid y_{1},y_{2},I)\parallel p(\cdot \mid y_{1},I){\big )}+D_{\mathrm {KL} }{\big (}p(\cdot \mid y_{1},I)\parallel p(x\mid I){\big )}.}
  
Все, что можно сказать, что в среднем, беря среднее, используя 
  
    
      
        p
        (
        
          y
          
            2
          
        
        ∣
        
          y
          
            1
          
        
        ,
        x
        ,
        I
        )
      
    
    {\displaystyle p(y_{2}\mid y_{1},x,I)}
  , обе стороны будут давать среднее значение.

Экспериментальная модель Байеса
Широко распространённая цель в экспериментальной модели Байеса — максимизировать ожидаемое РКЛ между априорным и апостериорным распределениями. Когда апостериорное приближено к Гауссовому распределению, модель, максимизирующая ожидаемое РКЛ, называется Байеса d-оптимальное.

Различающая информация
Расстояние Кульбака — Лейблера 
  
    
      
        
          D
          
            
              K
              L
            
          
        
        (
        p
        (
        x
        ∣
        
          H
          
            1
          
        
        )
        ∥
        p
        (
        x
        ∣
        
          H
          
            0
          
        
        )
        )
      
    
    {\displaystyle D_{\mathrm {KL} }(p(x\mid H_{1})\parallel p(x\mid H_{0}))}
   может также быть интерпретировано как ожидаемая различающая информация для 
  
    
      
        
          H
          
            1
          
        
      
    
    {\displaystyle H_{1}}
  над 
  
    
      
        
          H
          
            0
          
        
      
    
    {\displaystyle H_{0}}
  : средняя информация на одну выборку для различия в пользу гипотезы 
  
    
      
        
          H
          
            1
          
        
      
    
    {\displaystyle H_{1}}
  , против гипотезы 
  
    
      
        
          H
          
            0
          
        
      
    
    {\displaystyle H_{0}}
  , когда гипотеза 
  
    
      
        
          H
          
            1
          
        
      
    
    {\displaystyle H_{1}}
   верна. Еще одно имя для этой величины, данное Ирвингом Джоном Гудом, это ожидаемая масса доказательства для 
  
    
      
        
          H
          
            1
          
        
      
    
    {\displaystyle H_{1}}
  над 
  
    
      
        
          H
          
            0
          
        
      
    
    {\displaystyle H_{0}}
  , ожидаемая из каждой выборки.
Ожидаемая масса доказательства для 
  
    
      
        
          H
          
            1
          
        
      
    
    {\displaystyle H_{1}}
  над 
  
    
      
        
          H
          
            0
          
        
      
    
    {\displaystyle H_{0}}
   это не то же что информационный выигрыш, ожидаемый, например, для вероятностного распределения p(H) гипотезы, 
  
    
      
        
          D
          
            
              K
              L
            
          
        
        (
        p
        (
        x
        ∣
        
          H
          
            1
          
        
        )
        ∥
        p
        (
        x
        ∣
        
          H
          
            0
          
        
        )
        )
        ≠
        I
        G
        =
        
          D
          
            
              K
              L
            
          
        
        (
        p
        (
        H
        ∣
        x
        )
        ∥
        p
        (
        H
        ∣
        I
        )
        )
        .
      
    
    {\displaystyle D_{\mathrm {KL} }(p(x\mid H_{1})\parallel p(x\mid H_{0}))\neq IG=D_{\mathrm {KL} }(p(H\mid x)\parallel p(H\mid I)).}
  .
Любая из двух величин может быть использована как функция полезности в Байесовской экспериментальной форме, для выбора оптимального следующего вопроса для исследования, но вообще они приведут скорее к разным экспериментальным стратегиям.
В шкале энтропии информационного выигрыша очень маленькая разница между почти уверенностью и полной уверенностью — кодирование с почти полной уверенностью вряд ли потребует больше битов, чем кодирование с полной уверенностью. С другой стороны, в logit шкале подразумевается вес доказательств, и разница между двумя огромна, едва ли не бесконечна. Это может отражать разницу между почти уверенностью (на вероятностном уровне), скажем, в том, что Гипотеза Римана верна, и с полной уверенностью, что она верна, потому что имеется математическое доказательство. Две разные шкалы функции потерь для неопределенности обе являются полезными, согласно с тем, насколько хорошо каждая отражает конкретные обстоятельства рассматриваемой проблемы в задаче.

Принцип минимальной различающей информации
Идея РКЛ как различающей информации привела Кульбака к предположению Принципа Минимальной различающей информации (англ. Minimum Discrimination Information, MDI): учитывая новые факты, новое распределение 
  
    
      
        f
      
    
    {\displaystyle f}
   следует выбрать, из тех, которые трудно отличить от первоначального распределения 
  
    
      
        
          f
          
            0
          
        
      
    
    {\displaystyle f_{0}}
  ; потому что новые данные производят так мало информационного выигрыша 
  
    
      
        
          D
          
            K
            L
          
        
        (
        f
        ∥
        
          f
          
            0
          
        
        )
      
    
    {\displaystyle D_{KL}(f\parallel f_{0})}
   как только возможно.
Например, если мы имеем априорное распределение 
  
    
      
        p
        (
        x
        ,
        a
        )
      
    
    {\displaystyle p(x,a)}
   над 
  
    
      
        x
      
    
    {\displaystyle x}
   и 
  
    
      
        a
      
    
    {\displaystyle a}
  , и потом изучим истинное распределение 
  
    
      
        a
      
    
    {\displaystyle a}
   и 
  
    
      
        u
        (
        a
        )
      
    
    {\displaystyle u(a)}
  . РКЛ между новым совместным распределением для 
  
    
      
        x
      
    
    {\displaystyle x}
   и 
  
    
      
        a
      
    
    {\displaystyle a}
  , 
  
    
      
        q
        (
        x
        ∣
        a
        )
        u
        (
        a
        )
      
    
    {\displaystyle q(x\mid a)u(a)}
  , и прежнего априорного распределения было бы: 
  
    
      
        
          D
          
            
              K
              L
            
          
        
        (
        q
        (
        x
        ∣
        a
        )
        u
        (
        a
        )
        ∥
        p
        (
        x
        ,
        a
        )
        )
        =
        
          E
          
            u
            (
            a
            )
          
        
        ⁡
        {
        
          D
          
            
              K
              L
            
          
        
        (
        q
        (
        x
        ∣
        a
        )
        ∥
        p
        (
        x
        ∣
        a
        )
        )
        }
        +
        
          D
          
            
              K
              L
            
          
        
        (
        u
        (
        a
        )
        ∥
        p
        (
        a
        )
        )
        ,
      
    
    {\displaystyle D_{\mathrm {KL} }(q(x\mid a)u(a)\parallel p(x,a))=\operatorname {E} _{u(a)}\{D_{\mathrm {KL} }(q(x\mid a)\parallel p(x\mid a))\}+D_{\mathrm {KL} }(u(a)\parallel p(a)),}
  
то есть сумма РКЛ 
  
    
      
        p
        (
        a
        )
      
    
    {\displaystyle p(a)}
   априорного распределения для 
  
    
      
        a
      
    
    {\displaystyle a}
   из обновленного распределения 
  
    
      
        u
        (
        a
        )
      
    
    {\displaystyle u(a)}
  , плюс ожидаемое значение (используемое вероятностное распределение 
  
    
      
        u
        (
        a
        )
      
    
    {\displaystyle u(a)}
  ) РКЛ априорного условного распределения 
  
    
      
        p
        (
        x
        ∣
        a
        )
      
    
    {\displaystyle p(x\mid a)}
   из нового распределения 
  
    
      
        p
        (
        x
        ∣
        a
        )
      
    
    {\displaystyle p(x\mid a)}
  . (Заметьте что часто позднее ожидаемое значение называется условное РКЛ (или условная относительная энтропия) и обозначается 
  
    
      
        
          D
          
            K
            L
          
        
        (
        q
        (
        x
        ∣
        a
        )
        ∥
        p
        (
        x
        ∣
        a
        )
        )
      
    
    {\displaystyle D_{KL}(q(x\mid a)\parallel p(x\mid a))}
  . Это минимизирует, если 
  
    
      
        q
        (
        x
        ∣
        a
        )
        =
        p
        (
        x
        ∣
        a
        )
      
    
    {\displaystyle q(x\mid a)=p(x\mid a)}
   над общим содержанием 
  
    
      
        u
        (
        a
        )
      
    
    {\displaystyle u(a)}
  . И мы замечаем что этот результат объединяет теорему Байеса, если новое распределение 
  
    
      
        u
        (
        a
        )
      
    
    {\displaystyle u(a)}
   это по факту функция, уверенно представляющая, что 
  
    
      
        a
      
    
    {\displaystyle a}
   имеет одно частное значение.
Минимальная различающая информация может быть рассмотрена как расширение Принципа безразличия Лапласа (другое его название — принцип недостаточного основания) и Принципа максимума энтропии Джейнса. В частности, это естественное расширение принципа максимума энтропии из дискретного до непрерывного распределения, для которого энтропия Шеннона прекращается, чтобы быть очень удобной (см. дифференциальная энтропия), но РКЛ продолжает быть столь же актуальной.
В инженерной литературе, MDI иногда называется принципом минимума перекрестной энтропии. Минимизация РКЛ 
  
    
      
        m
      
    
    {\displaystyle m}
   из 
  
    
      
        p
      
    
    {\displaystyle p}
   в отношении 
  
    
      
        m
      
    
    {\displaystyle m}
   эквивалентна минимизации перекрестной энтропии 
  
    
      
        p
      
    
    {\displaystyle p}
   и 
  
    
      
        m
      
    
    {\displaystyle m}
  , так 
  
    
      
        H
        (
        p
        ,
        m
        )
        =
        H
        (
        p
        )
        +
        
          D
          
            
              K
              L
            
          
        
        (
        p
        ∥
        m
        )
        ,
      
    
    {\displaystyle H(p,m)=H(p)+D_{\mathrm {KL} }(p\parallel m),}
   который подходит, если попытаться выбрать точное приближенное значение до 
  
    
      
        p
      
    
    {\displaystyle p}
  .

Пример использования
Пусть по выборке 
  
    
      
        
          x
          
            1
          
        
        ,
        
          x
          
            2
          
        
        ,
        …
        ,
        
          x
          
            n
          
        
      
    
    {\displaystyle x_{1},x_{2},\dotsc ,x_{n}}
   из распределения некоторой случайной величины требуется восстановить плотность её распределения, заданную в виде параметрического семейства 
  
    
      
        f
        (
        x
        ,
        θ
        )
      
    
    {\displaystyle f(x,\theta )}
  , где 
  
    
      
        x
        ∈
        X
        ⊆
        R
      
    
    {\displaystyle x\in X\subseteq R}
   — аргумент функции, 
  
    
      
        θ
      
    
    {\displaystyle \theta }
   — неизвестный параметр. Оценка параметра 
  
    
      
        θ
      
    
    {\displaystyle \theta }
   может быть найдена как решение задачи минимизации расстояния Кульбака — Лейблера между плотностью 
  
    
      
        f
        (
        x
        ,
        θ
        )
      
    
    {\displaystyle f(x,\theta )}
   и эмпирической плотностью распределения, считающейся «истинной»,

  
    
      
        
          
            
              f
              ^
            
          
        
        (
        x
        )
        =
        
          
            1
            n
          
        
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          δ
        
        (
        x
        −
        
          x
          
            i
          
        
        )
      
    
    {\displaystyle {\hat {f}}(x)={\frac {1}{n}}\sum \limits _{i=1}^{n}\mathbf {\delta } (x-x_{i})}
  ,где 
  
    
      
        δ
      
    
    {\displaystyle \delta }
   — функция Дирака:

  
    
      
        
          
            
              θ
              ^
            
          
        
        =
        arg
        ⁡
        
          
            min
            θ
          
        
        
          D
          
            K
            L
          
        
        (
        
          
            
              f
              ^
            
          
        
        (
        x
        )
        ,
        f
        (
        x
        ,
        θ
        )
        )
        =
        arg
        ⁡
        
          
            max
            θ
          
        
        
          ∫
          
            X
          
          

          
        
        
          
            
              f
              ^
            
          
        
        (
        x
        )
        ln
        ⁡
        f
        (
        x
        ,
        θ
        )
        
        d
        x
        =
        arg
        ⁡
        
          
            max
            θ
          
        
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          ln
        
        f
        (
        
          x
          
            i
          
        
        ,
        θ
        )
      
    
    {\displaystyle {\hat {\theta }}=\operatorname {arg} {\underset {\theta }{\operatorname {min} }}D_{KL}({\hat {f}}(x),f(x,\theta ))=\operatorname {arg} {\underset {\theta }{\operatorname {max} }}\int \limits _{X}^{}{\hat {f}}(x)\ln f(x,\theta )\,dx=\operatorname {arg} {\underset {\theta }{\operatorname {max} }}\sum \limits _{i=1}^{n}\mathbf {\ln } f(x_{i},\theta )}
  .Нетрудно видеть, что решение этой задачи приводит к оценке максимального правдоподобия для параметра 
  
    
      
        θ
      
    
    {\displaystyle \theta }
  . В случае если фактическая плотность распределения случайной величины не принадлежит семейству 
  
    
      
        f
        (
        x
        ,
        θ
        )
      
    
    {\displaystyle f(x,\theta )}
  , найденная оценка 
  
    
      
        
          
            
              θ
              ^
            
          
        
      
    
    {\displaystyle {\hat {\theta }}}
   параметра 
  
    
      
        θ
      
    
    {\displaystyle \theta }
   называется квазиправдоподобной и обеспечивает наилучшую аппроксимацию фактического распределения, представленного выборкой, среди распределений с плотностями 
  
    
      
        f
        (
        x
        ,
        θ
        )
      
    
    {\displaystyle f(x,\theta )}
   с точки зрения расстояния Кульбака — Лейблера.


== Примечания ==