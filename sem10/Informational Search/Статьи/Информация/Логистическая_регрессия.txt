Логистическая регрессия или логит-регрессия (англ. logit model) — это статистическая модель, используемая для прогнозирования вероятности возникновения некоторого события путём подгонки данных к логистической кривой.

Описание
Логистическая регрессия применяется для прогнозирования вероятности возникновения некоторого события по значениям множества признаков. Для этого вводится так называемая зависимая переменная 
  
    
      
        y
      
    
    {\displaystyle y}
  , принимающая лишь одно из двух значений — как правило, это числа 0 (событие не произошло) и 1 (событие произошло), и множество независимых переменных (также называемых признаками, предикторами или регрессорами) — вещественных 
  
    
      
        
          x
          
            1
          
        
        ,
        
          x
          
            2
          
        
        ,
        .
        .
        .
        ,
        
          x
          
            n
          
        
      
    
    {\displaystyle x_{1},x_{2},...,x_{n}}
  , на основе значений которых требуется вычислить вероятность принятия того или иного значения зависимой переменной. Как и в случае линейной регрессии, для простоты записи вводится фиктивный признак 
  
    
      
        
          x
          
            0
          
        
        =
        1.
      
    
    {\displaystyle x_{0}=1.}
  
Делается предположение о том, что вероятность наступления события 
  
    
      
        y
        =
        1
      
    
    {\displaystyle y=1}
   равна:

  
    
      
        
          P
        
        {
        y
        =
        1
        ∣
        x
        }
        =
        f
        (
        z
        )
        ,
      
    
    {\displaystyle \mathbb {P} \{y=1\mid x\}=f(z),}
  где 
  
    
      
        z
        =
        
          θ
          
            T
          
        
        x
        =
        
          θ
          
            0
          
        
        +
        
          θ
          
            1
          
        
        
          x
          
            1
          
        
        +
        …
        +
        
          θ
          
            n
          
        
        
          x
          
            n
          
        
      
    
    {\displaystyle z=\theta ^{T}x=\theta _{0}+\theta _{1}x_{1}+\ldots +\theta _{n}x_{n}}
  , 
  
    
      
        x
      
    
    {\displaystyle x}
   и 
  
    
      
        θ
      
    
    {\displaystyle \theta }
   — векторы-столбцы значений независимых переменных 
  
    
      
        1
        ,
        
          x
          
            1
          
        
        ,
        …
        ,
        
          x
          
            n
          
        
      
    
    {\displaystyle 1,x_{1},\dots ,x_{n}}
   и параметров (коэффициентов регрессии) — вещественных чисел 
  
    
      
        
          θ
          
            0
          
        
        ,
        .
        .
        .
        ,
        
          θ
          
            n
          
        
      
    
    {\displaystyle \theta _{0},...,\theta _{n}}
  , соответственно, а 
  
    
      
        f
        (
        z
        )
      
    
    {\displaystyle f(z)}
   — так называемая логистическая функция (иногда также называемая сигмоидом или логит-функцией):

  
    
      
        f
        (
        z
        )
        =
        
          
            1
            
              1
              +
              
                e
                
                  −
                  z
                
              
            
          
        
        .
      
    
    {\displaystyle f(z)={\frac {1}{1+e^{-z}}}.}
  Так как 
  
    
      
        y
      
    
    {\displaystyle y}
   принимает лишь значения 0 и 1, то вероятность принять значение 0 равна:

  
    
      
        
          P
        
        {
        y
        =
        0
        ∣
        x
        }
        =
        1
        −
        f
        (
        z
        )
        =
        1
        −
        f
        (
        
          θ
          
            T
          
        
        x
        )
        .
      
    
    {\displaystyle \mathbb {P} \{y=0\mid x\}=1-f(z)=1-f(\theta ^{T}x).}
  Для краткости функцию распределения 
  
    
      
        y
      
    
    {\displaystyle y}
   при заданном 
  
    
      
        x
      
    
    {\displaystyle x}
   можно записать в таком виде:

  
    
      
        
          P
        
        {
        y
        ∣
        x
        }
        =
        f
        (
        
          θ
          
            T
          
        
        x
        
          )
          
            y
          
        
        (
        1
        −
        f
        (
        
          θ
          
            T
          
        
        x
        )
        
          )
          
            1
            −
            y
          
        
        ,
        
        y
        ∈
        {
        0
        ,
        1
        }
        .
      
    
    {\displaystyle \mathbb {P} \{y\mid x\}=f(\theta ^{T}x)^{y}(1-f(\theta ^{T}x))^{1-y},\quad y\in \{0,1\}.}
  Фактически, это есть распределение Бернулли с параметром, равным 
  
    
      
        f
        (
        
          θ
          
            T
          
        
        x
        )
      
    
    {\displaystyle f(\theta ^{T}x)}
  .

Подбор параметров
Для подбора параметров 
  
    
      
        
          θ
          
            0
          
        
        ,
        .
        .
        .
        ,
        
          θ
          
            n
          
        
      
    
    {\displaystyle \theta _{0},...,\theta _{n}}
   необходимо составить обучающую выборку, состоящую из наборов значений независимых переменных и соответствующих им значений зависимой переменной 
  
    
      
        y
      
    
    {\displaystyle y}
  . Формально, это множество пар 
  
    
      
        (
        
          x
          
            (
            1
            )
          
        
        ,
        
          y
          
            (
            1
            )
          
        
        )
        ,
        .
        .
        .
        ,
        (
        
          x
          
            (
            m
            )
          
        
        ,
        
          y
          
            (
            m
            )
          
        
        )
      
    
    {\displaystyle (x^{(1)},y^{(1)}),...,(x^{(m)},y^{(m)})}
  , где 
  
    
      
        
          x
          
            (
            i
            )
          
        
        ∈
        
          
            R
          
          
            n
          
        
      
    
    {\displaystyle x^{(i)}\in \mathbb {R} ^{n}}
   — вектор значений независимых переменных, а 
  
    
      
        
          y
          
            (
            i
            )
          
        
        ∈
        {
        0
        ,
        1
        }
      
    
    {\displaystyle y^{(i)}\in \{0,1\}}
   — соответствующее им значение 
  
    
      
        y
      
    
    {\displaystyle y}
  . Каждая такая пара называется обучающим примером.
Обычно используется метод максимального правдоподобия, согласно которому выбираются параметры 
  
    
      
        θ
      
    
    {\displaystyle \theta }
  , максимизирующие значение функции правдоподобия на обучающей выборке:

  
    
      
        
          
            
              θ
              ^
            
          
        
        =
        
          argmax
          
            θ
          
        
        ⁡
        L
        (
        θ
        )
        =
        
          argmax
          
            θ
          
        
        ⁡
        
          ∏
          
            i
            =
            1
          
          
            m
          
        
        
          P
        
        {
        y
        =
        
          y
          
            (
            i
            )
          
        
        ∣
        x
        =
        
          x
          
            (
            i
            )
          
        
        }
        .
      
    
    {\displaystyle {\hat {\theta }}=\operatorname {argmax} _{\theta }L(\theta )=\operatorname {argmax} _{\theta }\prod _{i=1}^{m}\mathbb {P} \{y=y^{(i)}\mid x=x^{(i)}\}.}
  Максимизация функции правдоподобия эквивалентна максимизации её логарифма:

  
    
      
        ln
        ⁡
        L
        (
        θ
        )
        =
        
          ∑
          
            i
            =
            1
          
          
            m
          
        
        log
        ⁡
        
          P
        
        {
        y
        =
        
          y
          
            (
            i
            )
          
        
        ∣
        x
        =
        
          x
          
            (
            i
            )
          
        
        }
        =
        
          ∑
          
            i
            =
            1
          
          
            m
          
        
        
          y
          
            (
            i
            )
          
        
        ln
        ⁡
        f
        (
        
          θ
          
            T
          
        
        
          x
          
            (
            i
            )
          
        
        )
        +
        (
        1
        −
        
          y
          
            (
            i
            )
          
        
        )
        ln
        ⁡
        (
        1
        −
        f
        (
        
          θ
          
            T
          
        
        
          x
          
            (
            i
            )
          
        
        )
        )
      
    
    {\displaystyle \ln L(\theta )=\sum _{i=1}^{m}\log \mathbb {P} \{y=y^{(i)}\mid x=x^{(i)}\}=\sum _{i=1}^{m}y^{(i)}\ln f(\theta ^{T}x^{(i)})+(1-y^{(i)})\ln(1-f(\theta ^{T}x^{(i)}))}
  , где 
  
    
      
        
          θ
          
            T
          
        
        
          x
          
            (
            i
            )
          
        
        =
        
          θ
          
            0
          
        
        +
        
          θ
          
            1
          
        
        
          x
          
            1
          
          
            (
            i
            )
          
        
        +
        ⋯
        +
        
          θ
          
            n
          
        
        
          x
          
            n
          
          
            (
            i
            )
          
        
        .
      
    
    {\displaystyle \theta ^{T}x^{(i)}=\theta _{0}+\theta _{1}x_{1}^{(i)}+\dots +\theta _{n}x_{n}^{(i)}.}
  Для максимизации этой функции может быть применён, например, метод градиентного спуска. Он заключается в выполнении следующих итераций, начиная с некоторого начального значения параметров 
  
    
      
        θ
      
    
    {\displaystyle \theta }
  :

  
    
      
        θ
        :=
        θ
        +
        α
        ∇
        ln
        ⁡
        L
        (
        θ
        )
        =
        θ
        +
        α
        
          ∑
          
            i
            =
            1
          
          
            m
          
        
        (
        
          y
          
            (
            i
            )
          
        
        −
        f
        (
        
          θ
          
            T
          
        
        
          x
          
            (
            i
            )
          
        
        )
        )
        
          x
          
            (
            i
            )
          
        
        ,
        α
        >
        0.
      
    
    {\displaystyle \theta :=\theta +\alpha \nabla \ln L(\theta )=\theta +\alpha \sum _{i=1}^{m}(y^{(i)}-f(\theta ^{T}x^{(i)}))x^{(i)},\alpha >0.}
  На практике также применяют метод Ньютона и стохастический градиентный спуск.

Регуляризация
Для улучшения обобщающей способности получающейся модели, то есть уменьшения эффекта переобучения, на практике часто рассматривается логистическая регрессия с регуляризацией.
Регуляризация заключается в том, что вектор параметров 
  
    
      
        θ
      
    
    {\displaystyle \theta }
   рассматривается как случайный вектор с некоторой заданной априорной плотностью распределения 
  
    
      
        p
        (
        θ
        )
      
    
    {\displaystyle p(\theta )}
  . Для обучения модели вместо метода наибольшего правдоподобия при этом используется метод максимизации апостериорной оценки, то есть ищутся параметры 
  
    
      
        θ
      
    
    {\displaystyle \theta }
  , максимизирующие величину:

  
    
      
        
          ∏
          
            i
            =
            1
          
          
            m
          
        
        
          P
        
        {
        
          y
          
            (
            i
            )
          
        
        ∣
        
          x
          
            (
            i
            )
          
        
        ,
        θ
        }
        ⋅
        p
        (
        θ
        )
        .
      
    
    {\displaystyle \prod _{i=1}^{m}\mathbb {P} \{y^{(i)}\mid x^{(i)},\theta \}\cdot p(\theta ).}
  В качестве априорного распределения часто выступает многомерное нормальное распределение 
  
    
      
        
          
            N
          
        
        (
        0
        ,
        
          σ
          
            2
          
        
        I
        )
      
    
    {\displaystyle {\mathcal {N}}(0,\sigma ^{2}I)}
   с нулевым средним и матрицей ковариации 
  
    
      
        
          σ
          
            2
          
        
        I
      
    
    {\displaystyle \sigma ^{2}I}
  , соответствующее априорному убеждению о том, что все коэффициенты регрессии должны быть небольшими числами, идеально — многие малозначимые коэффициенты должны быть нулями. Подставив плотность этого априорного распределения в формулу выше, и прологарифмировав, получим следующую оптимизационную задачу:

  
    
      
        
          ∑
          
            i
            =
            1
          
          
            m
          
        
        log
        ⁡
        
          P
        
        {
        
          y
          
            (
            i
            )
          
        
        ∣
        
          x
          
            (
            i
            )
          
        
        ,
        θ
        }
        −
        λ
        ‖
        θ
        
          ‖
          
            2
          
        
        
        →
        
          
            max
          
        
        ,
      
    
    {\displaystyle \sum \limits _{i=1}^{m}\log \mathbb {P} \{y^{(i)}\mid x^{(i)},\theta \}-\lambda \|\theta \|^{2}\,\to {\mbox{max}},}
  где 
  
    
      
        λ
        =
        
          
            const
          
        
        
          /
        
        
          
            σ
            
              2
            
          
        
      
    
    {\displaystyle \lambda ={\mbox{const}}/{\sigma ^{2}}}
   — параметр регуляризации. Этот метод известен как L2-регуляризованная логистическая регрессия, так как в целевую функцию входит L2-норма вектора параметров для регуляризации.
Если вместо L2-нормы использовать L1-норму, что эквивалентно использованию распределения Лапласа, как априорного, вместо нормального, то получится другой распространённый вариант метода — L1-регуляризованная логистическая регрессия:

  
    
      
        
          ∑
          
            i
            =
            1
          
          
            m
          
        
        log
        ⁡
        
          P
        
        {
        
          y
          
            (
            i
            )
          
        
        ∣
        
          x
          
            (
            i
            )
          
        
        ,
        θ
        }
        −
        λ
        ‖
        θ
        
          ‖
          
            1
          
        
        
        →
        
          
            max
          
        
        .
      
    
    {\displaystyle \sum _{i=1}^{m}\log \mathbb {P} \{y^{(i)}\mid x^{(i)},\theta \}-\lambda \|\theta \|_{1}\,\to {\mbox{max}}.}

Применение
Эта модель часто применяется для решения задач классификации — объект 
  
    
      
        x
      
    
    {\displaystyle x}
   можно отнести к классу 
  
    
      
        y
        =
        1
      
    
    {\displaystyle y=1}
  , если предсказанная моделью вероятность 
  
    
      
        
          P
        
        {
        y
        =
        1
        ∣
        x
        }
        >
        0
        
          ,
        
        5
      
    
    {\displaystyle \mathbb {P} \{y=1\mid x\}>0{,}5}
  , и к классу 
  
    
      
        y
        =
        0
      
    
    {\displaystyle y=0}
   в противном случае. Получающиеся при этом правила классификации являются линейными классификаторами.

Связанные  методы
На логистическую регрессию очень похожа пробит-регрессия, отличающаяся от неё лишь другим выбором функции 
  
    
      
        f
        (
        z
        )
      
    
    {\displaystyle f(z)}
  . Softmax-регрессия обобщает логистическую регрессию на случай многоклассовой классификации, то есть когда зависимая переменная 
  
    
      
        y
      
    
    {\displaystyle y}
   принимает более двух значений. Все эти модели в свою очередь являются представителями широкого класса статистических моделей — обобщённых линейных моделей.

См. также
Модель бинарного выбора
Пробит-регрессия

Литература
Andrew Ng. Stanford CS229 Lecture Notes