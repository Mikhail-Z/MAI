Человеческая память ассоциативна, то есть некоторое воспоминание может порождать большую связанную с ним область. Один предмет напоминает нам о другом, а этот другой о третьем. Если позволить нашим мыслям, они будут перемещаться от предмета к предмету по цепочке умственных ассоциаций. Например, несколько музыкальных тактов могут вызвать целую гамму чувственных воспоминаний, включая пейзажи, звуки и запахи. Напротив, обычная компьютерная память является локально адресуемой, предъявляется адрес и извлекается информация по этому адресу.

Ассоциативная память и Искусственный интеллект
Искусственная нейронная сеть с обратной связью формирует ассоциативную память. Подобно человеческой памяти по заданной части нужной информации вся информация извлекается из «памяти».
Автоассоциативной памятью — называют память, которая может завершить или исправить образ, но не может ассоциировать полученный образ с другим образом. Данный факт является результатом одноуровневой структуры ассоциативной памяти, в которой вектор появляется на выходе тех же нейронов, на которые поступает входной вектор. Такие сети неустойчивы. Для устойчивой сети последовательные итерации приводят ко все меньшим изменениям выхода, пока в конце концов выход не становится постоянным. Для многих сетей процесс никогда не заканчивается. Неустойчивые сети обладают интересными свойствами и изучались в качестве примера хаотических систем. В определенном смысле, это может быть достигнуто и без обратных связей, например перцептроном для случаев когда устойчивость важнее изучения хаотических систем.
Гетероассоциативной памятью — называют память, в которой при поступлении стимула на один набор нейронов, реакция по обратной связи появляется на другом наборе нейронов.
Первая модель автоассоциативной памяти была разработана Хопфилдом — Нейронная сеть Хопфилда. Чтобы добиться устойчивости пришлось весовые коэффициенты выбирать так, чтобы образовывать энергетические минимумы в нужных вершинах единичного гиперкуба.
Впоследствии Коско развил идеи Хопфилда и разработал модель гетероассоциативной памяти — двунаправленная ассоциативная память (ДАП).
Но точно такого же результата можно добиться используя широкий класс рекуррентных нейронных сетей, классическим примером которых служит сеть Элмана, при этом проблема устойчивости отпадает, а на весовые коэффициенты не накладываются такие жесткие условия, благодаря чему сеть обладает большей емкостью. Кроме того, рекуррентные нейронные сети могут описывать конечный автомат, при этом не теряя всех преимуществ искусственных нейронных сетей

Ассоциативная память и Программирование
Ряд работ рассматривал возможности понятия ассоциативной памяти в применении к языкам программирования и аппаратной реализации процессора. И в качестве рабочего определения пользовались следующим:

Под ассоциативной памятью обычно понимается некоторый набор, или совокупность элементов, обладающих способностью хранить информацию. Доступ к этим элементам осуществляется одновременно и параллельно в соответствии с содержанием хранящихся в них данных, а не путём задания адреса или расположения элемента.

Но такое понимание ассоциативной памяти отражает, в сущности, лишь факт наличия взаимосвязей между данными и не имеет отношения к самому механизму хранения информации. Поэтому для обозначения такого механизма хранения информации используется термин «память с адресацией по содержанию» (ПАС).
Как только акцент был сделан на устройство «памяти с адресацией по содержанию», то стало возможным упростить требования к самому пониманию ассоциативности, и разработать устройства лишь в некотором смысле обладающие ассоциативностью. Так, например, первое, что было упрощено — это предположение, что параллелизм при выполнении операций поиска по существу не является принципиальной функциональной характеристикой.
Второе упрощение связано с отрицанием необходимости распределенной памяти, так как ассоциативности в смысле памяти, с адресацией по содержанию, формально можно добиться и без необходимости распределять между элементами памяти информацию. В противовес этому можно хранить единицу информации целостно в определенной ячейке, имея лишь информацию о непосредственных связях данной ячейки с другими — таким образом, мы приходим к пониманию семантических сетей. Данные принципы также используются при индексировании и поиске в современных базах данных. Конечно, в этом смыcле это упрощение противоречит идеям коннективизма (которые базируются на искусственных нейронных сетях), и плавно перетекает к идеям символизма.
Главное, что теряется при таком упрощении — это одно из удивительных свойств биологической памяти. Известно, что разного рода повреждения ткани мозга приводят к нарушениям функциональных характеристик памяти. Тем не менее оказалось исключительно трудно выделить в работе отдельных нейронных структур явления, связанные с локализацией функций памяти. Объяснение этого базируется на предположении, что в мозгу следы памяти представлены в виде пространственно распределенных структур, формируемых в результате некоторого преобразования первичных восприятий.
Но тем не менее, хоть при таком упрощении были потеряны ряд биологически правдоподобных свойств, что важно при моделировании мозга, но зато в техническом смысле стало понятно как реализовать память, адресуемую по содержанию. Благодаря этому появились идеи о хешировании, которые затем были реализованы как в языках программирования, так и при аппаратной реализации некоторых процессоров.
Третье упрощение связано с точностью совпадения искомой информации. Выборка данных на основе их содержания всегда предполагает ту или иную форму сравнения задаваемого извне ключа, по которому должен осуществляться поиск, с некоторой частью или со всей информацией, которая хранится в ячейках памяти. Целью сравнения не всегда должно быть появление информации, совпадающей с ключевой. Например, при отыскании значений, которые расположены внутри заданного интервала. В этом случае мы имеем классический способ использования SQL при отборе из базы данных. Но возможен вариант поиска, при котором необходимо среди совокупности данных найти те, которые наилучшем образом (в смысле некоторой заданной меры) соответствуют ключевой информации.
В такой постановке задача ассоциативной выборки весьма близка к задаче распознавания образов. Но определяющим является методы, которые при этом используются — если смысл ассоциативности не подвержен описанным здесь упрощениям, то мы имеем дело с распознаванием образов с помощью искусственных нейронных сетей, в противном случае мы имеем дело с оптимизацией работы баз данных (а также аппаратных кэшей процессоров), или способами ассоциативного представления данных (например, семантических сетей). Отсюда должно быть понятно, что ассоциативное представление данных, и некоторые приемы работы с памятью, адресуемой по содержанию недостаточны для пониманием под этим ассоциативной памяти в полном смысле слова.
Четвертое упрощение может быть связано с так называемой проблемой временных ассоциаций, что с точки зрения программирования относится к теории автоматов. Эти проблемы связаны с разработкой методов запоминания и извлечения из памяти упорядоченных во времени последовательностей. При этом они могут разветвляться, образуя вторичные альтернативные последовательности, причем переход к одной из них определяется содержанием некоторой фоновой, или контекстной информации. Указанные последовательности могут также содержать замкнутые циклы.
Таким образом, с точки зрения программирования или символизма, по отношению к ассоциативной памяти имеются все те же проблемы и задачи, как и в искусственном интеллекте. Различием является то, что в программировании могут быть сделаны упрощения и построены методы, которые лишь частично удовлетворяют пониманию ассоциативной памяти. В то время как коннективизм пытается решить проблему ассоциативной памяти, используя методы которые не содержат упрощений в описанных здесь смыслах, обладают некоторой стохастичностью и непредсказуемостью в смысле работы метода, но в конечном счете дающих осмысленный результат в областях распознавания образов или адаптационного управления.

См. также
Семантическая сеть
Ассоциативная память

Литература
Ф. Вассерман. Нейрокомпьютерная техника: Теория и практика. — М.: «Мир», 1992.
Саймон Хайкин. Нейронные сети: полный курс = Neural Networks: A Comprehensive Foundation. — 2-е изд. — М.: «Вильямс», 2006. — С. 1104. — ISBN 0-13-273350-1.
Т. Кохонен. Ассоциативные запоминающие устройства = Content-Addressable Memories. — М.: "Мир", 1982.