Теория информации — раздел прикладной математики, радиотехники (теория обработки сигналов) и информатики, относящийся к измерению количества информации, её свойств и устанавливающий предельные соотношения для систем передачи данных. Как и любая математическая теория, теория оперирует математическими моделями, а не реальными физическими объектами (источниками и каналами связи). Использует, главным образом, математический аппарат теории вероятностей и математической статистики.
Основные разделы теории информации — кодирование источника (сжимающее кодирование) и канальное (помехоустойчивое) кодирование. Теория информации тесно связана с информационной энтропией, коммуникационными системами, криптографией и другими смежными дисциплинами.

Введение
Появление теории информации связано с опубликованием Клодом Шенноном работы «Математическая теория связи» в 1948 году. С точки зрения Шеннона, теория информации — раздел математической теории связи. Теория информации устанавливает основные границы возможностей систем передачи информации, задает исходные принципы их разработки и практического воплощения. Круг задач теории информации представляется с помощью структурной схемы, типичной системы передачи или хранения информации.

В схеме источником является любой объект вселенной, порождающий сообщения, которые должны быть перемещены в пространстве и времени. Независимо от изначальной физической природы, все подлежащие передаче сообщения обычно преобразуются в форму электрических сигналов, такие сигналы и рассматриваются как выход источника.
Кодер источника представляет информацию в наиболее компактной форме.
Кодер канала обрабатывает информацию для защиты сообщений от помех при передаче по каналу связи или возможных искажений при хранении информации.
Модулятор преобразовывает сообщения, формируемые кодером канала, в сигналы, согласованные с физической природой канала связи или средой накопителя информации.
Среда распространения информации (канал связи) вносит в процесс передачи информации случайный шум, который искажает сообщение и тем самым затрудняет его прочтение.
Блоки, расположенные на приёмной стороне, выполняют обратные операции и предоставляют получателю информацию в удобном для восприятия виде.
Трудность передачи сообщения не зависит от его содержания, так передавать бессмысленные сообщения не менее трудно, чем осмысленные.
Информацию можно выразить и как свойство объектов, и как результат их взаимодействия. Факт объективного существования информации независимо от нашего сознания для некоторых исследователей послужил поводом построения весьма неординарной точки зрения, что информация является третьей (наряду с материей и энергией) субстанцией материального мира. Но для информации пока не сформулировано фундаментальных законов сохранения и перехода в эквивалентное количество материи и/или энергии. На данный момент принято считать, что информация существует независимо от того, воспринимается она или нет, но проявляется только при взаимодействии.

История
Рождение теории информации зачастую связывают с размещением в июле-октябре 1948 года Клодом Шенноном работы в журнале американской телефонной компании «Bell System» под названием «Математическая теория связи». Но стоит упомянуть, что вклад в формулировку и построение теории информации также был внесён и многими другими выдающимися учёными. Сам Шеннон в начале своей статьи написал «Некоторые основные положения этой теории имеются в важных работах Найквиста и Хартли. В настоящее время теория расширена тем, что включено некоторое число новых факторов, в частности, влияние шума в канале».
В основном Шеннон развивал направление работ Хартли, используя понятие «информации», но сам термин не разъясняет, лишь оговаривает, что сообщения могут иметь какое-то «значение», то есть относиться к системе, имеющей свою физическую или умозрительную сущность (кибернетическая система). Теория Шеннона изначально рассматривалась как точно сформулированная математическая задача и дала возможность определить пропускную способность коммуникационного канала с шумом.

Кодирование данных
Кодирование являет собой процесс перехода сообщения на входе канала связи до кода сообщения на выходе, при этом информационная ценность сообщения должна оставаться неизменной.
В теории информации можно выделить следующие разделы:
1. Кодирование дискретных источников (модель кодирования данных «без потерь»).
2. Кодирование данных, обеспечивающих их передачу по каналу с шумом.
Код является однозначно декодируемым, если любая последовательность символов из алфавита кода (а, в основном, это 0 и 1) разбивается на отдельные слова.
Если ни одно кодовое слово не является началом другого, код называется префиксным и он является однозначно декодируемым. Следовательно, префиксность — достаточное, но не необходимое условие однозначной декодируемости. Требование префиксности ограничивает множество длин кодовых слов и не даёт возможности выбирать кодовые слова слишком короткими. Необходимым и достаточным условием существования префиксного кода объёма 
  
    
      
        M
      
    
    {\displaystyle M}
   с длинами кодовых слов 
  
    
      
        
          l
          
            1
          
        
        ,
        .
        .
        .
        ,
        
          l
          
            M
          
        
      
    
    {\displaystyle l_{1},...,l_{M}}
   является выполнение неравенства Крафта:

  
    
      
        
          ∑
          
            i
            =
            1
          
          
            M
          
        
        
          
            2
          
          
            −
            
              l
              
                i
              
            
          
        
        ⩽
        
          1
        
      
    
    {\displaystyle \sum _{i=1}^{M}{2}^{-l_{i}}\leqslant {1}}
  Также требуется рассмотреть код Шеннона-Фано — алгоритм префиксного неоднородного кодирования. Этот метод кодирования использует избыточность сообщения, заключённую в неоднородном распределении частот символов его алфавита, то есть заменяет коды более частых символов короткими двоичными последовательностями, а коды более редких символов — более длинными двоичными последовательностями. Рассмотрим источник, выбирающий буквы из множества 
  
    
      
        X
        =
        M
      
    
    {\displaystyle X=M}
   с вероятностями 
  
    
      
        
          p
          
            M
          
        
      
    
    {\displaystyle p_{M}}
  . Считаем, что буквы упорядочены по убыванию вероятностей (
  
    
      
        
          
            p
            
              1
            
          
        
        ⩾
        
          
            p
            
              2
            
          
        
        ⩾
        
          
            p
            
              M
            
          
        
      
    
    {\displaystyle {p_{1}}\geqslant {p_{2}}\geqslant {p_{M}}}
  ). Кодовым словом кода Шеннона для сообщения с номером 
  
    
      
        M
      
    
    {\displaystyle M}
   является двоичная последовательность, представляющая собой первые 
  
    
      
        l
        =
        −
        log
        ⁡
        
          
            p
            
              m
            
          
        
      
    
    {\displaystyle l=-\log {p_{m}}}
   разрядов после запятой в двоичной записи числа 
  
    
      
        
          q
          
            M
          
        
      
    
    {\displaystyle q_{M}}
  :

  
    
      
        
          
            q
            
              M
            
          
        
        =
        
          ∑
          
            i
            =
            1
          
          
            M
            −
            1
          
        
        
          p
          
            i
          
        
      
    
    {\displaystyle {q_{M}}=\sum _{i=1}^{M-1}p_{i}}
  3. Кодирование данных для систем со многими пользователями описывает оптимальное взаимодействие абонентов, использующих общий ресурс, например, канал связи.

См. также
Теория кодирования
Философия информации
Информационная энтропия
Сжатие данных
Алгоритмическая теория информации

Примечания
Литература
Б. Д. Кудряшов Теория информации, СПбГУ НИУ ИТМО
Фурсов В. А. Лекции по теории информации ISBN 5-7883-0458-X
Claude E. Shannon, Warren Weaver. The Mathematical Theory of Communication. Univ of Illinois Press, 1963. ISBN 0-252-72548-4
Thomas M. Cover, Joy A. Thomas. Elements of information theory New York: Wiley, 1991. ISBN 0-471-06259-6
R. Landauer, Information is Physical Proc. Workshop on Physics and Computation PhysComp’92 (IEEE Comp. Sci.Press, Los Alamitos, 1993) pp. 1–4.
Maxwell’s Demon: Entropy, Information, Computing, H. S. Leff and A. F. Rex, Editors, Princeton University Press, Princeton, NJ (1990). ISBN 0-691-08727-X
Шеннон К. Работы по теории информации и кибернетике. — М.: Изд. иностр. лит., 1963. — 830 с.
Колмогоров А. Н. Три подхода к определению понятия «количество информации», Пробл. передачи информ., 1:1 (1965), 3-11
MacKay, David. Information Theory, Inference, and Learning Algorithms. — Cambridge University Press, 2003. — ISBN 9780521642989.

Ссылки
Теория информации // Энциклопедия «Кругосвет».
Норберт Винер «Кибернетика или Управление и связь в животном и машине»
К. Шеннон, «Бандвагон»
Важные публикации в теории информации (англ.)
Традиционные подходы к количественному определению информации
Синергетическая теория информации
Холево А. С. Введение в квантовую теорию информации
compression.ru
Электронный учебник по теории информации
Электронный учебник по теории информации