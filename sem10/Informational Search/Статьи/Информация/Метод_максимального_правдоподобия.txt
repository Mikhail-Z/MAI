Ме́тод максима́льного правдоподо́бия или метод наибольшего правдоподобия (ММП, ML, MLE — англ. maximum likelihood estimation) в математической статистике — это метод оценивания неизвестного параметра путём максимизации функции правдоподобия. Основан на предположении о том, что вся информация о статистической выборке содержится в функции правдоподобия.
Метод максимального правдоподобия был проанализирован, рекомендован и значительно популяризирован Р. Фишером между 1912 и 1922 годами (хотя ранее он был использован Гауссом, Лапласом и другими).
Оценка максимального правдоподобия является популярным статистическим методом, который используется для создания статистической модели на основе данных и обеспечения оценки параметров модели.
Метод максимального правдоподобия соответствует многим известным методам оценки в области статистики. Например, вы интересуетесь таким антропометрическим параметром, как рост жителей России. Предположим, у вас имеются данные о росте некоторого количества людей, а не всего населения. Кроме того, предполагается, что рост является нормально распределённой величиной с неизвестной дисперсией и средним значением. Среднее значение и дисперсия роста в выборке являются максимально правдоподобными к среднему значению и дисперсии всего населения.
Для фиксированного набора данных и базовой вероятностной модели, используя метод максимального правдоподобия, мы получим значения параметров модели, которые делают данные «более близкими» к реальным. Оценка максимального правдоподобия даёт уникальный и простой способ определить решения в случае нормального распределения.
Метод оценки максимального правдоподобия применяется для широкого круга статистических моделей, в том числе:

линейные модели и обобщённые линейные модели;
факторный анализ;
моделирование структурных уравнений;
многие ситуации, в рамках проверки гипотезы и доверительного интервала формирования;
дискретные модели выбора.

Сущность метода
Пусть есть выборка 
  
    
      
        
          X
          
            1
          
        
        ,
        …
        ,
        
          X
          
            n
          
        
      
    
    {\displaystyle X_{1},\ldots ,X_{n}}
   из распределения 
  
    
      
        
          
            P
          
          
            θ
          
        
      
    
    {\displaystyle \mathbb {P} _{\theta }}
  , где 
  
    
      
        θ
        ∈
        Θ
      
    
    {\displaystyle \theta \in \Theta }
   — неизвестные параметры. Пусть 
  
    
      
        L
        (
        
          x
        
        ∣
        θ
        )
        :
        Θ
        →
        
          R
        
      
    
    {\displaystyle L(\mathbf {x} \mid \theta )\colon \Theta \to \mathbb {R} }
   — функция правдоподобия, где 
  
    
      
        
          x
        
        ∈
        
          
            R
          
          
            n
          
        
      
    
    {\displaystyle \mathbf {x} \in \mathbb {R} ^{n}}
  . Точечная оценка

  
    
      
        
          
            
              
                θ
                ^
              
            
          
          
            
              M
              Π
            
          
        
        =
        
          
            
              
                θ
                ^
              
            
          
          
            
              M
              Π
            
          
        
        (
        
          X
          
            1
          
        
        ,
        …
        ,
        
          X
          
            n
          
        
        )
        =
        
          
            
              a
              r
              g
              m
              a
              x
            
          
          
            θ
            ∈
            Θ
          
        
        ⁡
        L
        (
        
          X
          
            1
          
        
        ,
        …
        ,
        
          X
          
            n
          
        
        ∣
        θ
        )
      
    
    {\displaystyle {\hat {\theta }}_{\mathrm {M\Pi } }={\hat {\theta }}_{\mathrm {M\Pi } }(X_{1},\ldots ,X_{n})=\mathop {\rm {argmax}} \limits _{\theta \in \Theta }L(X_{1},\ldots ,X_{n}\mid \theta )}
  называется оце́нкой максима́льного правдоподо́бия параметра 
  
    
      
        θ
      
    
    {\displaystyle \theta }
  . Таким образом оценка максимального правдоподобия — это такая оценка, которая максимизирует функцию правдоподобия при фиксированной реализации выборки.
Часто вместо функции правдоподобия 
  
    
      
        L
      
    
    {\displaystyle L}
   используют логарифмическую функцию правдоподобия 
  
    
      
        l
        =
        ln
        ⁡
        L
      
    
    {\displaystyle l=\ln L}
  . Так как функция 
  
    
      
        x
        →
        ln
        ⁡
        x
        ,
        
        x
        >
        0
      
    
    {\displaystyle x\to \ln x,\;x>0}
   монотонно возрастает на всей области определения, максимум любой функции 
  
    
      
        L
        (
        θ
        )
      
    
    {\displaystyle L(\theta )}
   является максимумом функции 
  
    
      
        ln
        ⁡
        L
        (
        θ
        )
      
    
    {\displaystyle \ln L(\theta )}
  , и наоборот. Таким образом,

  
    
      
        
          
            
              
                θ
                ^
              
            
          
          
            
              M
              Π
            
          
        
        =
        
          
            
              a
              r
              g
              m
              a
              x
            
          
          
            θ
            ∈
            Θ
          
        
        ⁡
        l
        (
        
          X
          
            1
          
        
        ,
        …
        ,
        
          X
          
            n
          
        
        ∣
        θ
        )
      
    
    {\displaystyle {\hat {\theta }}_{\mathrm {M\Pi } }=\mathop {\rm {argmax}} \limits _{\theta \in \Theta }l(X_{1},\ldots ,X_{n}\mid \theta )}
  ,Если функция правдоподобия дифференцируема, то необходимое условие экстремума — равенство нулю её градиента:

  
    
      
        g
        (
        θ
        )
        =
        
          
            
              ∂
              l
              (
              
                x
              
              ,
              
                θ
                
                  0
                
              
              )
            
            
              ∂
              θ
            
          
        
        =
        0
      
    
    {\displaystyle g(\theta )={\frac {\partial l(\mathbf {x} ,\theta _{0})}{\partial \theta }}=0}
  Достаточное условие экстремума может быть сформулировано как отрицательная определённость гессиана — матрицы вторых производных:

  
    
      
        H
        =
        
          
            
              
                ∂
                
                  2
                
              
              l
              (
              
                x
              
              ,
              
                θ
                
                  0
                
              
              )
            
            
              ∂
              θ
              ∂
              
                θ
                
                  T
                
              
            
          
        
      
    
    {\displaystyle H={\frac {\partial ^{2}l(\mathbf {x} ,\theta _{0})}{\partial \theta \partial \theta ^{T}}}}
  Важное значение для оценки свойств оценок метода максимального правдоподобия играет так называемая информационная матрица, равная по определению:

  
    
      
        I
        (
        θ
        )
        =
        E
        [
        g
        (
        θ
        )
        g
        (
        θ
        
          )
          
            T
          
        
        ]
      
    
    {\displaystyle I(\theta )=E[g(\theta )g(\theta )^{T}]}
  В оптимальной точке информационная матрица совпадает с математическим ожиданием гессиана, взятым со знаком минус:

  
    
      
        I
        =
        −
        E
        (
        
          H
          
            0
          
        
        )
      
    
    {\displaystyle I=-E(H_{0})}

Свойства
Оценки максимального правдоподобия, вообще говоря, могут быть смещёнными (см. примеры), но являются состоятельными, асимптотически эффективными и асимптотически нормальными оценками. Асимптотическая нормальность означает, что
  
    
      
        
          
            n
          
        
        (
        
          
            
              θ
              ^
            
          
        
        −
        θ
        )
        
          
            →
            
              d
            
          
        
        N
        (
        0
        ,
        
          
            I
          
          
            ∞
          
          
            −
            1
          
        
        )
      
    
    {\displaystyle {\sqrt {n}}({\hat {\theta }}-\theta ){\xrightarrow {d}}N(0,{\boldsymbol {I}}_{\infty }^{-1})}
  где 
  
    
      
        
          
            I
          
          
            ∞
          
        
        =
        −
        
          lim
          
            n
            →
            ∞
          
        
        
          
            1
            n
          
        
        
          E
        
        (
        
          H
        
        )
      
    
    {\displaystyle {\boldsymbol {I}}_{\infty }=-\lim _{n\rightarrow \infty }{\frac {1}{n}}\mathbb {E} ({\boldsymbol {H}})}
   — асимптотическая информационная матрица.
Асимптотическая эффективность означает, что асимптотическая ковариационная матрица 
  
    
      
        
          
            I
          
          
            ∞
          
          
            −
            1
          
        
      
    
    {\displaystyle {\boldsymbol {I}}_{\infty }^{-1}}
   является нижней границей для всех состоятельных асимптотически нормальных оценок.

Если 
  
    
      
        
          
            
              θ
              ^
            
          
        
      
    
    {\displaystyle {\hat {\theta }}}
   — оценка метода максимального правдоподобия, параметров 
  
    
      
        θ
      
    
    {\displaystyle \theta }
  , то 
  
    
      
        g
        (
        
          
            
              θ
              ^
            
          
        
        )
      
    
    {\displaystyle g({\hat {\theta }})}
   является оценкой максимального правдоподобия для 
  
    
      
        g
        (
        θ
        )
      
    
    {\displaystyle g(\theta )}
  , где g — непрерывная функция (функциональная инвариантность). Таким образом, законы распределения данных можно параметризовать различным образом.
Также необходимым условием МП-оценок является выполнение системы вида:

  
    
      
        
          {
          
            
              
                
                  
                    
                      δ
                      
                        δ
                        
                          θ
                          
                            1
                          
                        
                      
                    
                  
                  ln
                  ⁡
                  
                    
                      L
                      
                        n
                      
                    
                  
                  
                    (
                    
                      
                        
                          
                            x
                            →
                          
                        
                      
                      ,
                      
                        
                          
                            θ
                            →
                          
                        
                      
                    
                    )
                  
                
                
                  =
                
                
                  0
                
              
              
                
                  ⋯
                
                
                  ⋯
                
                
              
              
                
                  
                    
                      δ
                      
                        δ
                        
                          θ
                          
                            k
                          
                        
                      
                    
                  
                  ln
                  ⁡
                  
                    
                      L
                      
                        n
                      
                    
                  
                  
                    (
                    
                      
                        
                          
                            x
                            →
                          
                        
                      
                      ,
                      
                        
                          
                            θ
                            →
                          
                        
                      
                    
                    )
                  
                
                
                  =
                
                
                  0
                
              
            
          
          
        
      
    
    {\displaystyle \left\{{\begin{matrix}{\frac {\delta }{\delta \theta _{1}}}\ln {L_{n}}\left({\vec {x}},{\vec {\theta }}\right)&=&0\\\cdots &\cdots &\\{\frac {\delta }{\delta \theta _{k}}}\ln {L_{n}}\left({\vec {x}},{\vec {\theta }}\right)&=&0\\\end{matrix}}\right.}
  где 
  
    
      
        
          L
          
            n
          
        
        
          (
          
            
              
                
                  x
                  →
                
              
            
            ,
            
              
                
                  θ
                  →
                
              
            
          
          )
        
        =
        
          ∏
          
            i
            =
            1
          
          
            n
          
        
        
          L
          
            1
          
        
        
          (
          
            
              x
              
                i
              
            
            ,
            
              
                
                  θ
                  →
                
              
            
          
          )
        
      
    
    {\displaystyle L_{n}\left({\vec {x}},{\vec {\theta }}\right)=\prod _{i=1}^{n}L_{1}\left(x_{i},{\vec {\theta }}\right)}
   — функция правдободобия выборки 
  
    
      
        
          
            
              x
              →
            
          
        
      
    
    {\displaystyle {\vec {x}}}
   объёма 
  
    
      
        n
      
    
    {\displaystyle n}

Примеры
Пусть 
  
    
      
        
          X
          
            1
          
        
        ,
        …
        ,
        
          X
          
            n
          
        
        ∼
        
          U
        
        [
        0
        ,
        θ
        ]
      
    
    {\displaystyle X_{1},\ldots ,X_{n}\sim \mathrm {U} [0,\theta ]}
   — независимая выборка из непрерывного равномерного распределения на отрезке 
  
    
      
        [
        0
        ,
        θ
        ]
      
    
    {\displaystyle [0,\theta ]}
  , где 
  
    
      
        θ
        >
        0
      
    
    {\displaystyle \theta >0}
   — неизвестный параметр. Тогда функция правдоподобия имеет вид
  
    
      
        f
        (
        
          x
        
        ∣
        θ
        )
        =
        
          
            {
            
              
                
                  
                    
                      1
                      
                        θ
                        
                          n
                        
                      
                    
                  
                  ,
                
                
                  
                    x
                  
                  ∈
                  [
                  0
                  ,
                  θ
                  
                    ]
                    
                      n
                    
                  
                  ⊂
                  
                    
                      R
                    
                    
                      n
                    
                  
                
              
              
                
                  0
                  ,
                
                
                  
                    x
                  
                  ∉
                  [
                  0
                  ,
                  θ
                  
                    ]
                    
                      n
                    
                  
                
              
            
            
          
        
        .
      
    
    {\displaystyle f(\mathbf {x} \mid \theta )={\begin{cases}{\frac {1}{\theta ^{n}}},&\mathbf {x} \in [0,\theta ]^{n}\subset \mathbb {R} ^{n}\\0,&\mathbf {x} \not \in [0,\theta ]^{n}\end{cases}}.}
  Последнее равенство может быть переписано в виде:

  
    
      
        f
        (
        
          x
        
        ∣
        θ
        )
        =
        
          
            {
            
              
                
                  
                    
                      1
                      
                        θ
                        
                          n
                        
                      
                    
                  
                  ,
                
                
                  θ
                  ≥
                  max
                  (
                  
                    x
                    
                      1
                    
                  
                  ,
                  …
                  ,
                  
                    x
                    
                      n
                    
                  
                  )
                
              
              
                
                  0
                  ,
                
                
                  θ
                  <
                  max
                  (
                  
                    x
                    
                      1
                    
                  
                  ,
                  …
                  ,
                  
                    x
                    
                      n
                    
                  
                  )
                
              
            
            
          
        
        ,
      
    
    {\displaystyle f(\mathbf {x} \mid \theta )={\begin{cases}{\frac {1}{\theta ^{n}}},&\theta \geq \max(x_{1},\ldots ,x_{n})\\0,&\theta <\max(x_{1},\ldots ,x_{n})\end{cases}},}
  где 
  
    
      
        
          x
        
        =
        (
        
          x
          
            1
          
        
        ,
        …
        ,
        
          x
          
            n
          
        
        
          )
          
            ⊤
          
        
      
    
    {\displaystyle \mathbf {x} =(x_{1},\ldots ,x_{n})^{\top }}
  , откуда видно, что своего максимума функция правдоподобия достигает в точке 
  
    
      
        θ
        =
        max
        (
        
          x
          
            1
          
        
        ,
        …
        ,
        
          x
          
            n
          
        
        )
      
    
    {\displaystyle \theta =\max(x_{1},\ldots ,x_{n})}
  . Таким образом

  
    
      
        
          
            
              
                θ
                ^
              
            
          
          
            
              M
              Π
            
          
        
        =
        max
        (
        
          X
          
            1
          
        
        ,
        …
        ,
        
          X
          
            n
          
        
        )
      
    
    {\displaystyle {\hat {\theta }}_{\mathrm {M\Pi } }=\max(X_{1},\ldots ,X_{n})}
  .Такая оценка будет смещенной: 
  
    
      
        P
        {
        max
        (
        
          X
          
            1
          
        
        ,
        …
        ,
        
          X
          
            n
          
        
        )
        ≤
        x
        }
        =
        
          
            (
            
              
                x
                θ
              
            
            )
          
          
            n
          
        
      
    
    {\displaystyle P\{\max(X_{1},\ldots ,X_{n})\leq x\}=\left({\frac {x}{\theta }}\right)^{n}}
  , откуда

  
    
      
        E
        
          
            
              
                θ
                ^
              
            
          
          
            
              M
              Π
            
          
        
        =
        
          ∫
          
            0
          
          
            θ
          
        
        x
        d
        
          
            (
            
              
                x
                θ
              
            
            )
          
          
            n
          
        
        =
        
          
            n
            
              n
              +
              1
            
          
        
        θ
      
    
    {\displaystyle E{\hat {\theta }}_{\mathrm {M\Pi } }=\int _{0}^{\theta }xd\left({\frac {x}{\theta }}\right)^{n}={\frac {n}{n+1}}\theta }
  

Пусть 
  
    
      
        
          X
          
            1
          
        
        ,
        …
        ,
        
          X
          
            n
          
        
        ∼
        
          N
        
        (
        μ
        ,
        
          σ
          
            2
          
        
        )
      
    
    {\displaystyle X_{1},\ldots ,X_{n}\sim \mathrm {N} (\mu ,\sigma ^{2})}
   — независимая выборка из нормального распределения с неизвестными средним и дисперсией. Построим оценку максимального правдоподобия 
  
    
      
        
          
            (
            
              
                
                  
                    
                      μ
                      ^
                    
                  
                
                
                  
                    M
                    Π
                  
                
              
              ,
              
                
                  
                    
                      
                        σ
                        
                          2
                        
                      
                      ^
                    
                  
                
                
                  
                    M
                    Π
                  
                
              
            
            )
          
          
            
              T
            
          
        
      
    
    {\displaystyle \left({\widehat {\mu }}_{\mathrm {M\Pi } },{\widehat {\sigma ^{2}}}_{\mathrm {M\Pi } }\right)^{\rm {T}}}
   для неизвестного вектора параметров 
  
    
      
        
          
            (
            
              μ
              ,
              
                σ
                
                  2
                
              
            
            )
          
          
            
              T
            
          
        
      
    
    {\displaystyle \left(\mu ,\sigma ^{2}\right)^{\rm {T}}}
  . Логарифмическая функция правдоподобия принимает вид
  
    
      
        L
        (
        
          x
        
        ∣
        μ
        ,
        
          σ
          
            2
          
        
        )
        =
        −
        
          
            n
            2
          
        
        ln
        ⁡
        (
        2
        π
        
          σ
          
            2
          
        
        )
        −
        
          
            1
            
              2
              
                σ
                
                  2
                
              
            
          
        
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        (
        
          X
          
            i
          
        
        −
        μ
        
          )
          
            2
          
        
      
    
    {\displaystyle L(\mathbf {x} \mid \mu ,\sigma ^{2})=-{\frac {n}{2}}\ln(2\pi \sigma ^{2})-{\frac {1}{2\sigma ^{2}}}\sum \limits _{i=1}^{n}(X_{i}-\mu )^{2}}
  .Чтобы найти её максимум, приравняем к нулю частные производные:

  
    
      
        
          {
          
            
              
                
                  
                    
                      
                        ∂
                        
                          ∂
                          μ
                        
                      
                    
                    L
                    (
                    
                      x
                    
                    ∣
                    μ
                    ,
                    
                      σ
                      
                        2
                      
                    
                    )
                    =
                    0
                  
                
              
              
                
                  
                    
                      
                        ∂
                        
                          ∂
                          
                            σ
                            
                              2
                            
                          
                        
                      
                    
                    L
                    (
                    
                      x
                    
                    ∣
                    μ
                    ,
                    
                      σ
                      
                        2
                      
                    
                    )
                    =
                    0
                  
                
              
            
          
          
        
        ⇒
        
          {
          
            
              
                
                  
                    
                      
                        
                          
                            ∑
                            
                              i
                              =
                              1
                            
                            
                              n
                            
                          
                          
                            X
                            
                              i
                            
                          
                          −
                          n
                          μ
                        
                        
                          σ
                          
                            2
                          
                        
                      
                    
                    =
                    0
                  
                
              
              
                
                  
                    −
                    
                      
                        n
                        
                          2
                          
                            σ
                            
                              2
                            
                          
                        
                      
                    
                    +
                    
                      
                        
                          
                            ∑
                            
                              i
                              =
                              1
                            
                            
                              n
                            
                          
                          (
                          
                            X
                            
                              i
                            
                          
                          −
                          μ
                          
                            )
                            
                              2
                            
                          
                        
                        
                          2
                          
                            
                              (
                              
                                σ
                                
                                  2
                                
                              
                              )
                            
                            
                              2
                            
                          
                        
                      
                    
                    =
                    0
                  
                
              
            
          
          
        
        ,
      
    
    {\displaystyle \left\{{\begin{matrix}\displaystyle {\frac {\partial }{\partial \mu }}L(\mathbf {x} \mid \mu ,\sigma ^{2})=0\\[10pt]\displaystyle {\frac {\partial }{\partial \sigma ^{2}}}L(\mathbf {x} \mid \mu ,\sigma ^{2})=0\\\end{matrix}}\right.\Rightarrow \left\{{\begin{matrix}\displaystyle {\frac {\sum \limits _{i=1}^{n}X_{i}-n\mu }{\sigma ^{2}}}=0\\[10pt]\displaystyle -{\frac {n}{2\sigma ^{2}}}+{\frac {\sum \limits _{i=1}^{n}(X_{i}-\mu )^{2}}{2\left(\sigma ^{2}\right)^{2}}}=0\\\end{matrix}}\right.,}
  откуда

  
    
      
        
          
            
              
                μ
                ^
              
            
          
          
            
              M
              Π
            
          
        
        =
        
          
            
              X
              ¯
            
          
        
      
    
    {\displaystyle {\hat {\mu }}_{\mathrm {M\Pi } }={\bar {X}}}
   — выборочное среднее, а

  
    
      
        
          
            
              
                
                  σ
                  
                    2
                  
                
                ^
              
            
          
          
            
              M
              Π
            
          
        
        =
        
          S
          
            n
          
          
            2
          
        
      
    
    {\displaystyle {\widehat {\sigma ^{2}}}_{\mathrm {M\Pi } }=S_{n}^{2}}
   — выборочная дисперсия.

Применение метода
Обработка эксперимента
Предположим, что мы измеряем некоторую величину 
  
    
      
        a
      
    
    {\textstyle a}
  . Сделав одно измерение, получили её значение 
  
    
      
        
          x
          
            1
          
        
      
    
    {\textstyle x_{1}}
   с ошибкой 
  
    
      
        
          σ
          
            1
          
        
      
    
    {\textstyle \sigma _{1}}
  : 
  
    
      
        
          x
          
            1
          
        
        ±
        
          σ
          
            1
          
        
      
    
    {\textstyle x_{1}\pm \sigma _{1}}
  . Запишем плотность вероятности
того, что величина 
  
    
      
        a
      
    
    {\textstyle a}
   примет значение 
  
    
      
        
          x
          
            1
          
        
      
    
    {\textstyle x_{1}}
  :

  
    
      
        W
        (
        a
        )
        =
        
          
            1
            
              2
              π
              
                σ
                
                  1
                
                
                  2
                
              
            
          
        
        exp
        ⁡
        
          [
          
            −
            
              
                
                  (
                  
                    x
                    
                      1
                    
                  
                  −
                  a
                  
                    )
                    
                      2
                    
                  
                
                
                  2
                  
                    σ
                    
                      1
                    
                    
                      2
                    
                  
                
              
            
          
          ]
        
      
    
    {\displaystyle W(a)={\frac {1}{\sqrt {2\pi \sigma _{1}^{2}}}}\exp \left[-{\frac {(x_{1}-a)^{2}}{2\sigma _{1}^{2}}}\right]}
  .
Теперь предположим, что мы провели несколько таких измерений и получили 
  
    
      
        
          x
          
            1
          
        
        ±
        
          σ
          
            1
          
        
        ,
        
          x
          
            2
          
        
        ±
        
          σ
          
            2
          
        
        …
        
          x
          
            n
          
        
        ±
        
          σ
          
            n
          
        
      
    
    {\textstyle x_{1}\pm \sigma _{1},x_{2}\pm \sigma _{2}\ldots x_{n}\pm \sigma _{n}}
  . Плотность вероятности того, что величина 
  
    
      
        a
      
    
    {\textstyle a}
   примет значения 
  
    
      
        
          x
          
            1
          
        
        ,
        
          x
          
            2
          
        
        …
        
          x
          
            n
          
        
      
    
    {\textstyle x_{1},x_{2}\ldots x_{n}}
  , будет:

  
    
      
        W
        (
        a
        )
        =
        
          ∏
          
            i
            =
            1
          
          
            n
          
        
        
          
            
              1
              
                2
                π
                
                  σ
                  
                    i
                  
                  
                    2
                  
                
              
            
          
          exp
          ⁡
          
            [
            
              −
              
                
                  
                    (
                    
                      x
                      
                        i
                      
                    
                    −
                    a
                    
                      )
                      
                        2
                      
                    
                  
                  
                    2
                    
                      σ
                      
                        i
                      
                      
                        2
                      
                    
                  
                
              
            
            ]
          
        
      
    
    {\displaystyle W(a)=\prod _{i=1}^{n}{{\frac {1}{\sqrt {2\pi \sigma _{i}^{2}}}}\exp \left[-{\frac {(x_{i}-a)^{2}}{2\sigma _{i}^{2}}}\right]}}
  .
Эта функция называется функцией правдоподобия. Наиболее вероятное значение измеряемой величины 
  
    
      
        
          a
          
            ∗
          
        
      
    
    {\textstyle a^{*}}
   определяется по максимуму функции правдоподобия. Более удобной является логарифмическая функция правдоподобия:

  
    
      
        L
        (
        a
        )
        =
        ln
        ⁡
        W
        (
        a
        )
        =
        −
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          
            
              (
              
                x
                
                  i
                
              
              −
              a
              
                )
                
                  2
                
              
            
            
              2
              
                σ
                
                  i
                
                
                  2
                
              
            
          
        
        +
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          ln
          ⁡
          
            
              1
              
                2
                π
                
                  σ
                  
                    i
                  
                  
                    2
                  
                
              
            
          
        
      
    
    {\displaystyle L(a)=\ln W(a)=-\sum _{i=1}^{n}{\frac {(x_{i}-a)^{2}}{2\sigma _{i}^{2}}}+\sum _{i=1}^{n}{\ln {\frac {1}{\sqrt {2\pi \sigma _{i}^{2}}}}}}
  .
Продифференцируем логарифмическую функцию правдоподобия по 
  
    
      
        a
      
    
    {\textstyle a}
  :

  
    
      
        
          
            
              ∂
              
                L
              
            
            
              ∂
              
                a
              
            
          
        
        =
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          
            
              
                x
                
                  i
                
              
              −
              a
            
            
              σ
              
                i
              
              
                2
              
            
          
        
      
    
    {\displaystyle {\frac {\partial {L}}{\partial {a}}}=\sum _{i=1}^{n}{\frac {x_{i}-a}{\sigma _{i}^{2}}}}
  .
Приравняем 
  
    
      
        
          
            
              ∂
              
                L
              
            
            
              ∂
              
                a
              
            
          
        
      
    
    {\displaystyle {\frac {\partial {L}}{\partial {a}}}}
   к 
  
    
      
        0
      
    
    {\textstyle 0}
   и получим некоторое значение 
  
    
      
        a
        =
        
          a
          
            ∗
          
        
      
    
    {\textstyle a=a^{*}}
  :

  
    
      
        
          a
          
            ∗
          
        
        =
        
          
            
              
                ∑
                
                  i
                  =
                  1
                
                
                  n
                
              
              
                
                  
                    x
                    
                      i
                    
                  
                  
                    σ
                    
                      i
                    
                    
                      2
                    
                  
                
              
            
            
              
                ∑
                
                  i
                  =
                  1
                
                
                  n
                
              
              
                
                  1
                  
                    σ
                    
                      i
                    
                    
                      2
                    
                  
                
              
            
          
        
      
    
    {\displaystyle a^{*}={\frac {\sum \limits _{i=1}^{n}{\frac {x_{i}}{\sigma _{i}^{2}}}}{\sum \limits _{i=1}^{n}{\frac {1}{\sigma _{i}^{2}}}}}}
  .
Крамер сформулировал следующую теорему:
Теорема: Не существует другого метода обработки результатов эксперимента, который дал бы лучшее приближение к истине, чем метод максимального правдоподобия.

Ошибки измерений
Предположим, что мы провели серию измерений и получили серию значений 
  
    
      
        
          a
          
            ∗
          
        
      
    
    {\textstyle a^{*}}
  , естественно записать, что это распределение будет иметь гауссовский вид:

  
    
      
        W
        (
        a
        )
        =
        
          
            1
            
              2
              π
              
                σ
                
                  
                    a
                    
                      ∗
                    
                  
                
                
                  2
                
              
            
          
        
        exp
        ⁡
        
          [
          
            −
            
              
                
                  (
                  
                    a
                    
                      ∗
                    
                  
                  −
                  a
                  
                    )
                    
                      2
                    
                  
                
                
                  2
                  
                    σ
                    
                      
                        a
                        
                          ∗
                        
                      
                    
                    
                      2
                    
                  
                
              
            
          
          ]
        
      
    
    {\displaystyle W(a)={\frac {1}{\sqrt {2\pi \sigma _{a^{*}}^{2}}}}\exp \left[-{\frac {(a^{*}-a)^{2}}{2\sigma _{a^{*}}^{2}}}\right]}
  .
Запишем логарифмическую функцию правдоподобия:
  
    
      
        L
        (
        a
        )
        =
        ln
        ⁡
        W
        (
        a
        )
        =
        −
        
          
            
              (
              
                a
                
                  ∗
                
              
              −
              a
              
                )
                
                  2
                
              
            
            
              2
              
                σ
                
                  
                    a
                    
                      ∗
                    
                  
                
                
                  2
                
              
            
          
        
        +
        
          ln
          ⁡
          
            
              1
              
                2
                π
                
                  σ
                  
                    
                      a
                      
                        ∗
                      
                    
                  
                  
                    2
                  
                
              
            
          
        
      
    
    {\displaystyle L(a)=\ln W(a)=-{\frac {(a^{*}-a)^{2}}{2\sigma _{a^{*}}^{2}}}+{\ln {\frac {1}{\sqrt {2\pi \sigma _{a^{*}}^{2}}}}}}
  .
Возьмем первую производную:

  
    
      
        
          
            
              ∂
              
                L
              
            
            
              ∂
              
                a
              
            
          
        
        =
        
          
            
              
                a
                
                  ∗
                
              
              −
              a
            
            
              σ
              
                
                  a
                  
                    ∗
                  
                
              
              
                2
              
            
          
        
      
    
    {\displaystyle {\frac {\partial {L}}{\partial {a}}}={\frac {a^{*}-a}{\sigma _{a^{*}}^{2}}}}
  .
Если 
  
    
      
        
          
            
              ∂
              
                L
              
            
            
              ∂
              
                a
              
            
          
        
        =
        0
      
    
    {\displaystyle {\frac {\partial {L}}{\partial {a}}}=0}
   , то 
  
    
      
        a
        =
        
          a
          
            ∗
          
        
      
    
    {\displaystyle a=a^{*}}
  . Теперь возьмем вторую производную:

  
    
      
        
          
            
              
                ∂
                
                  2
                
              
              
                L
              
            
            
              ∂
              
                
                  a
                
                
                  2
                
              
            
          
        
        =
        −
        
          
            1
            
              σ
              
                
                  a
                  
                    ∗
                  
                
              
              
                2
              
            
          
        
      
    
    {\displaystyle {\frac {\partial ^{2}{L}}{\partial {a}^{2}}}=-{\frac {1}{\sigma _{a^{*}}^{2}}}}
  , откуда

  
    
      
        
          σ
          
            
              a
              
                ∗
              
            
          
        
        =
        
          
            (
            
              −
              
                
                  
                    
                      ∂
                      
                        2
                      
                    
                    
                      L
                    
                  
                  
                    ∂
                    
                      
                        a
                      
                      
                        2
                      
                    
                  
                
              
              
                
                  
                    |
                  
                
                
                  a
                  =
                  
                    a
                    
                      ∗
                    
                  
                
              
            
            )
          
          
            −
            1
            
              /
            
            2
          
        
      
    
    {\displaystyle \sigma _{a^{*}}=\left(-{\frac {\partial ^{2}{L}}{\partial {a}^{2}}}{\Big |}_{a=a^{*}}\right)^{-1/2}}
  .
Это называется первой магической формулой.

Условный метод максимального правдоподобия
Условный метод максимального правдоподобия (Conditional ML) используется в регрессионных моделях. Суть метода заключается в том, что используется не полное совместное распределение всех переменных (зависимой и регрессоров), а только условное распределение зависимой переменной по факторам, то есть фактически распределение случайных ошибок регрессионной модели. Полная функция правдоподобия есть произведение «условной функции правдоподобия» и плотности распределения факторов. Условный ММП эквивалентен полному варианту ММП в том случае, когда распределение факторов никак не зависит от оцениваемых параметров. Это условие часто нарушается в моделях временных рядов, например в авторегрессионной модели. В данном случае, регрессорами являются прошлые значения зависимой переменной, а значит их значения также подчиняются той же AR-модели, то есть распределение регрессоров зависит от оцениваемых параметров. В таких случаях результаты применения условного и полного метода максимального правдоподобия будут различаться.

См. также
Правдоподобие принятой последовательности
Метод моментов
Обобщенный метод моментов
Метод наименьших квадратов
Метод инструментальных переменных
EM-алгоритм

Примечания
Литература
Магнус Я.Р., Катышев П.К., Пересецкий А.А. Эконометрика. Начальный курс. — М.: Дело, 2007. — 504 с. — ISBN 978-5-7749-0473-0.
Остапенко Р. И. Основы структурного моделирования в психологии и педагогике: учебно-методическое пособие для студентов психолого-педагогического факультета. — Воронеж.: ВГПУ, 2012. — 116 с. — ISBN 978-5-88519-886-8.
Никулин М. С. Отношения правдоподобия критерий // Математическая энциклопедия / Виноградов И. М. (гл. ред.). — М.: Советская энциклопедия, 1984. — Т. 4. — С. 151. — 1216 с.