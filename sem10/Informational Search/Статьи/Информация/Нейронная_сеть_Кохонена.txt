Нейронные сети Кохонена — класс нейронных сетей, основным элементом которых является слой Кохонена. Слой Кохонена состоит из адаптивных линейных сумматоров («линейных формальных нейронов»). Как правило, выходные сигналы слоя Кохонена обрабатываются по правилу «Победитель получает всё»: наибольший сигнал превращается в единичный, остальные обращаются в ноль.
По способам настройки входных весов сумматоров и по решаемым задачам различают много разновидностей сетей Кохонена. Наиболее известные из них:

сети векторного квантования сигналов, тесно связанные с простейшим базовым алгоритмом кластерного анализа (метод динамических ядер или K-средних);
самоорганизующиеся карты Кохонена (англ. self-organising maps, SOM);
сети векторного квантования, обучаемые с учителем (англ. learning vector quantization).

Слой Кохонена
Базовая версия
Слой Кохонена состоит из некоторого количества 
  
    
      
        n
      
    
    {\displaystyle n}
   параллельно действующих линейных элементов. Все они имеют одинаковое число входов 
  
    
      
        m
      
    
    {\displaystyle m}
   и получают на свои входы один и тот же вектор входных сигналов 
  
    
      
        x
        =
        (
        
          x
          
            1
          
        
        ,
        .
        .
        .
        
          x
          
            m
          
        
        )
      
    
    {\displaystyle x=(x_{1},...x_{m})}
  . На выходе 
  
    
      
        j
      
    
    {\displaystyle j}
  го линейного элемента получаем сигнал

  
    
      
        
          y
          
            j
          
        
        =
        
          w
          
            j
            0
          
        
        +
        
          ∑
          
            i
            =
            1
          
          
            m
          
        
        
          w
          
            j
            i
          
        
        
          x
          
            i
          
        
        ,
      
    
    {\displaystyle y_{j}=w_{j0}+\sum _{i=1}^{m}w_{ji}x_{i},}
  где:

  
    
      
        
          w
          
            j
            i
          
        
      
    
    {\displaystyle w_{ji}}
   — весовой коэффициент 
  
    
      
        i
      
    
    {\displaystyle i}
  -го входа 
  
    
      
        j
      
    
    {\displaystyle j}
  -го нейрона;

  
    
      
        i
      
    
    {\displaystyle i}
   — номер входа;

  
    
      
        j
      
    
    {\displaystyle j}
   — номер нейрона;

  
    
      
        
          w
          
            j
            0
          
        
      
    
    {\displaystyle w_{j0}}
   — пороговый коэффициент.После прохождения слоя линейных элементов сигналы посылаются на обработку по правилу «победитель забирает всё»: среди выходных сигналов выполняется поиск максимального 
  
    
      
        
          y
          
            j
          
        
      
    
    {\displaystyle y_{j}}
  ; его номер 
  
    
      
        
          j
          
            max
          
        
        =
        
          
            a
            r
            g
          
        
        
          max
          
            j
          
        
        {
        
          y
          
            j
          
        
        }
      
    
    {\displaystyle j_{\max }={\rm {arg}}\max _{j}\{y_{j}\}}
  . Окончательно, на выходе сигнал с номером 
  
    
      
        
          j
          
            max
          
        
      
    
    {\displaystyle j_{\max }}
   равен единице, остальные — нулю. Если максимум одновременно достигается для нескольких 
  
    
      
        
          j
          
            max
          
        
      
    
    {\displaystyle j_{\max }}
  , то:

либо принимают все соответствующие сигналы равными единице;
либо равным единице принимают только первый сигнал в списке (по соглашению).«Нейроны Кохонена можно воспринимать как набор электрических лампочек, так что для любого входного вектора загорается одна из них».

Геометрическая интерпретация
Большое распространение получили слои Кохонена, построенные следующим образом: каждому (
  
    
      
        j
      
    
    {\displaystyle j}
  -му) нейрону сопоставляется точка 
  
    
      
        
          W
          
            j
          
        
        =
        (
        
          w
          
            j
            1
          
        
        ,
        .
        .
        .
        
          w
          
            j
            m
          
        
        )
      
    
    {\displaystyle W_{j}=(w_{j1},...w_{jm})}
   в 
  
    
      
        m
      
    
    {\displaystyle m}
  -мерном пространстве (пространстве сигналов). Для входного вектора 
  
    
      
        x
        =
        (
        
          x
          
            1
          
        
        ,
        .
        .
        .
        
          x
          
            m
          
        
        )
      
    
    {\displaystyle x=(x_{1},...x_{m})}
   вычисляются его евклидовы расстояния 
  
    
      
        
          ρ
          
            j
          
        
        (
        x
        )
      
    
    {\displaystyle \rho _{j}(x)}
   до точек 
  
    
      
        
          W
          
            j
          
        
      
    
    {\displaystyle W_{j}}
   и «ближайший получает всё» — тот нейрон, для которого это расстояние минимально, выдаёт единицу, остальные — нули. Следует заметить, что для сравнения расстояний достаточно вычислять линейную функцию сигнала:

  
    
      
        
          ρ
          
            j
          
        
        (
        x
        
          )
          
            2
          
        
        =
        ‖
        x
        −
        
          W
          
            j
          
        
        
          ‖
          
            2
          
        
        =
        ‖
        
          W
          
            j
          
        
        
          ‖
          
            2
          
        
        −
        2
        
          ∑
          
            i
            =
            1
          
          
            m
          
        
        
          w
          
            j
            i
          
        
        
          x
          
            i
          
        
        +
        ‖
        x
        
          ‖
          
            2
          
        
      
    
    {\displaystyle \rho _{j}(x)^{2}=\|x-W_{j}\|^{2}=\|W_{j}\|^{2}-2\sum _{i=1}^{m}w_{ji}x_{i}+\|x\|^{2}}
  (здесь 
  
    
      
        ‖
        y
        ‖
      
    
    {\displaystyle \|y\|}
   — Евклидова длина вектора: 
  
    
      
        ‖
        y
        
          ‖
          
            2
          
        
        =
        
          ∑
          
            i
          
        
        
          y
          
            i
          
          
            2
          
        
      
    
    {\displaystyle \|y\|^{2}=\sum _{i}y_{i}^{2}}
  ).
Последнее слагаемое 
  
    
      
        ‖
        x
        
          ‖
          
            2
          
        
      
    
    {\displaystyle \|x\|^{2}}
   одинаково для всех нейронов, поэтому для нахождения ближайшей точки оно не нужно. Задача сводится к поиску номера наибольшего из значений линейных функций:

  
    
      
        
          j
          
            max
          
        
        =
        
          
            a
            r
            g
          
        
        
          max
          
            j
          
        
        
          {
          
            
              ∑
              
                i
                =
                1
              
              
                m
              
            
            
              w
              
                j
                i
              
            
            
              x
              
                i
              
            
            −
            
              
                1
                2
              
            
            ‖
            
              W
              
                j
              
            
            
              ‖
              
                2
              
            
          
          }
        
        .
      
    
    {\displaystyle j_{\max }={\rm {arg}}\max _{j}\left\{\sum _{i=1}^{m}w_{ji}x_{i}-{\frac {1}{2}}\|W_{j}\|^{2}\right\}.}
  Таким образом, координаты точки 
  
    
      
        
          W
          
            j
          
        
        =
        (
        
          w
          
            j
            1
          
        
        ,
        .
        .
        .
        
          w
          
            j
            m
          
        
        )
      
    
    {\displaystyle W_{j}=(w_{j1},...w_{jm})}
   совпадают с весами линейного нейрона слоя Кохонена (при этом значение порогового коэффициента 
  
    
      
        
          w
          
            j
            0
          
        
        =
        −
        ‖
        
          W
          
            j
          
        
        
          ‖
          
            2
          
        
        
          /
        
        2
      
    
    {\displaystyle w_{j0}=-\|W_{j}\|^{2}/2}
  ).
Если заданы точки 
  
    
      
        
          W
          
            j
          
        
        =
        (
        
          w
          
            j
            1
          
        
        ,
        .
        .
        .
        
          w
          
            j
            m
          
        
        )
      
    
    {\displaystyle W_{j}=(w_{j1},...w_{jm})}
  , то 
  
    
      
        m
      
    
    {\displaystyle m}
  -мерное пространство разбивается на соответствующие многогранники Вороного-Дирихле 
  
    
      
        
          V
          
            j
          
        
      
    
    {\displaystyle V_{j}}
  : многогранник 
  
    
      
        
          V
          
            j
          
        
      
    
    {\displaystyle V_{j}}
   состоит из точек, которые ближе к 
  
    
      
        
          W
          
            j
          
        
      
    
    {\displaystyle W_{j}}
  , чем к другим 
  
    
      
        
          W
          
            k
          
        
      
    
    {\displaystyle W_{k}}
   (
  
    
      
        k
        ≠
        j
      
    
    {\displaystyle k\neq j}
  ).

Сети векторного квантования
Задача векторного квантования с 
  
    
      
        k
      
    
    {\displaystyle k}
   кодовыми векторами 
  
    
      
        
          W
          
            j
          
        
      
    
    {\displaystyle W_{j}}
   для заданной совокупности входных векторов 
  
    
      
        S
      
    
    {\displaystyle S}
   ставится как задача минимизации искажения при кодировании, то есть при замещении каждого вектора из 
  
    
      
        S
      
    
    {\displaystyle S}
   соответствующим кодовым вектором. В базовом варианте сетей Кохонена используется метод наименьших квадратов и искажение 
  
    
      
        D
      
    
    {\displaystyle D}
   вычисляется по формуле

  
    
      
        D
        =
        
          ∑
          
            j
            =
            1
          
          
            k
          
        
        
          ∑
          
            x
            ∈
            
              K
              
                j
              
            
          
        
        ‖
        x
        −
        
          W
          
            j
          
        
        
          ‖
          
            2
          
        
        ,
      
    
    {\displaystyle D=\sum _{j=1}^{k}\sum _{x\in K_{j}}\|x-W_{j}\|^{2},}
  где 
  
    
      
        
          K
          
            j
          
        
      
    
    {\displaystyle K_{j}}
   состоит из тех точек 
  
    
      
        x
        ∈
        S
      
    
    {\displaystyle x\in S}
  , которые ближе к 
  
    
      
        
          W
          
            j
          
        
      
    
    {\displaystyle W_{j}}
  , чем к другим 
  
    
      
        
          W
          
            l
          
        
      
    
    {\displaystyle W_{l}}
   (
  
    
      
        l
        ≠
        j
      
    
    {\displaystyle l\neq j}
  ). Другими словами, 
  
    
      
        
          K
          
            j
          
        
      
    
    {\displaystyle K_{j}}
   состоит из тех точек 
  
    
      
        x
        ∈
        S
      
    
    {\displaystyle x\in S}
  , которые кодируются кодовым вектором 
  
    
      
        
          W
          
            j
          
        
      
    
    {\displaystyle W_{j}}
  .
Если совокупность 
  
    
      
        S
      
    
    {\displaystyle S}
   задана и хранится в памяти, то стандартным выбором в обучении соответствующей сети Кохонена является метод K-средних. Это метод расщепления:

при данном выборе кодовых векторов (они же весовые векторы сети) 
  
    
      
        
          W
          
            j
          
        
      
    
    {\displaystyle W_{j}}
   минимизацией 
  
    
      
        D
      
    
    {\displaystyle D}
   находим множества 
  
    
      
        
          K
          
            j
          
        
      
    
    {\displaystyle K_{j}}
   — они состоят из тех точек 
  
    
      
        x
        ∈
        S
      
    
    {\displaystyle x\in S}
  , которые ближе к 
  
    
      
        
          W
          
            j
          
        
      
    
    {\displaystyle W_{j}}
  , чем к другим 
  
    
      
        
          W
          
            l
          
        
      
    
    {\displaystyle W_{l}}
  ;
при данном разбиении 
  
    
      
        S
      
    
    {\displaystyle S}
   на множества 
  
    
      
        
          K
          
            j
          
        
      
    
    {\displaystyle K_{j}}
   минимизацией 
  
    
      
        D
      
    
    {\displaystyle D}
   находим оптимальные позиции кодовых векторов 
  
    
      
        
          W
          
            j
          
        
      
    
    {\displaystyle W_{j}}
   — для оценки по методу наименьших квадратов это просто средние арифметические:
  
    
      
        
          W
          
            j
          
        
        =
        
          
            1
            
              
                |
              
              
                K
                
                  j
                
              
              
                |
              
            
          
        
        
          ∑
          
            x
            ∈
            
              K
              
                j
              
            
          
        
        x
        ,
      
    
    {\displaystyle W_{j}={\frac {1}{|K_{j}|}}\sum _{x\in K_{j}}x,}
  где 
  
    
      
        
          |
        
        
          K
          
            j
          
        
        
          |
        
      
    
    {\displaystyle |K_{j}|}
   — число элементов в 
  
    
      
        
          K
          
            j
          
        
      
    
    {\displaystyle K_{j}}
  .
Далее итерируем. Этот метод расщепления сходится за конечное число шагов и даёт локальный минимум искажения.
Если же, например, совокупность 
  
    
      
        S
      
    
    {\displaystyle S}
   заранее не задана, или по каким-либо причинам не хранится в памяти, то широко используется онлайн метод. Векторы входных сигналов 
  
    
      
        x
      
    
    {\displaystyle x}
   обрабатываются по одному, для каждого из них находится ближайший кодовый вектор («победитель», который «забирает всё») 
  
    
      
        
          W
          
            j
            (
            x
            )
          
        
      
    
    {\displaystyle W_{j(x)}}
  . После этого данный кодовый вектор пересчитывается по формуле

  
    
      
        
          W
          
            j
            (
            x
            )
          
          
            
              n
              e
              w
            
          
        
        =
        
          W
          
            j
            (
            x
            )
          
          
            
              o
              l
              d
            
          
        
        (
        1
        −
        θ
        )
        +
        x
        θ
        ,
      
    
    {\displaystyle W_{j(x)}^{\rm {new}}=W_{j(x)}^{\rm {old}}(1-\theta )+x\theta ,}
  где 
  
    
      
        θ
        ∈
        (
        0
        ,
        1
        )
      
    
    {\displaystyle \theta \in (0,1)}
   — шаг обучения. Остальные кодовые векторы на этом шаге не изменяются.
Для обеспечения стабильности используется онлайн метод с затухающей скоростью обучения: если 
  
    
      
        T
      
    
    {\displaystyle T}
   — количество шагов обучения, то полагают 
  
    
      
        θ
        =
        θ
        (
        T
        )
      
    
    {\displaystyle \theta =\theta (T)}
  . Функцию 
  
    
      
        θ
        (
        T
        )
        >
        0
      
    
    {\displaystyle \theta (T)>0}
   выбирают таким образом, чтобы 
  
    
      
        θ
        (
        T
        )
        →
        0
      
    
    {\displaystyle \theta (T)\to 0}
   монотонно при 
  
    
      
        T
        →
        ∞
      
    
    {\displaystyle T\to \infty }
   и чтобы ряд 
  
    
      
        
          ∑
          
            T
            =
            1
          
          
            ∞
          
        
        θ
        (
        T
        )
      
    
    {\displaystyle \sum _{T=1}^{\infty }\theta (T)}
   расходился, например, 
  
    
      
        θ
        (
        T
        )
        =
        
          θ
          
            0
          
        
        
          /
        
        T
      
    
    {\displaystyle \theta (T)=\theta _{0}/T}
  .
Векторное квантование является намного более общей операцией, чем кластеризация, поскольку кластеры должны быть разделены между собой, тогда как совокупности 
  
    
      
        
          K
          
            j
          
        
      
    
    {\displaystyle K_{j}}
   для разных кодовых векторов 
  
    
      
        
          W
          
            j
          
        
      
    
    {\displaystyle W_{j}}
   не обязательно представляют собой раздельные кластеры. С другой стороны, при наличии разделяющихся кластеров векторное квантование может находить их и по-разному кодировать.

Самоорганизующиеся карты Кохонена
Идея и алгоритм обучения
Задача векторного квантования состоит, по своему существу, в наилучшей аппроксимации всей совокупности векторов данных 
  
    
      
        k
      
    
    {\displaystyle k}
   кодовыми векторами 
  
    
      
        
          W
          
            j
          
        
      
    
    {\displaystyle W_{j}}
  . Самоорганизующиеся карты Кохонена также аппроксимируют данные, однако при наличии дополнительной структуры в совокупности кодовых векторов (англ. codebook). Предполагается, что априори задана некоторая симметричная таблица «мер соседства» (или «мер близости») узлов: для каждой пары 
  
    
      
        j
        ,
        l
      
    
    {\displaystyle j,l}
   (
  
    
      
        j
        ,
        l
        =
        1
        ,
        .
        .
        .
        k
      
    
    {\displaystyle j,l=1,...k}
  ) определено число 
  
    
      
        
          η
          
            j
            l
          
        
      
    
    {\displaystyle \eta _{jl}}
   (
  
    
      
        0
        ≤
        
          η
          
            j
            l
          
        
        ≤
        1
      
    
    {\displaystyle 0\leq \eta _{jl}\leq 1}
  ) при этом диагональные элементы таблицы близости равны единице (
  
    
      
        
          η
          
            j
            j
          
        
        =
        1
      
    
    {\displaystyle \eta _{jj}=1}
  ).
Векторы входных сигналов 
  
    
      
        x
      
    
    {\displaystyle x}
   обрабатываются по одному, для каждого из них находится ближайший кодовый вектор («победитель», который «забирает всё») 
  
    
      
        
          W
          
            j
            (
            x
            )
          
        
      
    
    {\displaystyle W_{j(x)}}
  . После этого все кодовые векторы 
  
    
      
        
          W
          
            l
          
        
      
    
    {\displaystyle W_{l}}
  , для которых 
  
    
      
        
          η
          
            j
            (
            x
            )
            l
          
        
        ≠
        0
      
    
    {\displaystyle \eta _{j(x)l}\neq 0}
  , пересчитываются по формуле

  
    
      
        
          W
          
            l
          
          
            
              n
              e
              w
            
          
        
        =
        
          W
          
            l
          
          
            
              o
              l
              d
            
          
        
        (
        1
        −
        
          η
          
            j
            (
            x
            )
            l
          
        
        θ
        )
        +
        x
        
          η
          
            j
            (
            x
            )
            l
          
        
        θ
        ,
      
    
    {\displaystyle W_{l}^{\rm {new}}=W_{l}^{\rm {old}}(1-\eta _{j(x)l}\theta )+x\eta _{j(x)l}\theta ,}
  где 
  
    
      
        θ
        ∈
        (
        0
        ,
        1
        )
      
    
    {\displaystyle \theta \in (0,1)}
   — шаг обучения. Соседи кодового вектора — победителя (по априорно заданной таблице близости) сдвигаются в ту же сторону, что и этот вектор, пропорционально мере близости.
Чаще всего, таблица кодовых векторов представляется в виде фрагмента квадратной решётки на плоскости, а мера близости определяется, исходя из евклидового расстояния на плоскости.
Самоорганизующиеся карты Кохонена служат, в первую очередь, для визуализации и первоначального («разведывательного») анализа данных. Каждая точка данных отображается соответствующим кодовым вектором из решётки. Так получают представление данных на плоскости («карту данных»). На этой карте возможно отображение многих слоёв: количество данных, попадающих в узлы (то есть «плотность данных»), различные функции данных и так далее. При отображении этих слоёв полезен аппарат географических информационных систем (ГИС). В ГИС подложкой для изображения информационных слоев служит географическая карта. Карта данных является подложкой для произвольного по своей природе набора данных. Карта данных служит заменой географической карте там, где географической карты просто не существует. Принципиальное отличие в следующем: на географической карте соседние объекты обладают близкими географическими координатами, на карте данных близкие объекты обладают близкими свойствами. С помощью карты данных можно визуализировать данные, одновременно нанося на подложку сопровождающую информацию (подписи, аннотации, атрибуты, информационные раскраски). Карта служит также информационной моделью данных. С её помощью можно заполнять пробелы в данных. Эта способность используется, например, для решения задач прогнозирования.

Самоорганизующиеся карты и главные многообразия
Идея самоорганизующихся карт очень привлекательна и породила массу обобщений, однако, строго говоря, мы не знаем, что мы строим: карта — это результат работы алгоритма и не имеет отдельного («объектного») определения. Есть, однако, близкая теоретическая идея — главные многообразия (англ. principal manifolds). Эти многообразия обобщают линейные главные компоненты. Они были введены как линии или поверхности, проходящие через «середину» распределения данных, с помощью условия самосогласованности: каждая точка 
  
    
      
        x
      
    
    {\displaystyle x}
   на главном многообразии 
  
    
      
        M
      
    
    {\displaystyle M}
   является условным математическим ожиданием тех векторов 
  
    
      
        z
      
    
    {\displaystyle z}
  , которые проектируются на 
  
    
      
        x
      
    
    {\displaystyle x}
   (при условии 
  
    
      
        x
        =
        P
        (
        z
        )
      
    
    {\displaystyle x=P(z)}
  , где 
  
    
      
        P
      
    
    {\displaystyle P}
   — оператор проектирования окрестности 
  
    
      
        M
      
    
    {\displaystyle M}
   на 
  
    
      
        M
      
    
    {\displaystyle M}
  ),

  
    
      
        x
        =
        
          E
        
        (
        z
        
          |
        
        P
        (
        z
        )
        =
        x
        )
        .
      
    
    {\displaystyle x=\mathbf {E} (z|P(z)=x).}
  Самоорганизующиеся карты могут рассматриваться как аппроксимации главных многообразий и популярны в этом качестве.

Упругие карты
Метод аппроксимации многомерных данных, основанный на минимизации «энергии упругой деформации» карты, погружённой в пространство данных, был предложен  А. Н. Горбанём в 1996 году, и впоследствии развит им совместно с А. Ю. Зиновьевым, А. А. Россиевым и А. А. Питенко. Метод основан на аналогии между главным многообразием и эластичной мембраной и упругой пластиной. В этом смысле он является развитием классической идеи сплайна (хотя упругие карты и не являются многомерными сплайнами).
Пусть задана совокупность входных векторов 
  
    
      
        S
      
    
    {\displaystyle S}
  . Так же, как и сети векторного квантования и самоорганизующиеся карты, упругая карта представлена как совокупность кодовых векторов (узлов) 
  
    
      
        
          W
          
            j
          
        
      
    
    {\displaystyle W_{j}}
   в пространстве сигналов. Множество данных 
  
    
      
        S
      
    
    {\displaystyle S}
   разделено на классы 
  
    
      
        
          K
          
            j
          
        
      
    
    {\displaystyle K_{j}}
  , состоящие из тех точек 
  
    
      
        x
        ∈
        S
      
    
    {\displaystyle x\in S}
  , которые ближе к 
  
    
      
        
          W
          
            j
          
        
      
    
    {\displaystyle W_{j}}
  , чем к другим 
  
    
      
        
          W
          
            l
          
        
      
    
    {\displaystyle W_{l}}
   (
  
    
      
        l
        ≠
        j
      
    
    {\displaystyle l\neq j}
  ). Искажение кодирования 
  
    
      
        D
      
    
    {\displaystyle D}
  

  
    
      
        D
        =
        
          ∑
          
            j
            =
            1
          
          
            k
          
        
        
          ∑
          
            x
            ∈
            
              K
              
                j
              
            
          
        
        ‖
        x
        −
        
          W
          
            j
          
        
        
          ‖
          
            2
          
        
        ,
      
    
    {\displaystyle D=\sum _{j=1}^{k}\sum _{x\in K_{j}}\|x-W_{j}\|^{2},}
  может трактоваться как суммарная энергия пружин единичной жёсткости, связывающих векторы данных с соответствующими кодовыми векторами.
На множестве узлов задана дополнительная структура: некоторые пары связаны «упругими связями», а некоторые тройки объединены в «рёбра жёсткости». Обозначим множество пар, связанных упругими связями, через 
  
    
      
        E
      
    
    {\displaystyle E}
  , а множество троек, составляющих рёбра жёсткости, через 
  
    
      
        G
      
    
    {\displaystyle G}
  . Например, в квадратной решётке ближайшие узлы (как по вертикали, так и погоризонтали) связываются упругими связями, а ребра жёсткости образуются вертикальными и горизонтальными тройками ближайших узлов. Энергия деформации карты состоит из двух слагаемых:

энергия растяжения 
  
    
      
        
          U
          
            E
          
        
        =
        λ
        
          ∑
          
            (
            
              W
              
                i
              
            
            ,
            
              W
              
                j
              
            
            )
            ∈
            E
          
        
        ‖
        
          W
          
            i
          
        
        −
        
          W
          
            j
          
        
        
          ‖
          
            2
          
        
        ;
      
    
    {\displaystyle U_{E}=\lambda \sum _{(W_{i},W_{j})\in E}\|W_{i}-W_{j}\|^{2};}
  
энергия изгиба 
  
    
      
        
          U
          
            G
          
        
        =
        μ
        
          ∑
          
            (
            
              W
              
                i
              
            
            ,
            
              W
              
                j
              
            
            ,
            
              W
              
                l
              
            
            )
            ∈
            G
          
        
        ‖
        
          W
          
            i
          
        
        −
        2
        
          W
          
            j
          
        
        +
        
          W
          
            l
          
        
        
          ‖
          
            2
          
        
        ;
      
    
    {\displaystyle U_{G}=\mu \sum _{(W_{i},W_{j},W_{l})\in G}\|W_{i}-2W_{j}+W_{l}\|^{2};}
  где 
  
    
      
        λ
        ,
        μ
      
    
    {\displaystyle \lambda ,\mu }
   — соответствующие модули упругости.
Задача построения упругой карты состоит в минимизации функционала

  
    
      
        U
        =
        D
        +
        
          U
          
            E
          
        
        +
        
          U
          
            G
          
        
        ;
      
    
    {\displaystyle U=D+U_{E}+U_{G};}
  Если разбиение совокупности входных векторов 
  
    
      
        S
      
    
    {\displaystyle S}
   на классы 
  
    
      
        
          K
          
            j
          
        
      
    
    {\displaystyle K_{j}}
   фиксировано, то минимизация 
  
    
      
        U
      
    
    {\displaystyle U}
   — линейная задача с разреженной матрицей коэффициентов. Поэтому, как и для сетей векторного квантования, применяется метод расщепления: фиксируем 
  
    
      
        {
        
          W
          
            j
          
        
        }
      
    
    {\displaystyle \{W_{j}\}}
   — ищем 
  
    
      
        {
        
          K
          
            j
          
        
        }
      
    
    {\displaystyle \{K_{j}\}}
   — для данных 
  
    
      
        {
        
          K
          
            j
          
        
        }
      
    
    {\displaystyle \{K_{j}\}}
   ищем 
  
    
      
        {
        
          W
          
            j
          
        
        }
      
    
    {\displaystyle \{W_{j}\}}
   — для данных 
  
    
      
        {
        
          W
          
            j
          
        
        }
      
    
    {\displaystyle \{W_{j}\}}
   ищем 
  
    
      
        {
        
          K
          
            j
          
        
        }
      
    
    {\displaystyle \{K_{j}\}}
   — …
Алгоритм сходится к (локальному) минимуму 
  
    
      
        U
      
    
    {\displaystyle U}
  .
Метод упругих карт позволяет решать все задачи, которые решают самоорганизующиеся карты Кохонена, однако обладает большей регулярностью и предсказуемостью. При увеличении модуля изгиба 
  
    
      
        μ
      
    
    {\displaystyle \mu }
   упругие карты приближаются к линейным главным компонентам. При уменьшении обоих модулей упругости они превращаются в Кохоненовские сети векторного квантования. В настоящее время упругие карты интенсивно используются для анализа многомерных данных в биоинформатике. Соответствующее программное обеспечение опубликовано и свободно доступно на сайте института Кюри (Париж).
На рисунке представлены результаты визуализации данных по раку молочной железы. Эти данные содержат 286 примеров с указанием уровня экспрессии 17816 генов. Они доступны онлайн как ставший классическим тестовый пример для визуализации и картографии данных.

Сети векторного квантования, обучаемые с учителем
Решается задача классификации. Число классов может быть любым. Изложим алгоритм для двух классов, 
  
    
      
        
          
            A
          
        
      
    
    {\displaystyle {\mathbf {A} }}
   и 
  
    
      
        
          
            B
          
        
      
    
    {\displaystyle {\mathbf {B} }}
  . Исходно для обучения системы поступают данные, класс которых известен. Задача: найти для класса 
  
    
      
        
          
            A
          
        
      
    
    {\displaystyle {\mathbf {A} }}
   некоторое количество 
  
    
      
        
          k
          
            
              A
            
          
        
      
    
    {\displaystyle k_{\mathbf {A} }}
   кодовых векторов 
  
    
      
        
          W
          
            j
          
          
            
              A
            
          
        
      
    
    {\displaystyle W_{j}^{\mathbf {A} }}
  , а для класса 
  
    
      
        
          
            B
          
        
      
    
    {\displaystyle {\mathbf {B} }}
   некоторое (возможно другое) количество 
  
    
      
        
          k
          
            
              B
            
          
        
      
    
    {\displaystyle k_{\mathbf {B} }}
   кодовых векторов 
  
    
      
        
          W
          
            l
          
          
            
              B
            
          
        
      
    
    {\displaystyle W_{l}^{\mathbf {B} }}
   таким образом, чтобы итоговая сеть Кохонена с 
  
    
      
        
          k
          
            
              A
            
          
        
        +
        
          k
          
            
              B
            
          
        
      
    
    {\displaystyle k_{\mathbf {A} }+k_{\mathbf {B} }}
   кодовыми векторами

  
    
      
        
          W
          
            j
          
          
            
              A
            
          
        
      
    
    {\displaystyle W_{j}^{\mathbf {A} }}
  , 
  
    
      
        
          W
          
            l
          
          
            
              B
            
          
        
      
    
    {\displaystyle W_{l}^{\mathbf {B} }}
   (объединяем оба семейства) осуществляла классификацию по следующему решающему правилу:

если для вектора входных сигналов 
  
    
      
        x
      
    
    {\displaystyle x}
   ближайший кодовый вектор («победитель», который в слое Кохонена «забирает всё») принадлежит семейству 
  
    
      
        {
        
          W
          
            j
          
          
            
              A
            
          
        
        }
      
    
    {\displaystyle \{W_{j}^{\mathbf {A} }\}}
  , то 
  
    
      
        x
      
    
    {\displaystyle x}
   принадлежит классу 
  
    
      
        
          
            A
          
        
      
    
    {\displaystyle {\mathbf {A} }}
  ; если же ближайший к 
  
    
      
        x
      
    
    {\displaystyle x}
   кодовый вектор принадлежит семейству 
  
    
      
        {
        
          W
          
            l
          
          
            
              B
            
          
        
        }
      
    
    {\displaystyle \{W_{l}^{\mathbf {B} }\}}
  , то 
  
    
      
        x
      
    
    {\displaystyle x}
   принадлежит классу 
  
    
      
        
          
            B
          
        
      
    
    {\displaystyle {\mathbf {B} }}
  .С каждым кодовым вектором объединённого семейства 
  
    
      
        {
        
          W
          
            j
          
          
            
              A
            
          
        
        }
        ∪
        {
        
          W
          
            l
          
          
            
              B
            
          
        
        }
      
    
    {\displaystyle \{W_{j}^{\mathbf {A} }\}\cup \{W_{l}^{\mathbf {B} }\}}
   связан многогранник Вороного-Дирихле. Обозначим эти многогранники 
  
    
      
        
          V
          
            j
          
          
            
              A
            
          
        
      
    
    {\displaystyle V_{j}^{\mathbf {A} }}
  , 
  
    
      
        
          V
          
            l
          
          
            
              B
            
          
        
      
    
    {\displaystyle V_{l}^{\mathbf {B} }}
   соответственно. Класс 
  
    
      
        
          
            A
          
        
      
    
    {\displaystyle {\mathbf {A} }}
   в пространстве сигналов, согласно решающему правилу, соответствует объединению 
  
    
      
        
          ∪
          
            j
          
        
        
          V
          
            j
          
          
            
              A
            
          
        
      
    
    {\displaystyle \cup _{j}V_{j}^{\mathbf {A} }}
  , а класс 
  
    
      
        
          
            B
          
        
      
    
    {\displaystyle {\mathbf {B} }}
   соответствует объединению 
  
    
      
        
          ∪
          
            l
          
        
        
          V
          
            l
          
          
            
              B
            
          
        
      
    
    {\displaystyle \cup _{l}V_{l}^{\mathbf {B} }}
  . Геометрия таких объединений многогранников может быть весьма сложной (см. рисунок с примером возможного разбиения на классы).
Правила обучения сети онлайн строится на основе базового правила обучения сети векторного квантования. Пусть на вход системы подаётся вектор сигналов 
  
    
      
        x
      
    
    {\displaystyle x}
  , класс которого известен. Если он классифицируется системой правильно, то соответствующий 
  
    
      
        x
      
    
    {\displaystyle x}
   кодовый вектор 
  
    
      
        W
      
    
    {\displaystyle W}
  слегка сдвигается в сторону вектора сигнала («поощрение»)

  
    
      
        
          W
          
            
              n
              e
              w
            
          
        
        =
        
          W
          
            
              o
              l
              d
            
          
        
        (
        1
        −
        θ
        )
        +
        x
        θ
        ,
      
    
    {\displaystyle W^{\rm {new}}=W^{\rm {old}}(1-\theta )+x\theta ,}
  Если же 
  
    
      
        x
      
    
    {\displaystyle x}
   классифицируется неправильно, то соответствующий 
  
    
      
        x
      
    
    {\displaystyle x}
   кодовый вектор 
  
    
      
        W
      
    
    {\displaystyle W}
  слегка сдвигается в противоположную сторону от сигнала («наказание»)

  
    
      
        
          W
          
            
              n
              e
              w
            
          
        
        =
        
          W
          
            
              o
              l
              d
            
          
        
        (
        1
        +
        θ
        )
        −
        x
        θ
        ,
      
    
    {\displaystyle W^{\rm {new}}=W^{\rm {old}}(1+\theta )-x\theta ,}
  где 
  
    
      
        θ
        ∈
        (
        0
        ,
        1
        )
      
    
    {\displaystyle \theta \in (0,1)}
   — шаг обучения. Для обеспечения стабильности используется онлайн метод с затухающей скоростью обучения. Возможно также использование разных шагов для «поощрения» правильного решения и для «наказания» неправильного.
Это — простейшая (базовая) версия метода. Существует множество других модификаций.

Примечания
См. также
Раскраска графа