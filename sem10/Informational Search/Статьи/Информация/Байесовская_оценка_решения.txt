В математической статистике и теории принятия решений байесовская оценка решения — это статистическая оценка, минимизирующая апостериорное математическое ожидание функции потерь (то есть апостериорное ожидание потерь). Иначе говоря, она максимизирует апостериорное математическое ожидание функции полезности. В рамках теории Байеса данную оценку можно определить как оценку апостериорного максимума.

Определение
Предположим, что неизвестный параметр 
  
    
      
        θ
      
    
    {\displaystyle \theta }
   имеет априорное распределение 
  
    
      
        π
      
    
    {\displaystyle \pi }
  . Пусть 
  
    
      
        
          
            
              θ
              ^
            
          
        
        =
        
          
            
              θ
              ^
            
          
        
        (
        x
        )
      
    
    {\displaystyle {\hat {\theta }}={\hat {\theta }}(x)}
   — оценка параметра 
  
    
      
        θ
      
    
    {\displaystyle \theta }
  , основанная на некоторых измерениях 
  
    
      
        x
      
    
    {\displaystyle x}
  , и пусть 
  
    
      
        L
        (
        θ
        ,
        
          
            
              θ
              ^
            
          
        
        )
      
    
    {\displaystyle L(\theta ,{\hat {\theta }})}
   — квадратичная функция потерь, а байесовский риск параметра 
  
    
      
        
          
            
              θ
              ^
            
          
        
      
    
    {\displaystyle {\hat {\theta }}}
   — это 
  
    
      
        
          E
          
            π
          
        
        (
        L
        (
        θ
        ,
        
          
            
              θ
              ^
            
          
        
        )
        )
      
    
    {\displaystyle E_{\pi }(L(\theta ,{\hat {\theta }}))}
  , где математическое ожидание берётся по распределению 
  
    
      
        θ
      
    
    {\displaystyle \theta }
  : это определяет функцию риска как функцию от 
  
    
      
        
          
            
              θ
              ^
            
          
        
      
    
    {\displaystyle {\hat {\theta }}}
  . Тогда байесовской оценкой будет называться такая оценка 
  
    
      
        
          
            
              θ
              ^
            
          
        
      
    
    {\displaystyle {\hat {\theta }}}
  , которая минимизирует байесовский риск среди всех прочих оценок. Равнозначно оценка, минимизирующая апостериорные ожидаемые потери 
  
    
      
        E
        (
        L
        (
        θ
        ,
        
          
            
              θ
              ^
            
          
        
        )
        ∣
        x
        )
      
    
    {\displaystyle E(L(\theta ,{\hat {\theta }})\mid x)}
   для каждого x, также минимизирует байесовский риск и таким образом является байесовской оценкой.В случае некорректного априорного распределения оценка, минимизирующая апостериорное ожидание потерь для каждого x, называется обобщённой байесовской оценкой.

Примеры
Оценка минимальной среднеквадратичной ошибки
Наиболее часто используемой функцией риска для Байесовской оценки является функция среднеквадратичной ошибки (в англоязычной литературе обозначаемая как MSE).[1] MSE определяется как 
  
    
      
        
          M
          S
          E
        
        =
        E
        
          [
          
            (
            
              
                
                  θ
                  ^
                
              
            
            (
            x
            )
            −
            θ
            
              )
              
                2
              
            
          
          ]
        
        ,
      
    
    {\displaystyle \mathrm {MSE} =E\left[({\widehat {\theta }}(x)-\theta )^{2}\right],}
  
где Математическое ожидание берётся по совместному распределению 
  
    
      
        θ
      
    
    {\displaystyle \theta }
   и 
  
    
      
        x
      
    
    {\displaystyle x}
  .

Апостериорное среднее
Если использовать MSE как функцию риска, то Байесовская оценка неизвестного параметра — это просто среднее апостериорного распределения:
  
    
      
        
          
            
              θ
              ^
            
          
        
        (
        x
        )
        =
        E
        [
        θ
        
          |
        
        x
        ]
        =
        ∫
        θ
        p
        (
        θ
        
          |
        
        x
        )
        
        d
        θ
        .
      
    
    {\displaystyle {\widehat {\theta }}(x)=E[\theta |x]=\int \theta p(\theta |x)\,d\theta .}
  
Это известно как оценка минимальной среднеквадратичной ошибки. Байесовский риск, в этом случае, это апостериорная дисперсия.

Байесовский риск для сопряжённого априорного распределения
В тех случаях, когда нет веских причин предпочесть одно априорное распределение вероятности над другим, для простоты используется cопряжённое априорное распределение. Оно определяется как априорное распределение, принадлежащее некоторому параметрическому семейству, чьё результирующее апостериорное распределение также принадлежит этому семейству. Это важное свойство, поскольку Байесовская оценка, а также его статистические характеристики (дисперсия, доверительный интервал и т. д.) могут быть получены из апостериорного распределения.
Оно, в частности, применимо в последовательном оценивании, где апостериорное распределение текущих измерений используется как априорное в следующем измерении. С каждой новой итерацией таких измерений апостериорное распределение обычно становится всё более сложным, и часто Байесовская оценка не может вычислена без использования численных методов.
Несколько примеров сопряжённых априорных распределений:

Если x|θ распределен нормально, x|θ ~ N(θ,σ2) и априорное распределение тоже нормально, θ ~ N(μ,τ2), тогда апостериорное распределение тоже имеет нормальное распределение и Байесовская оценка под MSE задаётся как:
  
    
      
        
          
            
              θ
              ^
            
          
        
        (
        x
        )
        =
        
          
            
              σ
              
                2
              
            
            
              
                σ
                
                  2
                
              
              +
              
                τ
                
                  2
                
              
            
          
        
        μ
        +
        
          
            
              τ
              
                2
              
            
            
              
                σ
                
                  2
                
              
              +
              
                τ
                
                  2
                
              
            
          
        
        x
        .
      
    
    {\displaystyle {\widehat {\theta }}(x)={\frac {\sigma ^{2}}{\sigma ^{2}+\tau ^{2}}}\mu +{\frac {\tau ^{2}}{\sigma ^{2}+\tau ^{2}}}x.}
  

Если x1,…,xn — независимые одинаково распределённые по Пуассону случайные величины xi|θ ~ P(θ), и если априорное распределено по гамма-распределению θ ~ G(a, b), тогда апостериорное тоже имеет гамма-распределение, и Байесовская оценка под MSE задаётся как:
  
    
      
        
          
            
              θ
              ^
            
          
        
        (
        X
        )
        =
        
          
            
              n
              
                
                  X
                  ¯
                
              
              +
              a
            
            
              n
              +
              
                
                  1
                  b
                
              
            
          
        
        .
      
    
    {\displaystyle {\widehat {\theta }}(X)={\frac {n{\overline {X}}+a}{n+{\frac {1}{b}}}}.}
  

Если x1,…,xn независимые одинаково непрерывно равномерно распределенные случайные величины xi|θ~U(0,θ), а априорное имеет распределение Парето θ~Pa(θ0,a), тогда апостериорное также имеет распределение Парето, и Байесовская оценка под MSE задаётся как:
  
    
      
        
          
            
              θ
              ^
            
          
        
        (
        X
        )
        =
        
          
            
              (
              a
              +
              n
              )
              max
              
                (
                
                  θ
                  
                    0
                  
                
                ,
                
                  x
                  
                    1
                  
                
                ,
                .
                .
                .
                ,
                
                  x
                  
                    n
                  
                
                )
              
            
            
              a
              +
              n
              −
              1
            
          
        
        .
      
    
    {\displaystyle {\widehat {\theta }}(X)={\frac {(a+n)\max {(\theta _{0},x_{1},...,x_{n})}}{a+n-1}}.}

Альтернативные функции риска
Функции риска выбираются в зависимости от того, как измеряется интервал между оценкой и неизвестным параметром. MSE наиболее часто используемая функция риска, в первую очередь из за её простоты. Тем не менее, иногда используются и альтернативные функции риска. Далее идут несколько примеров таких альтернатив. Далее апостериорная обобщённая функция распределения обозначена как 
  
    
      
        F
      
    
    {\displaystyle F}
  .

Апостериорная медиана и другие квантили
«Линейная» функция потерь с 
  
    
      
        a
        >
        0
      
    
    {\displaystyle a>0}
  , выбирающая медиану апостериорного распределения как Байесовскую оценку:
  
    
      
        L
        (
        θ
        ,
        
          
            
              θ
              ^
            
          
        
        )
        =
        a
        
          |
        
        θ
        −
        
          
            
              θ
              ^
            
          
        
        
          |
        
      
    
    {\displaystyle L(\theta ,{\widehat {\theta }})=a|\theta -{\widehat {\theta }}|}
  

  
    
      
        F
        (
        
          
            
              θ
              ^
            
          
        
        (
        x
        )
        
          |
        
        X
        )
        =
        
          
            
              1
              2
            
          
        
        .
      
    
    {\displaystyle F({\widehat {\theta }}(x)|X)={\tfrac {1}{2}}.}
  Другая «линейная» функция потерь, назначающая разные «веса» 
  
    
      
        a
        ,
        b
        >
        0
      
    
    {\displaystyle a,b>0}
   сверху или снизу оценки. Она выбирает квантиль из из апостериорного распределения и является обобщением предыдущей функции потерь.
  
    
      
        L
        (
        θ
        ,
        
          
            
              θ
              ^
            
          
        
        )
        =
        
          
            {
            
              
                
                  a
                  
                    |
                  
                  θ
                  −
                  
                    
                      
                        θ
                        ^
                      
                    
                  
                  
                    |
                  
                  ,
                
                
                  
                    
                      for 
                    
                  
                  θ
                  −
                  
                    
                      
                        θ
                        ^
                      
                    
                  
                  ≥
                  0
                
              
              
                
                  b
                  
                    |
                  
                  θ
                  −
                  
                    
                      
                        θ
                        ^
                      
                    
                  
                  
                    |
                  
                  ,
                
                
                  
                    
                      for 
                    
                  
                  θ
                  −
                  
                    
                      
                        θ
                        ^
                      
                    
                  
                  <
                  0
                
              
            
            
          
        
      
    
    {\displaystyle L(\theta ,{\widehat {\theta }})={\begin{cases}a|\theta -{\widehat {\theta }}|,&{\mbox{for }}\theta -{\widehat {\theta }}\geq 0\\b|\theta -{\widehat {\theta }}|,&{\mbox{for }}\theta -{\widehat {\theta }}<0\end{cases}}}
  

  
    
      
        F
        (
        
          
            
              θ
              ^
            
          
        
        (
        x
        )
        
          |
        
        X
        )
        =
        
          
            a
            
              a
              +
              b
            
          
        
        .
      
    
    {\displaystyle F({\widehat {\theta }}(x)|X)={\frac {a}{a+b}}.}

Оценка апостериорного максимума
Следующая функция потерь более сложная: она устанавливает оценку апостериорного максимума или точку, близкую к ней, в зависимости от кривизны и характеристик апостериорного распределения. Маленькие значения параметра 
  
    
      
        K
        >
        0
      
    
    {\displaystyle K>0}
   рекомендованы для использования метода как приближения(
  
    
      
        L
        >
        0
      
    
    {\displaystyle L>0}
  ):

  
    
      
        L
        (
        θ
        ,
        
          
            
              θ
              ^
            
          
        
        )
        =
        
          
            {
            
              
                
                  0
                  ,
                
                
                  
                    
                      for 
                    
                  
                  
                    |
                  
                  θ
                  −
                  
                    
                      
                        θ
                        ^
                      
                    
                  
                  
                    |
                  
                  <
                  K
                
              
              
                
                  L
                  ,
                
                
                  
                    
                      for 
                    
                  
                  
                    |
                  
                  θ
                  −
                  
                    
                      
                        θ
                        ^
                      
                    
                  
                  
                    |
                  
                  ≥
                  K
                  .
                
              
            
            
          
        
      
    
    {\displaystyle L(\theta ,{\widehat {\theta }})={\begin{cases}0,&{\mbox{for }}|\theta -{\widehat {\theta }}|<K\\L,&{\mbox{for }}|\theta -{\widehat {\theta }}|\geq K.\end{cases}}}
  Несмотря на то, что функция среднеквадратичной ошибки наиболее распространена и обоснованна, можно использовать и другие функции потерь.

Обобщённые Баесовские оценки
До сих пор предполагалось, что априорное распределение 
  
    
      
        p
      
    
    {\displaystyle p}
   — это истинное вероятностное распределение, так как

  
    
      
        ∫
        p
        (
        θ
        )
        d
        θ
        =
        1.
      
    
    {\displaystyle \int p(\theta )d\theta =1.}
  Однако, порой это может быть слишком жестким требованием. Например, не существует такого распределения (покрывающего всё множество R вещественных чисел), для которого каждое вещественное число было бы равновозможным. Однако же, в некотором смысле, такое распределение кажется естественным выбором для неинформативного априорного распределения, то есть для априорного распределения, не отдающего предпочтения некоторому фиксированному значению неизвестного параметра. По прежнему можно определить функцию 
  
    
      
        p
        (
        θ
        )
        =
        1
      
    
    {\displaystyle p(\theta )=1}
  , но это уже не будет корректным вероятностным распределением, так как оно имеет бесконечную массу.

  
    
      
        ∫
        
          p
          (
          θ
          )
          d
          θ
        
        =
        ∞
        .
      
    
    {\displaystyle \int {p(\theta )d\theta }=\infty .}
  Такие меры множества 
  
    
      
        p
        (
        θ
        )
      
    
    {\displaystyle p(\theta )}
   являются некорректными априорными распределениями.
Использование некорректных априорных распределений означает, что Байесовский риск не определён (так как данное априорное распределение, по факту, не является вероятностным распределением и мы не можем взять Математическое ожидание от него). Следовательно, неверно говорить о Байесовской оценке минимизирующей Байесовский риск. Как бы то ни было, можно вычислить апостериорное распределение как

  
    
      
        p
        (
        θ
        
          |
        
        x
        )
        =
        
          
            
              p
              (
              x
              
                |
              
              θ
              )
              p
              (
              θ
              )
            
            
              ∫
              p
              (
              x
              
                |
              
              θ
              )
              p
              (
              θ
              )
              d
              θ
            
          
        
        .
      
    
    {\displaystyle p(\theta |x)={\frac {p(x|\theta )p(\theta )}{\int p(x|\theta )p(\theta )d\theta }}.}
  Не стоит забывать, что Теорема Байеса применима только к корректным распределениям, и значит не представляется возможным использование её здесь. Тем не менее, нередко встречаются случаи, когда для результирующего апостериорного распределения будет допустимы такие вероятностные распределения. В этом случае, апостериорные ожидаемые потери

  
    
      
        ∫
        
          L
          (
          θ
          ,
          a
          )
          p
          (
          θ
          
            |
          
          x
          )
          d
          θ
        
      
    
    {\displaystyle \int {L(\theta ,a)p(\theta |x)d\theta }}
  
хорошо определены и конечны. Напомним, что для корректного распределения Байесовские оценки минимизируют апостериорные потери. Когда априорное распределение некорректно, оценка минимизирующая апостериорное ожидание потери называется обобщённой Байесовской оценкой.

Эмпирические Байесовские оценки
Байесовские оценки, полученные эмпирическим методом Байеса называются эмпирическими Байесовскими оценками. Этот метод позволяет использовать вспомогательные данные в разработке Байесовской оценки. Их можно получить эмпирически, путём наблюдения за смежными параметрами. Это делается исходя из предположения, что оцениваемые параметры берутся из одних и тех же априорных данных. Например, если произвести независимые наблюдения за разными параметрами, то иногда можно улучшить эффективность оценки конкретного параметра путём использования данных из других наблюдений.
Существуют параметрические и непараметрические методики эмпирических Байесовских оценок. Параметрические предпочтительнее, потому что более применимы и более аккуратны на небольших объёмах данных.

Свойства
Допустимость
Байесовские правила, имеющие конечный Байесовский риск обычно являются допустимыми. Далее приведены некоторые примеры теорем о допустимости.

Если Байесовское решающее правило уникально, значит оно приемлемо. К примеру, как указано выше, под среднеквадратической ошибкой (MSE) Байесовское правило уникально и, следовательно, допустимо.
Если параметр θ принадлежит дискретному множеству, тогда все Байесовские правила допустимы.
Если параметр θ принадлежит непрерывному (не-дискретному множеству), и функция риска R(θ,δ) непрерывна в θ для каждого δ, тогда все Байесовские правила допустимы.В то же время, обобщённое Байесовское правило часто не определяет Байесовский риск в случае некорректного априорного распределения. Эти правила часто недопустимы и подтверждение их допустимости может вызвать затруднения. Для примера, обобщённая Байесовская оценка сдвига параметра θ, основанная на выборке с нормальным распределением, недопустима для 
  
    
      
        p
        >
        2
      
    
    {\displaystyle p>2}
  . Этот парадокс известен как парадокс Штайна.[2]

Практические примеры использования Байесовских оценок
Сайт Internet Movie Database использует специальную формулу для расчёта и сравнения рейтингов фильмов пользователями. Следующая байесовская формула изначально использовалась для расчёта взвешенного среднего показателя для Топ-250 фильмов, впрочем с тех пор формула изменилась:

  
    
      
        W
        =
        
          
            
              R
              v
              +
              C
              m
            
            
              v
              +
              m
            
          
        
         
      
    
    {\displaystyle W={Rv+Cm \over v+m}\ }
  где:

  
    
      
        W
         
      
    
    {\displaystyle W\ }
   = взвешенный рейтинг

  
    
      
        R
         
      
    
    {\displaystyle R\ }
   = средний рейтинг фильма, выраженный числом от 1 до 10 = (рейтинг)

  
    
      
        v
         
      
    
    {\displaystyle v\ }
   = количество голосов за фильм = (голоса)

  
    
      
        m
         
      
    
    {\displaystyle m\ }
   = вес, поставленный априорной оценкой (оценка основывается на распределении среднего рейтинга среди всех фильмов)

  
    
      
        C
         
      
    
    {\displaystyle C\ }
   = средняя оценка по всем фильмам (в настоящее время равняется 7.0)Подход IMDB гарантирует, что фильм, оцененный несколько сот раз исключительно оценкой 10 не сможет подняться в рейтинге выше, чем, например, фильм «Крёстный отец», со средней оценкой 9.2 от более чем 500,000 пользователей.

См. также
Байесовское программирование

Примечания
Ссылки
http://info.alnam.ru/book_osr.php?id=91
http://lib.alnam.ru/book_inst.php?id=24
Интуитивное объяснение теоремы Байеса