Достаточная статистика для параметра 
  
    
      
        θ
        ∈
        Θ
        ,
        
      
    
    {\displaystyle \theta \in \Theta ,\;}
   определяющая некоторое семейство 
  
    
      
        
          F
          
            θ
          
        
      
    
    {\displaystyle F_{\theta }}
   распределений вероятности — статистика 
  
    
      
        T
        =
        
          T
        
        (
        X
        )
        ,
        
      
    
    {\displaystyle T=\mathrm {T} (X),\;}
   такая, что условная вероятность выборки 
  
    
      
        X
        =
        
          X
          
            1
          
        
        ,
        
          X
          
            2
          
        
        ,
        …
        ,
        
          X
          
            n
          
        
        
      
    
    {\displaystyle X=X_{1},X_{2},\ldots ,X_{n}\;}
   при данном значении 
  
    
      
        
          T
        
        (
        X
        )
        
      
    
    {\displaystyle \mathrm {T} (X)\;}
   не зависит от параметра 
  
    
      
        θ
        
        .
      
    
    {\displaystyle \theta \;.}
   То есть выполняется равенство:

  
    
      
        
          P
        
        (
        X
        ∈
        
          
            
              X
              ¯
            
          
        
        
          |
        
        
          T
        
        (
        X
        )
        =
        t
        ,
        θ
        )
        =
        
          P
        
        (
        X
        ∈
        
          
            
              X
              ¯
            
          
        
        
          |
        
        
          T
        
        (
        X
        )
        =
        t
        )
        ,
      
    
    {\displaystyle \mathbb {P} (X\in {\bar {X}}|\mathrm {T} (X)=t,\theta )=\mathbb {P} (X\in {\bar {X}}|\mathrm {T} (X)=t),}
  Достаточная статистика 
  
    
      
        
          T
        
        (
        X
        )
        ,
        
      
    
    {\displaystyle \mathrm {T} (X),\;}
   таким образом содержит в себе всю информацию о параметре 
  
    
      
        θ
        
      
    
    {\displaystyle \theta \;}
  , которая может быть получена на основе выборки X. Поэтому понятие достаточной статистики широко используется в теории оценки параметров.
Наиболее простой достаточной статистикой является сама выборка 
  
    
      
        
          T
        
        (
        X
        )
        =
        X
        
      
    
    {\displaystyle \mathrm {T} (X)=X\;}
  , однако действительно важными являются случаи, когда размерность достаточной статистики значительно меньше размерности выборки, в частности, когда достаточная статистика выражается лишь несколькими числами.
Достаточная статистика 
  
    
      
        S
        =
        
          S
        
        (
        X
        )
        
      
    
    {\displaystyle S=\mathrm {S} (X)\;}
   называется минимально достаточной, если для каждой достаточной статистики T существует неслучайная измеримая функция g, что 
  
    
      
        S
        (
        X
        )
        =
        g
        (
        T
        (
        X
        )
        )
      
    
    {\displaystyle S(X)=g(T(X))}
   почти всюду.

Теорема факторизации
Теорема факторизации даёт способ практического нахождения достаточной статистики для распределения вероятности. Она даёт достаточные и необходимые условия достаточности статистики и утверждение теорем иногда используется в качестве определения.
Пусть 
  
    
      
        
          T
        
        (
        X
        )
        
      
    
    {\displaystyle \mathrm {T} (X)\;}
   — некоторая статистика, а 
  
    
      
        
          f
          
            θ
          
        
        (
        x
        )
      
    
    {\displaystyle f_{\theta }(x)}
   — условная функция плотности или функция вероятности (в зависимости от вида распределения) для вектора наблюдений X. Тогда 
  
    
      
        
          T
        
        (
        X
        )
        
      
    
    {\displaystyle \mathrm {T} (X)\;}
   является достаточной статистикой для параметра 
  
    
      
        θ
        ∈
        Θ
        ,
        
      
    
    {\displaystyle \theta \in \Theta ,\;}
  , если и только если существуют такие измеримые функции 
  
    
      
        h
      
    
    {\displaystyle h}
   и 
  
    
      
        g
      
    
    {\displaystyle g}
  , что можно записать:

  
    
      
        
          f
          
            θ
          
        
        (
        x
        )
        =
        h
        (
        x
        )
        
        g
        (
        θ
        ,
        
          T
        
        (
        x
        )
        )
      
    
    {\displaystyle f_{\theta }(x)=h(x)\,g(\theta ,\mathrm {T} (x))}

Доказательство
Ниже приведено доказательство для частного случая, когда распределение вероятностей является дискретным. Тогда 
  
    
      
        
          f
          
            θ
          
        
        (
        x
        )
        =
        
          P
        
        (
        X
        =
        x
        
          |
        
        θ
        )
      
    
    {\displaystyle f_{\theta }(x)=\mathbb {P} (X=x|\theta )}
   — Функция вероятности.
Пусть данная функция имеет факторизацию, как в формулировке теоремы, и 
  
    
      
        
          T
        
        (
        x
        )
        =
        t
        .
      
    
    {\displaystyle \mathrm {T} (x)=t.}
  
Тогда имеем:

  
    
      
        
          
            
              
                
                  P
                
                (
                X
                =
                x
                
                  |
                
                
                  T
                
                (
                X
                )
                =
                t
                ,
                θ
                )
              
              
                
                =
                
                  
                    
                      
                        P
                      
                      (
                      X
                      =
                      x
                      
                        |
                      
                      θ
                      )
                    
                    
                      
                        P
                      
                      (
                      
                        T
                      
                      (
                      X
                      )
                      =
                      t
                      
                        |
                      
                      θ
                      )
                    
                  
                
              
              
                =
                
                  
                    
                      h
                      (
                      x
                      )
                      
                      g
                      (
                      θ
                      ,
                      
                        T
                      
                      (
                      x
                      )
                      )
                    
                    
                      
                        ∑
                        
                          x
                          :
                          
                            T
                          
                          (
                          x
                          )
                          =
                          t
                        
                      
                      h
                      (
                      x
                      )
                      
                      g
                      (
                      θ
                      ,
                      
                        T
                      
                      (
                      x
                      )
                      )
                    
                  
                
              
            
            
              
              
                
                =
                
                  
                    
                      h
                      (
                      x
                      )
                      
                      g
                      (
                      θ
                      ,
                      t
                      )
                    
                    
                      
                        ∑
                        
                          x
                          :
                          
                            T
                          
                          (
                          x
                          )
                          =
                          t
                        
                      
                      h
                      (
                      x
                      )
                      
                      g
                      (
                      θ
                      ,
                      t
                      )
                    
                  
                
              
              
                =
                
                  
                    
                      h
                      (
                      x
                      )
                      
                    
                    
                      
                        ∑
                        
                          x
                          :
                          
                            T
                          
                          (
                          x
                          )
                          =
                          t
                        
                      
                      h
                      (
                      x
                      )
                      
                    
                  
                
                .
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}\mathbb {P} (X=x|\mathrm {T} (X)=t,\theta )&={\frac {\mathbb {P} (X=x|\theta )}{\mathbb {P} (\mathrm {T} (X)=t|\theta )}}&={\frac {h(x)\,g(\theta ,\mathrm {T} (x))}{\sum _{x:\mathrm {T} (x)=t}h(x)\,g(\theta ,\mathrm {T} (x))}}\\&={\frac {h(x)\,g(\theta ,t)}{\sum _{x:\mathrm {T} (x)=t}h(x)\,g(\theta ,t)}}&={\frac {h(x)\,}{\sum _{x:\mathrm {T} (x)=t}h(x)\,}}.\end{aligned}}}
  Отсюда видим, что условная вероятность вектора X при заданном значении статистики 
  
    
      
        
          T
        
        (
        X
        )
        
      
    
    {\displaystyle \mathrm {T} (X)\;}
   не зависит от параметра и соответственно 
  
    
      
        
          T
        
        (
        X
        )
        
      
    
    {\displaystyle \mathrm {T} (X)\;}
   — достаточная статистика.
Наоборот можем записать:

  
    
      
        
          P
        
        (
        X
        =
        x
        
          |
        
        θ
        )
        =
        
          P
        
        (
        X
        =
        x
        
          |
        
        
          T
        
        (
        X
        )
        =
        t
        ,
        θ
        )
        ⋅
        
          P
        
        (
        
          T
        
        (
        X
        )
        =
        t
        
          |
        
        θ
        )
        .
      
    
    {\displaystyle \mathbb {P} (X=x|\theta )=\mathbb {P} (X=x|\mathrm {T} (X)=t,\theta )\cdot \mathbb {P} (\mathrm {T} (X)=t|\theta ).}
  Из приведённого выше имеем, что первый множитель правой части не зависит от параметра 
  
    
      
        θ
      
    
    {\displaystyle \theta }
   и его можно взять за функцию 
  
    
      
        h
        (
        x
        )
      
    
    {\displaystyle h(x)}
   из формулировки теоремы. Другой множитель является функцией от 
  
    
      
        θ
        
      
    
    {\displaystyle \theta \;}
   и 
  
    
      
        
          T
        
        (
        X
        )
        ,
        
      
    
    {\displaystyle \mathrm {T} (X),\;}
   и его можно взять за функцию 
  
    
      
        g
        (
        θ
        ,
        
          T
        
        (
        x
        )
        )
        .
      
    
    {\displaystyle g(\theta ,\mathrm {T} (x)).}
   Таким образом, получена необходимая декомпозиция, что завершает доказательство теоремы.

Примеры
Распределение Бернулли
Пусть 
  
    
      
        
          X
          
            1
          
        
        ,
        
          X
          
            2
          
        
        ,
        …
        ,
        
          X
          
            n
          
        
        
      
    
    {\displaystyle X_{1},X_{2},\ldots ,X_{n}\;}
   — последовательность случайных величин, что равны 1 с вероятностью 
  
    
      
        p
      
    
    {\displaystyle p}
   и равны 0 с вероятностью 
  
    
      
        1
        −
        p
      
    
    {\displaystyle 1-p}
   (то есть, имеют распределение Бернулли). Тогда

  
    
      
        
          P
        
        (
        
          x
          
            1
          
        
        ,
        …
        
          x
          
            n
          
        
        
          |
        
        p
        )
        =
        
          p
          
            ∑
            
              x
              
                i
              
            
          
        
        (
        1
        −
        p
        
          )
          
            n
            −
            ∑
            
              x
              
                i
              
            
          
        
        =
        
          p
          
            
              T
            
            (
            x
            )
          
        
        (
        1
        −
        p
        
          )
          
            n
            −
            
              T
            
            (
            x
            )
          
        
        ,
      
    
    {\displaystyle \mathbb {P} (x_{1},\ldots x_{n}|p)=p^{\sum x_{i}}(1-p)^{n-\sum x_{i}}=p^{\mathrm {T} (x)}(1-p)^{n-\mathrm {T} (x)},}
  если взять 
  
    
      
        
          T
        
        (
        X
        )
        =
        
          X
          
            1
          
        
        +
        …
        +
        
          X
          
            n
          
        
        .
      
    
    {\displaystyle \mathrm {T} (X)=X_{1}+\ldots +X_{n}.}
  
Тогда данная статистика является достаточной согласно теореме факторизации, если обозначить

  
    
      
        g
        (
        p
        ,
        
          T
        
        (
        
          x
          
            1
          
        
        ,
        …
        
          x
          
            n
          
        
        )
        )
        =
        
          p
          
            
              T
            
            (
            
              x
              
                1
              
            
            ,
            …
            
              x
              
                n
              
            
            )
          
        
        (
        1
        −
        p
        
          )
          
            n
            −
            
              T
            
            (
            
              x
              
                1
              
            
            ,
            …
            
              x
              
                n
              
            
            )
          
        
        ,
      
    
    {\displaystyle g(p,\mathrm {T} (x_{1},\ldots x_{n}))=p^{\mathrm {T} (x_{1},\ldots x_{n})}(1-p)^{n-\mathrm {T} (x_{1},\ldots x_{n})},}
  

  
    
      
        h
        (
        
          x
          
            1
          
        
        ,
        …
        
          x
          
            n
          
        
        )
        =
        1.
      
    
    {\displaystyle h(x_{1},\ldots x_{n})=1.}

Распределение Пуассона
Пусть 
  
    
      
        
          X
          
            1
          
        
        ,
        
          X
          
            2
          
        
        ,
        …
        ,
        
          X
          
            n
          
        
        
      
    
    {\displaystyle X_{1},X_{2},\ldots ,X_{n}\;}
   — последовательность случайных величин с распределением Пуассона. Тогда

  
    
      
        
          P
        
        (
        
          x
          
            1
          
        
        ,
        …
        
          x
          
            n
          
        
        
          |
        
        λ
        )
        =
        
          
            
              
                e
                
                  −
                  λ
                
              
              
                λ
                
                  
                    x
                    
                      1
                    
                  
                
              
            
            
              
                x
                
                  1
                
              
              !
            
          
        
        ⋅
        
          
            
              
                e
                
                  −
                  λ
                
              
              
                λ
                
                  
                    x
                    
                      2
                    
                  
                
              
            
            
              
                x
                
                  2
                
              
              !
            
          
        
        ⋯
        
          
            
              
                e
                
                  −
                  λ
                
              
              
                λ
                
                  
                    x
                    
                      n
                    
                  
                
              
            
            
              
                x
                
                  n
                
              
              !
            
          
        
        =
        
          e
          
            −
            n
            λ
          
        
        
          λ
          
            (
            
              x
              
                1
              
            
            +
            
              x
              
                2
              
            
            +
            ⋯
            +
            
              x
              
                n
              
            
            )
          
        
        ⋅
        
          
            1
            
              
                x
                
                  1
                
              
              !
              
                x
                
                  2
                
              
              !
              ⋯
              
                x
                
                  n
                
              
              !
            
          
        
        =
        
          e
          
            −
            n
            λ
          
        
        
          λ
          
            
              T
            
            (
            x
            )
          
        
        ⋅
        
          
            1
            
              
                x
                
                  1
                
              
              !
              
                x
                
                  2
                
              
              !
              ⋯
              
                x
                
                  n
                
              
              !
            
          
        
      
    
    {\displaystyle \mathbb {P} (x_{1},\ldots x_{n}|\lambda )={e^{-\lambda }\lambda ^{x_{1}} \over x_{1}!}\cdot {e^{-\lambda }\lambda ^{x_{2}} \over x_{2}!}\cdots {e^{-\lambda }\lambda ^{x_{n}} \over x_{n}!}=e^{-n\lambda }\lambda ^{(x_{1}+x_{2}+\cdots +x_{n})}\cdot {1 \over x_{1}!x_{2}!\cdots x_{n}!}=e^{-n\lambda }\lambda ^{\mathrm {T} (x)}\cdot {1 \over x_{1}!x_{2}!\cdots x_{n}!}}
  
где 
  
    
      
        
          T
        
        (
        X
        )
        =
        
          X
          
            1
          
        
        +
        …
        +
        
          X
          
            n
          
        
        .
      
    
    {\displaystyle \mathrm {T} (X)=X_{1}+\ldots +X_{n}.}
  
Данная статистика является достаточной согласно теореме факторизации, если обозначить

  
    
      
        g
        (
        λ
        ,
        
          T
        
        (
        
          x
          
            1
          
        
        ,
        …
        
          x
          
            n
          
        
        )
        )
        =
        
          e
          
            −
            n
            λ
          
        
        
          λ
          
            
              T
            
            (
            x
            )
          
        
      
    
    {\displaystyle g(\lambda ,\mathrm {T} (x_{1},\ldots x_{n}))=e^{-n\lambda }\lambda ^{\mathrm {T} (x)}}
  

  
    
      
        h
        (
        
          x
          
            1
          
        
        ,
        …
        
          x
          
            n
          
        
        )
        =
        
          
            1
            
              
                x
                
                  1
                
              
              !
              
                x
                
                  2
                
              
              !
              ⋯
              
                x
                
                  n
                
              
              !
            
          
        
      
    
    {\displaystyle h(x_{1},\ldots x_{n})={1 \over x_{1}!x_{2}!\cdots x_{n}!}}

Равномерное распределение
Пусть 
  
    
      
        
          X
          
            1
          
        
        ,
        
          X
          
            2
          
        
        ,
        …
        ,
        
          X
          
            n
          
        
        
      
    
    {\displaystyle X_{1},X_{2},\ldots ,X_{n}\;}
   — последовательность равномерно распределённых случайных величин 
  
    
      
        
          X
          
            1
          
        
        ,
        
          X
          
            2
          
        
        ,
        …
        ,
        
          X
          
            n
          
        
        
         
        U
        (
        a
        ,
        b
        )
      
    
    {\displaystyle X_{1},X_{2},\ldots ,X_{n}\;~U(a,b)}
   . Для этого случая

  
    
      
        
          P
        
        (
        
          x
          
            1
          
        
        ,
        …
        
          x
          
            n
          
        
        
          |
        
        λ
        )
        =
        
          
            (
            
              b
              −
              a
            
            )
          
          
            −
            n
          
        
        
          
            1
          
          
            {
            a
            
            ≤
            
            
              min
              
                1
                ≤
                i
                ≤
                n
              
            
            
              X
              
                i
              
            
            }
          
        
        
          
            1
          
          
            {
            
              max
              
                1
                ≤
                i
                ≤
                n
              
            
            
              X
              
                i
              
            
            
            ≤
            
            b
            }
          
        
        .
      
    
    {\displaystyle \mathbb {P} (x_{1},\ldots x_{n}|\lambda )=\left(b-a\right)^{-n}\mathbf {1} _{\{a\,\leq \,\min _{1\leq i\leq n}X_{i}\}}\mathbf {1} _{\{\max _{1\leq i\leq n}X_{i}\,\leq \,b\}}.}
  Отсюда следует, что статистика 
  
    
      
        T
        (
        X
        )
        =
        
          (
          
            
              min
              
                1
                ≤
                i
                ≤
                n
              
            
            
              X
              
                i
              
            
            ,
            
              max
              
                1
                ≤
                i
                ≤
                n
              
            
            
              X
              
                i
              
            
          
          )
        
      
    
    {\displaystyle T(X)=\left(\min _{1\leq i\leq n}X_{i},\max _{1\leq i\leq n}X_{i}\right)}
   является достаточной.

Нормальное распределение
Для случайных величин 
  
    
      
        
          X
          
            1
          
        
        ,
        
          X
          
            2
          
        
        ,
        …
        ,
        
          X
          
            n
          
        
        
      
    
    {\displaystyle X_{1},X_{2},\ldots ,X_{n}\;}
   с нормальным распределением 
  
    
      
        
          
            N
          
        
        (
        μ
        ,
        
        
          σ
          
            2
          
        
        )
      
    
    {\displaystyle {\mathcal {N}}(\mu ,\,\sigma ^{2})}
   достаточной статистикой будет 
  
    
      
        
          T
        
        (
        X
        )
        =
        
          (
          
            
              ∑
              
                i
                =
                1
              
              
                n
              
            
            
              X
              
                i
              
            
            ,
            
              ∑
              
                i
                =
                1
              
              
                n
              
            
            
              X
              
                i
              
              
                2
              
            
          
          )
        
        
        .
      
    
    {\displaystyle \mathrm {T} (X)=\left(\sum _{i=1}^{n}X_{i},\sum _{i=1}^{n}X_{i}^{2}\right)\,.}

Свойства
Для достаточной статистики T и биективного отображения 
  
    
      
        ϕ
      
    
    {\displaystyle \phi }
   статистика 
  
    
      
        ϕ
        (
        T
        )
      
    
    {\displaystyle \phi (T)}
   тоже является достаточной.
Если 
  
    
      
        δ
        (
        X
        )
      
    
    {\displaystyle \delta (X)}
   — статистическая оценка некоторого параметра 
  
    
      
        θ
        ,
      
    
    {\displaystyle \theta ,}
   
  
    
      
        
          T
        
        (
        X
        )
        ,
        
      
    
    {\displaystyle \mathrm {T} (X),\;}
   — некоторая достаточная статистика и 
  
    
      
        
          δ
          
            1
          
        
        (
        X
        )
        =
        
          
            E
          
        
        [
        δ
        (
        X
        )
        
          |
        
        T
        (
        X
        )
        ]
      
    
    {\displaystyle \delta _{1}(X)={\textrm {E}}[\delta (X)|T(X)]}
   то 
  
    
      
        
          δ
          
            1
          
        
        (
        X
        )
      
    
    {\displaystyle \delta _{1}(X)}
   является лучшей оценкой параметра в смысле среднеквадратичного отклонения, то есть выполняется неравенство
  
    
      
        
          
            E
          
        
        [
        (
        
          δ
          
            1
          
        
        (
        X
        )
        −
        θ
        
          )
          
            2
          
        
        ]
        ≤
        
          
            E
          
        
        [
        (
        δ
        (
        X
        )
        −
        θ
        
          )
          
            2
          
        
        ]
      
    
    {\displaystyle {\textrm {E}}[(\delta _{1}(X)-\theta )^{2}]\leq {\textrm {E}}[(\delta (X)-\theta )^{2}]}
  
причём равенство достигается лишь когда 
  
    
      
        δ
      
    
    {\displaystyle \delta }
   является измеримой функцией от T. (Теорема Рао — Блэквелла — Колмогорова)Из предыдущего получается, что оценка может быть оптимальной в смысле среднеквадратичного отклонения лишь когда она является измеримой функцией минимальной достаточной статистики.
Если статистика 
  
    
      
        T
        =
        
          T
        
        (
        X
        )
        ,
        
      
    
    {\displaystyle T=\mathrm {T} (X),\;}
   является достаточной и полной (то есть, из того, что 
  
    
      
        
          E
          
            θ
          
        
        [
        g
        (
        T
        (
        X
        )
        )
        ]
        =
        0
        ,
        
        ∀
        θ
        ∈
        Θ
      
    
    {\displaystyle E_{\theta }[g(T(X))]=0,\,\forall \theta \in \Theta }
   следует, что 
  
    
      
        
          P
          
            θ
          
        
        (
        g
        (
        T
        (
        X
        )
        )
        =
        0
        )
        =
        1
        
        ∀
        θ
        ∈
        Θ
      
    
    {\displaystyle P_{\theta }(g(T(X))=0)=1\,\forall \theta \in \Theta }
  ), то произвольная измеримая функция от неё является оптимальной оценкой своего математического ожидания.

См. также
Статистическая оценка
Параметр

Литература
Kholevo, A.S. (2001), «Sufficient statistic», in Hazewinkel, Michiel, Encyclopaedia of Mathematics, Springer, ISBN 978-1-55608-010-4
Lehmann, E. L.; Casella, G. (1998). Theory of Point Estimation (2nd ed.). Springer. Chapter 4. ISBN 0-387-98502-6.
Леман Э. Теория точечного оценивания. — М.: Наука, 1991. — 448 с. — ISBN 5-02-013941-6.